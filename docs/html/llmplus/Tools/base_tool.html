<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>llmplus.Tools.base_tool API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>llmplus.Tools.base_tool</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from ..Prompts.prompt_template import PromptTemplate
from ..Models.Cores.base_core import BaseLLM
from abc import ABC, abstractmethod
from typing import List, Iterator, Optional, Union, Type, Tuple, Any, Dict

class BaseTool(ABC):
    &#34;&#34;&#34;This is a base class for tools for LLMs.
    &#34;&#34;&#34;
    def __init__(self, name: str = &#39;base_tool&#39;, description: str = &#39;This is a tool from the base tool class. It does not do anything.&#39;, 
                 key_phrases: List[str] = [], verbose: bool = True) -&gt; None:
        &#34;&#34;&#34;Initialising the tool.
        &#34;&#34;&#34;
        self._name = name
        self._description = description
        self._verbose = verbose
        self._key_phrases = key_phrases

    @property
    def name(self) -&gt; str:
        &#34;&#34;&#34;Name of the tool.

        Returns:
            str: Name of the tool.
        &#34;&#34;&#34;
        return self._name
    
    @property
    def pretty_name(self) -&gt; str:
        &#34;&#34;&#34;Pretty name of the tool.

        Returns:
            str: Pretty name of the tool.
        &#34;&#34;&#34;
        pretty = self.name.replace(&#39;_&#39;, &#39; &#39;)
        return pretty.title()
    
    @property
    def description(self) -&gt; str:
        &#34;&#34;&#34;Description of the tool.

        Returns:
            str: Description of the tool.
        &#34;&#34;&#34;
        import re
        newlines = re.compile(r&#39;[\s\r\n\t]+&#39;)
        return newlines.sub(&#39; &#39;, self._description)
    
    @property
    def key_phrases(self) -&gt; List[str]:
        &#34;&#34;&#34;List of words to trigger the tool in a chat setup.

        Returns:
            List[str]: List of words to trigger the tool in a chat setup.
        &#34;&#34;&#34;
        return self._key_phrases
    
    @property
    def _avail_steps(self) -&gt; List[str]:
        &#34;&#34;&#34;List of available steps of the tool.

        Returns:
            List[str]: List of available steps of the tool.
        &#34;&#34;&#34;
        import inspect
        attrs = dir(self)
        core_methods = [&#39;run&#39;, &#39;print&#39;, &#39;_execute_step&#39;, &#39;_tool_schema&#39;, &#39;_validate_schema&#39;, &#39;_avail_steps&#39;]
        steps = list(filter(lambda x: x not in core_methods, attrs))
        steps = list(filter(lambda x: inspect.ismethod(getattr(self, x)), steps))
        return steps
    
    def print(self, text: str, **kwargs) -&gt; None:
        &#34;&#34;&#34;Print the given text if verbose is True.

        Args:
            text (str): Text to print.
        &#34;&#34;&#34;
        if self._verbose:
            print(text, **kwargs)

    def _execute_step(self, step: str, **kwargs) -&gt; Any:
        &#34;&#34;&#34;Executing a step (a method in your tool class).

        Args:
            step (str): Step to execute.

        Returns:
            Any: Output of the step.
        &#34;&#34;&#34;
        if step not in self._avail_steps:
            raise ValueError(f&#39;Step &#34;{step}&#34; not implemented yet.&#39;)
        method = getattr(self, step)
        return method(**kwargs)
    
    @abstractmethod
    def _tool_schema(self) -&gt; Dict[str, Any]:
        &#34;&#34;&#34;Defining how the run method runs the tool. This method is the most important method of the tool class and must be implemented properly.
        The returned dictionary is the order of running the tool steps, with the keys as the name of the steps and the value as a dictionary input and output of the step.
        Example: {
            &#34;step1&#34;: {&#34;input&#34;: [&#34;tool_input&#34;], &#34;output&#34;: [&#34;var1&#34;, &#34;var2&#34;]}, 
            &#34;step2&#34;: {&#34;input&#34;: [&#34;var1&#34;], &#34;output&#34;: [&#34;var3&#34;]}, 
            &#34;final_step&#34;: {&#34;input&#34;: [&#34;var2&#34;, &#34;var3&#34;, &#34;llm&#34;, &#34;stream&#34;, &#34;prompt_template&#34;], &#34;output&#34;: [&#34;final_output&#34;]}
            }
        As demonstrated in the example, the final step must have the &#34;stream&#34; and &#34;prompt_template&#34; arguments to allow the tool to return a string or an iterator of string as the llm response.

        Returns:
            Dict[str, Any]: Tool run schema dictionary.
        &#34;&#34;&#34;
        pass

    def _validate_schema(self, **kwargs) -&gt; None:
        &#34;&#34;&#34;Raise an error if the _tool_schema is not implemented properly.
        &#34;&#34;&#34;
        import inspect
        schema = self._tool_schema()
        steps = list(schema.keys())
        if len(steps) == 0:
            raise RuntimeError(f&#39;&#34;_tool_schema&#34; cannot return an empty dictinoary.&#39;)
        final_step = list(schema.keys())[-1]
        var_space = [&#34;tool_input&#34;, &#34;llm&#34;, &#34;stream&#34;, &#34;history&#34;, &#34;prompt_template&#34;] + list(kwargs.keys())
        for k, v in schema.items():
            if type(v) != dict:
                raise TypeError(f&#39;Value for step &#34;{k}&#34; is not a dictionary.&#39;)
            for i in [&#39;input&#39;, &#39;output&#39;]:
                if i not in v.keys():
                    raise ValueError(f&#39;&#34;{i}&#34; not in the dictionary of step &#34;{k}&#34;.&#39;)
                if type(v[i]) != list:
                    raise ValueError(f&#39;&#34;{i}&#34; of step &#34;{k}&#34; is not a list.&#39;)
            if k == final_step:
                if &#34;stream&#34; not in v[&#39;input&#39;]:
                    raise ValueError(f&#39;Argument &#34;stream&#34; not in the input of the final step &#34;{k}&#34;.&#39;)
                if &#34;llm&#34; not in v[&#39;input&#39;]:
                    raise ValueError(f&#39;Argument &#34;llm&#34; not in the input of the final step &#34;{k}&#34;.&#39;)
                if &#34;final_output&#34; not in v[&#39;output&#39;]:
                    raise ValueError(f&#39;Argument &#34;final_output&#34; not in the output of the final step &#34;{k}&#34;.&#39;)
                if inspect.signature(getattr(self, k)).return_annotation != Union[str, Iterator[str]]:
                    raise TypeError(f&#39;The output of the final step &#34;{k}&#34; might not be &#34;Union[str, Iterator[str]]&#34;, if it is please add the appropriate type hinting.&#39;)
            if any(i not in var_space for i in v[&#39;input&#39;]):
                raise ValueError(f&#39;Some inputs of step &#34;{k}&#34; are not created prior to executing the step.&#39;)
            var_space.extend(v[&#39;output&#39;])

    def run(self, tool_input: str, llm: Type[BaseLLM], prompt_template: Optional[PromptTemplate] = None, 
            stream: bool = False, history: Optional[Union[List[str], List[Tuple[str, str]]]] = None, add_footnote: bool = False, **kwargs) -&gt; Union[str, Iterator[str]]:
        &#34;&#34;&#34;Run the tool and return the output as a string or an iterator of strings.

        Args:
            tool_input (str): String input for to run the tool.
            llm (Type[BaseLLM]): LLM to generate the output in a conversational setup.
            prompt_template (Optional[PromptTemplate], optional): prompt_template to format the chat history and create final output. If not given, the llm default prompt template will be used. Defaults to None.
            stream (bool, optional): Whether to stream the output, if True, a generator of the output will be returned. Defaults to False.
            history (Optional[Union[List[str], List[Tuple[str, str]]]], optional): Snippet of chat history to help running the tool if required. Defaults to None.
            add_footnote (bool, optional): Whether to append to footnote to the output. Defaults to False.

        Returns:
            Union[str, Iterator[str]]: Output of the tool.
        &#34;&#34;&#34;
        import gc
        self._validate_schema(**kwargs)
        var_space = dict(
            tool_input = tool_input,
            llm = llm,
            prompt_template = llm.core.prompt_template if prompt_template is None else prompt_template,
            stream = stream,
            history = history
        )
        var_space.update(kwargs)
        for k, v in self._tool_schema().items():
            input_dict = {i: var_space[i] for i in v[&#39;input&#39;]}
            output_dict = dict()
            self.print(k)
            if len(v[&#39;output&#39;]) == 0:
                self._execute_step(k, **input_dict)
            elif len(v[&#39;output&#39;]) == 1:
                output_dict[v[&#39;output&#39;][0]] = self._execute_step(k, **input_dict)
            else:
                outputs = self._execute_step(k, **input_dict)
                output_dict = dict(zip(v[&#39;output&#39;], outputs))
            self.print(&#39;\tOutputs:&#39;)
            for s, o in output_dict.items():
                if s != &#39;final_output&#39;:
                    limit = 100
                    output = str(o).replace(&#39;\n&#39;, &#39; &#39;)
                    output = output.replace(&#39;\r&#39;, &#39; &#39;)
                    output = output if len(output) &lt;= limit else output[:limit] + &#39;...&#39;
                    self.print(f&#39;\t{s}: {output}&#39;)
            self.print(&#39;\n&#39;)
            var_space.update(output_dict)
        final = var_space[&#39;final_output&#39;]
        if ((&#39;footnote&#39; in var_space.keys()) &amp; add_footnote):
            final += f&#39;\n\n---\n{var_space[&#34;footnote&#34;]}&#39;
        del var_space
        gc.collect()
        return final

    
    def run_with_chat(self, tool_input: str, llm: Type[BaseLLM], prompt_template: Optional[PromptTemplate] = None, 
            stream: bool = False, history: Optional[Union[List[str], List[Tuple[str, str]]]] = None, 
            add_footnote: bool = True, **kwargs) -&gt; Iterator[Union[str, Tuple[str, str], Iterator[str]]]:
        &#34;&#34;&#34;Running tool with chat, it will yield the markdown friendly string of tool info for each steps and the final output, along with any extra information after the final output.

        Args:
            tool_input (str): String input for to run the tool.
            llm (Type[BaseLLM]): LLM to generate the output in a conversational setup.
            prompt_template (Optional[PromptTemplate], optional): prompt_template to format the chat history and create final output. If not given, the llm default prompt template will be used. Defaults to None.
            stream (bool, optional): Whether to stream the output, if True, a generator of the output will be returned. Defaults to False.
            history (Optional[Union[List[str], List[Tuple[str, str]]]], optional): Snippet of chat history to help running the tool if required. Defaults to None.
            add_footnote (bool, optional): Whether to append to footnote to the output. Defaults to True.

        Yields:
            Iterator[Union[str, Iterator[str]]]: Iterator of the markdown friendly string of tool info for each steps and the final output, along with any extra information after the final output.
        &#34;&#34;&#34;
        self._validate_schema(**kwargs)
        var_space = dict(
            tool_input = tool_input,
            llm = llm,
            prompt_template = llm.core.prompt_template if prompt_template is None else prompt_template,
            stream = stream,
            history = history
        )
        var_space.update(kwargs)
        info = []
        name = self.name.replace(&#39;_&#39;, &#39; &#39;).title()
        header = f&#39;Running &#34;{name}&#34;...&#39;
        yield (header, &#39;\n&#39;.join(info))

        for k, v in self._tool_schema().items():
            input_dict = {i: var_space[i] for i in v[&#39;input&#39;]}
            output_dict = dict()
            info.append(f&#39;{k}:  &#39;)
            yield (header + k, &#39;\n&#39;.join(info))
            if len(v[&#39;output&#39;]) == 0:
                self._execute_step(k, **input_dict)
            elif len(v[&#39;output&#39;]) == 1:
                output_dict[v[&#39;output&#39;][0]] = self._execute_step(k, **input_dict)
            else:
                outputs = self._execute_step(k, **input_dict)
                output_dict = dict(zip(v[&#39;output&#39;], outputs))
            info.append(&#39;\tOutputs:  &#39;)
            for s, o in output_dict.items():
                if s != &#39;final_output&#39;:
                    limit = 100
                    output = str(o).replace(&#39;\n&#39;, &#39; &#39;)
                    output = output.replace(&#39;\r&#39;, &#39; &#39;)
                    output = output if len(output) &lt;= limit else output[:limit] + &#39;...&#39;
                    info.append(f&#39;\t{s}: {output}  &#39;)
            info.append(&#39;\n&#39;)
            var_space.update(output_dict)
            yield (header + k, &#39;\n&#39;.join(info))
        yield (f&#39;&#34;{name}&#34; completed.&#39;, &#39;\n&#39;.join(info))
        yield var_space[&#39;final_output&#39;]

        if ((&#39;footnote&#39; in var_space.keys()) &amp; add_footnote):
            yield var_space[&#34;footnote&#34;]

        </code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="llmplus.Tools.base_tool.BaseTool"><code class="flex name class">
<span>class <span class="ident">BaseTool</span></span>
<span>(</span><span>name: str = 'base_tool', description: str = 'This is a tool from the base tool class. It does not do anything.', key_phrases: List[str] = [], verbose: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>This is a base class for tools for LLMs.</p>
<p>Initialising the tool.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BaseTool(ABC):
    &#34;&#34;&#34;This is a base class for tools for LLMs.
    &#34;&#34;&#34;
    def __init__(self, name: str = &#39;base_tool&#39;, description: str = &#39;This is a tool from the base tool class. It does not do anything.&#39;, 
                 key_phrases: List[str] = [], verbose: bool = True) -&gt; None:
        &#34;&#34;&#34;Initialising the tool.
        &#34;&#34;&#34;
        self._name = name
        self._description = description
        self._verbose = verbose
        self._key_phrases = key_phrases

    @property
    def name(self) -&gt; str:
        &#34;&#34;&#34;Name of the tool.

        Returns:
            str: Name of the tool.
        &#34;&#34;&#34;
        return self._name
    
    @property
    def pretty_name(self) -&gt; str:
        &#34;&#34;&#34;Pretty name of the tool.

        Returns:
            str: Pretty name of the tool.
        &#34;&#34;&#34;
        pretty = self.name.replace(&#39;_&#39;, &#39; &#39;)
        return pretty.title()
    
    @property
    def description(self) -&gt; str:
        &#34;&#34;&#34;Description of the tool.

        Returns:
            str: Description of the tool.
        &#34;&#34;&#34;
        import re
        newlines = re.compile(r&#39;[\s\r\n\t]+&#39;)
        return newlines.sub(&#39; &#39;, self._description)
    
    @property
    def key_phrases(self) -&gt; List[str]:
        &#34;&#34;&#34;List of words to trigger the tool in a chat setup.

        Returns:
            List[str]: List of words to trigger the tool in a chat setup.
        &#34;&#34;&#34;
        return self._key_phrases
    
    @property
    def _avail_steps(self) -&gt; List[str]:
        &#34;&#34;&#34;List of available steps of the tool.

        Returns:
            List[str]: List of available steps of the tool.
        &#34;&#34;&#34;
        import inspect
        attrs = dir(self)
        core_methods = [&#39;run&#39;, &#39;print&#39;, &#39;_execute_step&#39;, &#39;_tool_schema&#39;, &#39;_validate_schema&#39;, &#39;_avail_steps&#39;]
        steps = list(filter(lambda x: x not in core_methods, attrs))
        steps = list(filter(lambda x: inspect.ismethod(getattr(self, x)), steps))
        return steps
    
    def print(self, text: str, **kwargs) -&gt; None:
        &#34;&#34;&#34;Print the given text if verbose is True.

        Args:
            text (str): Text to print.
        &#34;&#34;&#34;
        if self._verbose:
            print(text, **kwargs)

    def _execute_step(self, step: str, **kwargs) -&gt; Any:
        &#34;&#34;&#34;Executing a step (a method in your tool class).

        Args:
            step (str): Step to execute.

        Returns:
            Any: Output of the step.
        &#34;&#34;&#34;
        if step not in self._avail_steps:
            raise ValueError(f&#39;Step &#34;{step}&#34; not implemented yet.&#39;)
        method = getattr(self, step)
        return method(**kwargs)
    
    @abstractmethod
    def _tool_schema(self) -&gt; Dict[str, Any]:
        &#34;&#34;&#34;Defining how the run method runs the tool. This method is the most important method of the tool class and must be implemented properly.
        The returned dictionary is the order of running the tool steps, with the keys as the name of the steps and the value as a dictionary input and output of the step.
        Example: {
            &#34;step1&#34;: {&#34;input&#34;: [&#34;tool_input&#34;], &#34;output&#34;: [&#34;var1&#34;, &#34;var2&#34;]}, 
            &#34;step2&#34;: {&#34;input&#34;: [&#34;var1&#34;], &#34;output&#34;: [&#34;var3&#34;]}, 
            &#34;final_step&#34;: {&#34;input&#34;: [&#34;var2&#34;, &#34;var3&#34;, &#34;llm&#34;, &#34;stream&#34;, &#34;prompt_template&#34;], &#34;output&#34;: [&#34;final_output&#34;]}
            }
        As demonstrated in the example, the final step must have the &#34;stream&#34; and &#34;prompt_template&#34; arguments to allow the tool to return a string or an iterator of string as the llm response.

        Returns:
            Dict[str, Any]: Tool run schema dictionary.
        &#34;&#34;&#34;
        pass

    def _validate_schema(self, **kwargs) -&gt; None:
        &#34;&#34;&#34;Raise an error if the _tool_schema is not implemented properly.
        &#34;&#34;&#34;
        import inspect
        schema = self._tool_schema()
        steps = list(schema.keys())
        if len(steps) == 0:
            raise RuntimeError(f&#39;&#34;_tool_schema&#34; cannot return an empty dictinoary.&#39;)
        final_step = list(schema.keys())[-1]
        var_space = [&#34;tool_input&#34;, &#34;llm&#34;, &#34;stream&#34;, &#34;history&#34;, &#34;prompt_template&#34;] + list(kwargs.keys())
        for k, v in schema.items():
            if type(v) != dict:
                raise TypeError(f&#39;Value for step &#34;{k}&#34; is not a dictionary.&#39;)
            for i in [&#39;input&#39;, &#39;output&#39;]:
                if i not in v.keys():
                    raise ValueError(f&#39;&#34;{i}&#34; not in the dictionary of step &#34;{k}&#34;.&#39;)
                if type(v[i]) != list:
                    raise ValueError(f&#39;&#34;{i}&#34; of step &#34;{k}&#34; is not a list.&#39;)
            if k == final_step:
                if &#34;stream&#34; not in v[&#39;input&#39;]:
                    raise ValueError(f&#39;Argument &#34;stream&#34; not in the input of the final step &#34;{k}&#34;.&#39;)
                if &#34;llm&#34; not in v[&#39;input&#39;]:
                    raise ValueError(f&#39;Argument &#34;llm&#34; not in the input of the final step &#34;{k}&#34;.&#39;)
                if &#34;final_output&#34; not in v[&#39;output&#39;]:
                    raise ValueError(f&#39;Argument &#34;final_output&#34; not in the output of the final step &#34;{k}&#34;.&#39;)
                if inspect.signature(getattr(self, k)).return_annotation != Union[str, Iterator[str]]:
                    raise TypeError(f&#39;The output of the final step &#34;{k}&#34; might not be &#34;Union[str, Iterator[str]]&#34;, if it is please add the appropriate type hinting.&#39;)
            if any(i not in var_space for i in v[&#39;input&#39;]):
                raise ValueError(f&#39;Some inputs of step &#34;{k}&#34; are not created prior to executing the step.&#39;)
            var_space.extend(v[&#39;output&#39;])

    def run(self, tool_input: str, llm: Type[BaseLLM], prompt_template: Optional[PromptTemplate] = None, 
            stream: bool = False, history: Optional[Union[List[str], List[Tuple[str, str]]]] = None, add_footnote: bool = False, **kwargs) -&gt; Union[str, Iterator[str]]:
        &#34;&#34;&#34;Run the tool and return the output as a string or an iterator of strings.

        Args:
            tool_input (str): String input for to run the tool.
            llm (Type[BaseLLM]): LLM to generate the output in a conversational setup.
            prompt_template (Optional[PromptTemplate], optional): prompt_template to format the chat history and create final output. If not given, the llm default prompt template will be used. Defaults to None.
            stream (bool, optional): Whether to stream the output, if True, a generator of the output will be returned. Defaults to False.
            history (Optional[Union[List[str], List[Tuple[str, str]]]], optional): Snippet of chat history to help running the tool if required. Defaults to None.
            add_footnote (bool, optional): Whether to append to footnote to the output. Defaults to False.

        Returns:
            Union[str, Iterator[str]]: Output of the tool.
        &#34;&#34;&#34;
        import gc
        self._validate_schema(**kwargs)
        var_space = dict(
            tool_input = tool_input,
            llm = llm,
            prompt_template = llm.core.prompt_template if prompt_template is None else prompt_template,
            stream = stream,
            history = history
        )
        var_space.update(kwargs)
        for k, v in self._tool_schema().items():
            input_dict = {i: var_space[i] for i in v[&#39;input&#39;]}
            output_dict = dict()
            self.print(k)
            if len(v[&#39;output&#39;]) == 0:
                self._execute_step(k, **input_dict)
            elif len(v[&#39;output&#39;]) == 1:
                output_dict[v[&#39;output&#39;][0]] = self._execute_step(k, **input_dict)
            else:
                outputs = self._execute_step(k, **input_dict)
                output_dict = dict(zip(v[&#39;output&#39;], outputs))
            self.print(&#39;\tOutputs:&#39;)
            for s, o in output_dict.items():
                if s != &#39;final_output&#39;:
                    limit = 100
                    output = str(o).replace(&#39;\n&#39;, &#39; &#39;)
                    output = output.replace(&#39;\r&#39;, &#39; &#39;)
                    output = output if len(output) &lt;= limit else output[:limit] + &#39;...&#39;
                    self.print(f&#39;\t{s}: {output}&#39;)
            self.print(&#39;\n&#39;)
            var_space.update(output_dict)
        final = var_space[&#39;final_output&#39;]
        if ((&#39;footnote&#39; in var_space.keys()) &amp; add_footnote):
            final += f&#39;\n\n---\n{var_space[&#34;footnote&#34;]}&#39;
        del var_space
        gc.collect()
        return final

    
    def run_with_chat(self, tool_input: str, llm: Type[BaseLLM], prompt_template: Optional[PromptTemplate] = None, 
            stream: bool = False, history: Optional[Union[List[str], List[Tuple[str, str]]]] = None, 
            add_footnote: bool = True, **kwargs) -&gt; Iterator[Union[str, Tuple[str, str], Iterator[str]]]:
        &#34;&#34;&#34;Running tool with chat, it will yield the markdown friendly string of tool info for each steps and the final output, along with any extra information after the final output.

        Args:
            tool_input (str): String input for to run the tool.
            llm (Type[BaseLLM]): LLM to generate the output in a conversational setup.
            prompt_template (Optional[PromptTemplate], optional): prompt_template to format the chat history and create final output. If not given, the llm default prompt template will be used. Defaults to None.
            stream (bool, optional): Whether to stream the output, if True, a generator of the output will be returned. Defaults to False.
            history (Optional[Union[List[str], List[Tuple[str, str]]]], optional): Snippet of chat history to help running the tool if required. Defaults to None.
            add_footnote (bool, optional): Whether to append to footnote to the output. Defaults to True.

        Yields:
            Iterator[Union[str, Iterator[str]]]: Iterator of the markdown friendly string of tool info for each steps and the final output, along with any extra information after the final output.
        &#34;&#34;&#34;
        self._validate_schema(**kwargs)
        var_space = dict(
            tool_input = tool_input,
            llm = llm,
            prompt_template = llm.core.prompt_template if prompt_template is None else prompt_template,
            stream = stream,
            history = history
        )
        var_space.update(kwargs)
        info = []
        name = self.name.replace(&#39;_&#39;, &#39; &#39;).title()
        header = f&#39;Running &#34;{name}&#34;...&#39;
        yield (header, &#39;\n&#39;.join(info))

        for k, v in self._tool_schema().items():
            input_dict = {i: var_space[i] for i in v[&#39;input&#39;]}
            output_dict = dict()
            info.append(f&#39;{k}:  &#39;)
            yield (header + k, &#39;\n&#39;.join(info))
            if len(v[&#39;output&#39;]) == 0:
                self._execute_step(k, **input_dict)
            elif len(v[&#39;output&#39;]) == 1:
                output_dict[v[&#39;output&#39;][0]] = self._execute_step(k, **input_dict)
            else:
                outputs = self._execute_step(k, **input_dict)
                output_dict = dict(zip(v[&#39;output&#39;], outputs))
            info.append(&#39;\tOutputs:  &#39;)
            for s, o in output_dict.items():
                if s != &#39;final_output&#39;:
                    limit = 100
                    output = str(o).replace(&#39;\n&#39;, &#39; &#39;)
                    output = output.replace(&#39;\r&#39;, &#39; &#39;)
                    output = output if len(output) &lt;= limit else output[:limit] + &#39;...&#39;
                    info.append(f&#39;\t{s}: {output}  &#39;)
            info.append(&#39;\n&#39;)
            var_space.update(output_dict)
            yield (header + k, &#39;\n&#39;.join(info))
        yield (f&#39;&#34;{name}&#34; completed.&#39;, &#39;\n&#39;.join(info))
        yield var_space[&#39;final_output&#39;]

        if ((&#39;footnote&#39; in var_space.keys()) &amp; add_footnote):
            yield var_space[&#34;footnote&#34;]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="llmplus.Tools.web_search_tool.WebSearchTool" href="web_search_tool.html#llmplus.Tools.web_search_tool.WebSearchTool">WebSearchTool</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="llmplus.Tools.base_tool.BaseTool.description"><code class="name">var <span class="ident">description</span> : str</code></dt>
<dd>
<div class="desc"><p>Description of the tool.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>Description of the tool.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def description(self) -&gt; str:
    &#34;&#34;&#34;Description of the tool.

    Returns:
        str: Description of the tool.
    &#34;&#34;&#34;
    import re
    newlines = re.compile(r&#39;[\s\r\n\t]+&#39;)
    return newlines.sub(&#39; &#39;, self._description)</code></pre>
</details>
</dd>
<dt id="llmplus.Tools.base_tool.BaseTool.key_phrases"><code class="name">var <span class="ident">key_phrases</span> : List[str]</code></dt>
<dd>
<div class="desc"><p>List of words to trigger the tool in a chat setup.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>List[str]</code></dt>
<dd>List of words to trigger the tool in a chat setup.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def key_phrases(self) -&gt; List[str]:
    &#34;&#34;&#34;List of words to trigger the tool in a chat setup.

    Returns:
        List[str]: List of words to trigger the tool in a chat setup.
    &#34;&#34;&#34;
    return self._key_phrases</code></pre>
</details>
</dd>
<dt id="llmplus.Tools.base_tool.BaseTool.name"><code class="name">var <span class="ident">name</span> : str</code></dt>
<dd>
<div class="desc"><p>Name of the tool.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>Name of the tool.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def name(self) -&gt; str:
    &#34;&#34;&#34;Name of the tool.

    Returns:
        str: Name of the tool.
    &#34;&#34;&#34;
    return self._name</code></pre>
</details>
</dd>
<dt id="llmplus.Tools.base_tool.BaseTool.pretty_name"><code class="name">var <span class="ident">pretty_name</span> : str</code></dt>
<dd>
<div class="desc"><p>Pretty name of the tool.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>Pretty name of the tool.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def pretty_name(self) -&gt; str:
    &#34;&#34;&#34;Pretty name of the tool.

    Returns:
        str: Pretty name of the tool.
    &#34;&#34;&#34;
    pretty = self.name.replace(&#39;_&#39;, &#39; &#39;)
    return pretty.title()</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="llmplus.Tools.base_tool.BaseTool.print"><code class="name flex">
<span>def <span class="ident">print</span></span>(<span>self, text: str, **kwargs) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Print the given text if verbose is True.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>text</code></strong> :&ensp;<code>str</code></dt>
<dd>Text to print.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def print(self, text: str, **kwargs) -&gt; None:
    &#34;&#34;&#34;Print the given text if verbose is True.

    Args:
        text (str): Text to print.
    &#34;&#34;&#34;
    if self._verbose:
        print(text, **kwargs)</code></pre>
</details>
</dd>
<dt id="llmplus.Tools.base_tool.BaseTool.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self, tool_input: str, llm: Type[<a title="llmplus.Models.Cores.base_core.BaseLLM" href="../Models/Cores/base_core.html#llmplus.Models.Cores.base_core.BaseLLM">BaseLLM</a>], prompt_template: Optional[<a title="llmplus.Prompts.prompt_template.PromptTemplate" href="../Prompts/prompt_template.html#llmplus.Prompts.prompt_template.PromptTemplate">PromptTemplate</a>] = None, stream: bool = False, history: Union[List[str], List[Tuple[str, str]], ForwardRef(None)] = None, add_footnote: bool = False, **kwargs) ‑> Union[str, Iterator[str]]</span>
</code></dt>
<dd>
<div class="desc"><p>Run the tool and return the output as a string or an iterator of strings.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tool_input</code></strong> :&ensp;<code>str</code></dt>
<dd>String input for to run the tool.</dd>
<dt><strong><code>llm</code></strong> :&ensp;<code>Type[BaseLLM]</code></dt>
<dd>LLM to generate the output in a conversational setup.</dd>
<dt><strong><code>prompt_template</code></strong> :&ensp;<code>Optional[PromptTemplate]</code>, optional</dt>
<dd>prompt_template to format the chat history and create final output. If not given, the llm default prompt template will be used. Defaults to None.</dd>
<dt><strong><code>stream</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to stream the output, if True, a generator of the output will be returned. Defaults to False.</dd>
<dt><strong><code>history</code></strong> :&ensp;<code>Optional[Union[List[str], List[Tuple[str, str]]]]</code>, optional</dt>
<dd>Snippet of chat history to help running the tool if required. Defaults to None.</dd>
<dt><strong><code>add_footnote</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to append to footnote to the output. Defaults to False.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Union[str, Iterator[str]]</code></dt>
<dd>Output of the tool.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(self, tool_input: str, llm: Type[BaseLLM], prompt_template: Optional[PromptTemplate] = None, 
        stream: bool = False, history: Optional[Union[List[str], List[Tuple[str, str]]]] = None, add_footnote: bool = False, **kwargs) -&gt; Union[str, Iterator[str]]:
    &#34;&#34;&#34;Run the tool and return the output as a string or an iterator of strings.

    Args:
        tool_input (str): String input for to run the tool.
        llm (Type[BaseLLM]): LLM to generate the output in a conversational setup.
        prompt_template (Optional[PromptTemplate], optional): prompt_template to format the chat history and create final output. If not given, the llm default prompt template will be used. Defaults to None.
        stream (bool, optional): Whether to stream the output, if True, a generator of the output will be returned. Defaults to False.
        history (Optional[Union[List[str], List[Tuple[str, str]]]], optional): Snippet of chat history to help running the tool if required. Defaults to None.
        add_footnote (bool, optional): Whether to append to footnote to the output. Defaults to False.

    Returns:
        Union[str, Iterator[str]]: Output of the tool.
    &#34;&#34;&#34;
    import gc
    self._validate_schema(**kwargs)
    var_space = dict(
        tool_input = tool_input,
        llm = llm,
        prompt_template = llm.core.prompt_template if prompt_template is None else prompt_template,
        stream = stream,
        history = history
    )
    var_space.update(kwargs)
    for k, v in self._tool_schema().items():
        input_dict = {i: var_space[i] for i in v[&#39;input&#39;]}
        output_dict = dict()
        self.print(k)
        if len(v[&#39;output&#39;]) == 0:
            self._execute_step(k, **input_dict)
        elif len(v[&#39;output&#39;]) == 1:
            output_dict[v[&#39;output&#39;][0]] = self._execute_step(k, **input_dict)
        else:
            outputs = self._execute_step(k, **input_dict)
            output_dict = dict(zip(v[&#39;output&#39;], outputs))
        self.print(&#39;\tOutputs:&#39;)
        for s, o in output_dict.items():
            if s != &#39;final_output&#39;:
                limit = 100
                output = str(o).replace(&#39;\n&#39;, &#39; &#39;)
                output = output.replace(&#39;\r&#39;, &#39; &#39;)
                output = output if len(output) &lt;= limit else output[:limit] + &#39;...&#39;
                self.print(f&#39;\t{s}: {output}&#39;)
        self.print(&#39;\n&#39;)
        var_space.update(output_dict)
    final = var_space[&#39;final_output&#39;]
    if ((&#39;footnote&#39; in var_space.keys()) &amp; add_footnote):
        final += f&#39;\n\n---\n{var_space[&#34;footnote&#34;]}&#39;
    del var_space
    gc.collect()
    return final</code></pre>
</details>
</dd>
<dt id="llmplus.Tools.base_tool.BaseTool.run_with_chat"><code class="name flex">
<span>def <span class="ident">run_with_chat</span></span>(<span>self, tool_input: str, llm: Type[<a title="llmplus.Models.Cores.base_core.BaseLLM" href="../Models/Cores/base_core.html#llmplus.Models.Cores.base_core.BaseLLM">BaseLLM</a>], prompt_template: Optional[<a title="llmplus.Prompts.prompt_template.PromptTemplate" href="../Prompts/prompt_template.html#llmplus.Prompts.prompt_template.PromptTemplate">PromptTemplate</a>] = None, stream: bool = False, history: Union[List[str], List[Tuple[str, str]], ForwardRef(None)] = None, add_footnote: bool = True, **kwargs) ‑> Iterator[Union[str, Tuple[str, str], Iterator[str]]]</span>
</code></dt>
<dd>
<div class="desc"><p>Running tool with chat, it will yield the markdown friendly string of tool info for each steps and the final output, along with any extra information after the final output.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tool_input</code></strong> :&ensp;<code>str</code></dt>
<dd>String input for to run the tool.</dd>
<dt><strong><code>llm</code></strong> :&ensp;<code>Type[BaseLLM]</code></dt>
<dd>LLM to generate the output in a conversational setup.</dd>
<dt><strong><code>prompt_template</code></strong> :&ensp;<code>Optional[PromptTemplate]</code>, optional</dt>
<dd>prompt_template to format the chat history and create final output. If not given, the llm default prompt template will be used. Defaults to None.</dd>
<dt><strong><code>stream</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to stream the output, if True, a generator of the output will be returned. Defaults to False.</dd>
<dt><strong><code>history</code></strong> :&ensp;<code>Optional[Union[List[str], List[Tuple[str, str]]]]</code>, optional</dt>
<dd>Snippet of chat history to help running the tool if required. Defaults to None.</dd>
<dt><strong><code>add_footnote</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to append to footnote to the output. Defaults to True.</dd>
</dl>
<h2 id="yields">Yields</h2>
<dl>
<dt><code>Iterator[Union[str, Iterator[str]]]</code></dt>
<dd>Iterator of the markdown friendly string of tool info for each steps and the final output, along with any extra information after the final output.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_with_chat(self, tool_input: str, llm: Type[BaseLLM], prompt_template: Optional[PromptTemplate] = None, 
        stream: bool = False, history: Optional[Union[List[str], List[Tuple[str, str]]]] = None, 
        add_footnote: bool = True, **kwargs) -&gt; Iterator[Union[str, Tuple[str, str], Iterator[str]]]:
    &#34;&#34;&#34;Running tool with chat, it will yield the markdown friendly string of tool info for each steps and the final output, along with any extra information after the final output.

    Args:
        tool_input (str): String input for to run the tool.
        llm (Type[BaseLLM]): LLM to generate the output in a conversational setup.
        prompt_template (Optional[PromptTemplate], optional): prompt_template to format the chat history and create final output. If not given, the llm default prompt template will be used. Defaults to None.
        stream (bool, optional): Whether to stream the output, if True, a generator of the output will be returned. Defaults to False.
        history (Optional[Union[List[str], List[Tuple[str, str]]]], optional): Snippet of chat history to help running the tool if required. Defaults to None.
        add_footnote (bool, optional): Whether to append to footnote to the output. Defaults to True.

    Yields:
        Iterator[Union[str, Iterator[str]]]: Iterator of the markdown friendly string of tool info for each steps and the final output, along with any extra information after the final output.
    &#34;&#34;&#34;
    self._validate_schema(**kwargs)
    var_space = dict(
        tool_input = tool_input,
        llm = llm,
        prompt_template = llm.core.prompt_template if prompt_template is None else prompt_template,
        stream = stream,
        history = history
    )
    var_space.update(kwargs)
    info = []
    name = self.name.replace(&#39;_&#39;, &#39; &#39;).title()
    header = f&#39;Running &#34;{name}&#34;...&#39;
    yield (header, &#39;\n&#39;.join(info))

    for k, v in self._tool_schema().items():
        input_dict = {i: var_space[i] for i in v[&#39;input&#39;]}
        output_dict = dict()
        info.append(f&#39;{k}:  &#39;)
        yield (header + k, &#39;\n&#39;.join(info))
        if len(v[&#39;output&#39;]) == 0:
            self._execute_step(k, **input_dict)
        elif len(v[&#39;output&#39;]) == 1:
            output_dict[v[&#39;output&#39;][0]] = self._execute_step(k, **input_dict)
        else:
            outputs = self._execute_step(k, **input_dict)
            output_dict = dict(zip(v[&#39;output&#39;], outputs))
        info.append(&#39;\tOutputs:  &#39;)
        for s, o in output_dict.items():
            if s != &#39;final_output&#39;:
                limit = 100
                output = str(o).replace(&#39;\n&#39;, &#39; &#39;)
                output = output.replace(&#39;\r&#39;, &#39; &#39;)
                output = output if len(output) &lt;= limit else output[:limit] + &#39;...&#39;
                info.append(f&#39;\t{s}: {output}  &#39;)
        info.append(&#39;\n&#39;)
        var_space.update(output_dict)
        yield (header + k, &#39;\n&#39;.join(info))
    yield (f&#39;&#34;{name}&#34; completed.&#39;, &#39;\n&#39;.join(info))
    yield var_space[&#39;final_output&#39;]

    if ((&#39;footnote&#39; in var_space.keys()) &amp; add_footnote):
        yield var_space[&#34;footnote&#34;]</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="llmplus.Tools" href="index.html">llmplus.Tools</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="llmplus.Tools.base_tool.BaseTool" href="#llmplus.Tools.base_tool.BaseTool">BaseTool</a></code></h4>
<ul class="two-column">
<li><code><a title="llmplus.Tools.base_tool.BaseTool.description" href="#llmplus.Tools.base_tool.BaseTool.description">description</a></code></li>
<li><code><a title="llmplus.Tools.base_tool.BaseTool.key_phrases" href="#llmplus.Tools.base_tool.BaseTool.key_phrases">key_phrases</a></code></li>
<li><code><a title="llmplus.Tools.base_tool.BaseTool.name" href="#llmplus.Tools.base_tool.BaseTool.name">name</a></code></li>
<li><code><a title="llmplus.Tools.base_tool.BaseTool.pretty_name" href="#llmplus.Tools.base_tool.BaseTool.pretty_name">pretty_name</a></code></li>
<li><code><a title="llmplus.Tools.base_tool.BaseTool.print" href="#llmplus.Tools.base_tool.BaseTool.print">print</a></code></li>
<li><code><a title="llmplus.Tools.base_tool.BaseTool.run" href="#llmplus.Tools.base_tool.BaseTool.run">run</a></code></li>
<li><code><a title="llmplus.Tools.base_tool.BaseTool.run_with_chat" href="#llmplus.Tools.base_tool.BaseTool.run_with_chat">run_with_chat</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>