<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>llmflex.Prompts.prompt_template API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>llmflex.Prompts.prompt_template</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from __future__ import annotations
from jinja2 import Environment
from typing import List, Dict, Any, Optional, Literal, Union, Tuple

DEFAULT_SYSTEM_MESSAGE = &#34;&#34;&#34;This is a conversation between a human user and a helpful AI assistant.&#34;&#34;&#34;

class PromptTemplate:
    &#34;&#34;&#34;Class for storing prompt format presets.
    &#34;&#34;&#34;
    def __init__(
            self,
            template: str,
            eos_token: Optional[str],
            bos_token: Optional[str],
            stop: Optional[List[str]] = None,
            force_real_template: bool = False
    ) -&gt; None:
        &#34;&#34;&#34;Initialising the chat prompt class.

        Args:
            template (str): Jinja2 template.
            eos_token (Optional[str]): EOS token string.
            bos_token (Optional[str]): BOS token string.
            stop (Optional[List[str]], optional): List of stop strings for the llm. If None is given, the EOS token string will be used. Defaults to None.
            force_real_template (bool, optional): Whether to render the given template. For most templates it has no effects. Only for some restrictive templates like llama2. Defaults to False.
        &#34;&#34;&#34;
        self._template = template
        self._eos_token = eos_token
        self._bos_token = bos_token
        self._stop = stop
        self._force_real_template = force_real_template

    @property
    def template(self) -&gt; str:
        return self._template
    
    @property
    def _hidden_template(self) -&gt; str:
        &#34;&#34;&#34;To fix issues with some default templates like llama 2.

        Returns:
            str: The actual template to be rendered.
        &#34;&#34;&#34;
        config = hidden_presets.get(self.template_name)
        return self.template if config is None else config[&#39;template&#39;]
    
    @property
    def rendered_template(self) -&gt; Environment:
        if not hasattr(self, &#39;_rendered_template&#39;):
            from jinja2 import BaseLoader
            template = self.template if self._force_real_template else self._hidden_template
            self._rendered_template = Environment(loader=BaseLoader).from_string(template)
        return self._rendered_template

    @property
    def eos_token(self) -&gt; Optional[str]:
        return self._eos_token
    
    @property
    def bos_token(self) -&gt; Optional[str]:
        return self._bos_token

    @property
    def stop(self) -&gt; List[str]:
        return self._stop if isinstance(self._stop, list) else [self.eos_token] if self.eos_token is not None else []
    
    @property
    def template_name(self) -&gt; str:
        &#34;&#34;&#34;Name of the template.

        Returns:
            str: Name of the template.
        &#34;&#34;&#34;
        if not hasattr(self, &#39;_template_name&#39;):
            self._template_name = &#39;Unititled template&#39;
            for k, v in presets.items():
                if self.template == v[&#39;template&#39;]:
                    self._template_name = k
                    break
        return self._template_name

    def format_history(self, history: Union[List[str], List[Tuple[str, str]]], return_list: bool = False) -&gt; Union[str, List[Dict[str, str]]]:
        &#34;&#34;&#34;Formatting a list of conversation history into a full string of conversation history or a list of messages for the Jinja template to render.

        Args:
            history (Union[List[str], List[Tuple[str, str]]]): List of conversation history. 
            return_list (bool, optional): Whether to return a list of messages for the Jinja template to render. Defaults to False.

        Returns:
            Union[str, List[Dict[str, str]]]: A full string of conversation history or a list of messages for the Jinja template to render.
        &#34;&#34;&#34;
        if len(history) == 0:
            return [] if return_list else &#39;&#39;
        elif not isinstance(history[0], str):
            body = list(map(lambda x: [dict(role=&#39;user&#39;, content=x[0]), dict(role=&#39;assistant&#39;, content=x[1])], history))
            body = sum(body, [])
        else:
            length = len(history)
            is_even = length % 2 == 0
            half = int(length / 2) if is_even else int((length + 1) / 2)
            roles = [&#39;user&#39;, &#39;assistant&#39;] * half
            roles = roles if is_even else roles[1:]
            body = list(map(lambda x: dict(role=x[0], content=x[1]), list(zip(roles, history))))
        if return_list:
            return body
        return self.rendered_template.render(messages=body, bos_token=self.bos_token, eos_token=self.eos_token, add_generation_prompt=False)

    def create_prompt(self, user: str, system: str = DEFAULT_SYSTEM_MESSAGE, history: Optional[Union[List[str], List[Tuple[str, str]]]] = None) -&gt; str:
        &#34;&#34;&#34;Creating the full chat prompt.

        Args:
            user (str): Latest user input.
            system (str, optional): System message. Defaults to DEFAULT_SYSTEM_MESSAGE.
            history (Optional[Union[List[str], List[Tuple[str, str]]]], optional): List of conversation history. Defaults to None.

        Returns:
            str: The full prompt.
        &#34;&#34;&#34;
        head = [dict(role=&#39;system&#39;, content=system)] if system.strip(&#39; \n\r\t&#39;) != &#39;&#39; else []
        history = [] if history is None else history
        body = self.format_history(history=history, return_list=True)
        tail = [dict(role=&#39;user&#39;, content=user)]
        prompt = head + body + tail
        return self.rendered_template.render(messages=prompt, bos_token=self.bos_token, eos_token=self.eos_token, add_generation_prompt=True)
    
    def create_custom_prompt(self, messages: List[Dict[str, str]], add_generation_prompt: bool = True) -&gt; str:
        &#34;&#34;&#34;Creating a custom prompt with your given list of messages. Each message should contain a dictionary with the key &#34;role&#34; and &#34;content&#34;.

        Args:
            messages (List[Dict[str, str]]): List of messages. Each message should contain a dictionary with the key &#34;role&#34; and &#34;content&#34;.
            add_generation_prompt (bool, optional): Whether to add the assistant tokens at the end of the prompt. Defaults to True.

        Returns:
            str: The full prompt given your messages.
        &#34;&#34;&#34;
        return self.rendered_template.render(messages=messages, bos_token=self.bos_token, eos_token=self.eos_token, add_generation_prompt=add_generation_prompt)
    
    @classmethod
    def from_dict(cls, format_dict: Dict[str, Any], template_name: Optional[str] = None) -&gt; PromptTemplate:
        &#34;&#34;&#34;Initialise the prompt template from a dictionary.

        Args:
            format_dict (Dict[str, Any]): Dictionary of the prompt format.
            template_name (Optional[str], optional): Name of the template. Defaults to None.

        Returns:
            PromptTemplate: The initialised PromptTemplate instance.
        &#34;&#34;&#34;
        template = cls(**format_dict)
        if template_name is not None:
            template._template_name = template_name
        return template
    
    @classmethod
    def from_json(cls, file_dir: str) -&gt; PromptTemplate:
        &#34;&#34;&#34;Initialise the prompt template from a json file.

        Args:
            file_dir (str): json file path of the prompt format.

        Returns:
            PromptTemplatet: The initialised PromptTemplate instance.
        &#34;&#34;&#34;
        from ..utils import read_json
        return cls.from_dict(read_json(file_dir=file_dir), template_name=file_dir)
    
    @classmethod
    def from_preset(cls, style: Literal[&#39;Default&#39;, &#39;Llama2&#39;, &#39;Vicuna&#39;, &#39;ChatML&#39;, &#39;Zephyr&#39;, &#39;OpenChat&#39;, &#39;Alpaca&#39;], force_real_template: bool = False) -&gt; PromptTemplate:
        &#34;&#34;&#34;Initialise the prompt template from a preset.

        Args:
            style (Literal[&amp;#39;Default&amp;#39;, &amp;#39;Llama2&amp;#39;, &amp;#39;Vicuna&amp;#39;, &amp;#39;ChatML&amp;#39;, &amp;#39;Zephyr&amp;#39;, &amp;#39;OpenChat&amp;#39;, &amp;#39;Alpaca&amp;#39;]): Format of the prompt.

        Returns:
            PromptTemplate: The initialised PromptTemplate instance.
        &#34;&#34;&#34;
        from copy import deepcopy
        preset = deepcopy(presets[style])
        preset[&#39;force_real_template&#39;] = force_real_template
        return cls.from_dict(preset, template_name=style)
    
    def to_dict(self) -&gt; Dict[str, Any]:
        &#34;&#34;&#34;Export the class as a dictionary.

        Returns:
            Dict[str, Any]: Prompt format as a dictionary.
        &#34;&#34;&#34;
        return dict(
            template = self.template,
            eos_token = self.eos_token,
            bos_token = self.bos_token,
            stop = self.stop if self.stop is not None else [self.eos_token]
        )
    
presets = {
    &#39;Default&#39; : {
        &#39;template&#39;: &#34;{% if messages[0][&#39;role&#39;] == &#39;system&#39; %}{% set loop_messages = messages[1:] %}{{ &#39;SYSTEM:\n&#39; + messages[0][&#39;content&#39;].strip() + &#39;\n\n&#39; }}{% else %}{% set loop_messages = messages %}{% endif %}{% for message in loop_messages %}{% if message[&#39;role&#39;] == &#39;user&#39; %}{{ &#39;USER: &#39; + message[&#39;content&#39;].strip() + &#39;\n&#39; }}{% elif message[&#39;role&#39;] == &#39;assistant&#39; %}{{ &#39;ASSISTANT: &#39; + message[&#39;content&#39;].strip() + &#39;\n&#39; }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ &#39;ASSISTANT:&#39; }}{% endif %}&#34;,
        &#39;eos_token&#39;: &#39;&lt;/s&gt;&#39;,
        &#39;bos_token&#39;: &#39;&lt;s&gt;&#39;,
        &#39;stop&#39;: [&#39;\nASSISTANT&#39;, &#39;\nUSER:&#39;, &#39;ASSISTANT:&#39;, &#39;USER:&#39;]
    },
    &#39;Llama2&#39; : {
        &#39;template&#39;: &#34;{% if messages[0][&#39;role&#39;] == &#39;system&#39; %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0][&#39;content&#39;] %}{% elif false == true and not &#39;&lt;&lt;SYS&gt;&gt;&#39; in messages[0][&#39;content&#39;] %}{% set loop_messages = messages %}{% set system_message = &#39;You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don\\&#39;t know the answer to a question, please don\\&#39;t share false information.&#39; %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message[&#39;role&#39;] == &#39;user&#39;) != (loop.index0 % 2 == 0) %}{{ raise_exception(&#39;Conversation roles must alternate user/assistant/user/assistant/...&#39;) }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = &#39;&lt;&lt;SYS&gt;&gt;\\n&#39; + system_message + &#39;\\n&lt;&lt;/SYS&gt;&gt;\\n\\n&#39; + message[&#39;content&#39;] %}{% else %}{% set content = message[&#39;content&#39;] %}{% endif %}{% if message[&#39;role&#39;] == &#39;user&#39; %}{{ bos_token + &#39;[INST] &#39; + content.strip() + &#39; [/INST]&#39; }}{% elif message[&#39;role&#39;] == &#39;system&#39; %}{{ &#39;&lt;&lt;SYS&gt;&gt;\\n&#39; + content.strip() + &#39;\\n&lt;&lt;/SYS&gt;&gt;\\n\\n&#39; }}{% elif message[&#39;role&#39;] == &#39;assistant&#39; %}{{ &#39; &#39;  + content.strip() + &#39; &#39; + eos_token }}{% endif %}{% endfor %}&#34;,
        &#39;eos_token&#39;: &#39;&lt;/s&gt;&#39;,
        &#39;bos_token&#39;: &#39;&lt;s&gt;&#39;,
        &#39;stop&#39;: [&#39;&lt;/s&gt;&#39;, &#39;&lt;/s&gt;&lt;s&gt;&#39;, &#39;[INST]&#39;, &#39;&lt;s&gt;[INST]&#39;]
    },
    &#39;Vicuna&#39; : {
        &#39;template&#39;: &#34;{% if messages[0][&#39;role&#39;] == &#39;system&#39; %}{% set loop_messages = messages[1:] %}{{ messages[0][&#39;content&#39;].strip() + &#39;\n\n&#39; }}{% else %}{% set loop_messages = messages %}{% endif %}{% for message in loop_messages %}{% if message[&#39;role&#39;] == &#39;user&#39; %}{{ &#39;USER: &#39; + message[&#39;content&#39;].strip() + &#39;\n&#39; }}{% elif message[&#39;role&#39;] == &#39;assistant&#39; %}{{ &#39;ASSISTANT: &#39; + message[&#39;content&#39;].strip() + eos_token + &#39;\n&#39; }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ &#39;ASSISTANT:&#39; }}{% endif %}&#34;,
        &#39;eos_token&#39;: &#39;&lt;/s&gt;&#39;,
        &#39;bos_token&#39;: &#39;&lt;s&gt;&#39;,
        &#39;stop&#39;: [&#39;\nASSISTANT&#39;, &#39;\nUSER:&#39;, &#39;ASSISTANT:&#39;, &#39;USER:&#39;, &#39;&lt;/s&gt;&#39;]
    },
    &#39;ChatML&#39; : {
        &#39;template&#39;: &#34;{% for message in messages %}{{&#39;&lt;|im_start|&gt;&#39; + message[&#39;role&#39;] + &#39;\n&#39; + message[&#39;content&#39;] + &#39;&lt;|im_end|&gt;&#39; + &#39;\n&#39;}}{% endfor %}{% if add_generation_prompt %}{{ &#39;&lt;|im_start|&gt;assistant\n&#39; }}{% endif %}&#34;,
        &#39;eos_token&#39;: &#39;&lt;|im_end|&gt;&#39;,
        &#39;bos_token&#39;: &#39;&lt;|endoftext|&gt;&#39;,
        &#39;stop&#39;: [&#39;&lt;|im_start|&gt;&#39;, &#39;&lt;|im_end|&gt;&#39;, &#39;&lt;|im_start|&gt;user\n&#39;, &#39;&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n&#39;]
    },
    &#39;Zephyr&#39; : {
        &#39;template&#39;: &#34;{% for message in messages %}\n{% if message[&#39;role&#39;] == &#39;user&#39; %}\n{{ &#39;&lt;|user|&gt;\n&#39; + message[&#39;content&#39;] + eos_token }}\n{% elif message[&#39;role&#39;] == &#39;system&#39; %}\n{{ &#39;&lt;|system|&gt;\n&#39; + message[&#39;content&#39;] + eos_token }}\n{% elif message[&#39;role&#39;] == &#39;assistant&#39; %}\n{{ &#39;&lt;|assistant|&gt;\n&#39;  + message[&#39;content&#39;] + eos_token }}\n{% endif %}\n{% if loop.last and add_generation_prompt %}\n{{ &#39;&lt;|assistant|&gt;&#39; }}\n{% endif %}\n{% endfor %}&#34;,
        &#39;eos_token&#39;: &#39;&lt;/s&gt;&#39;,
        &#39;bos_token&#39;: &#39;&lt;s&gt;&#39;,
        &#39;stop&#39;: [&#39;&lt;/s&gt;&#39;, &#39;&lt;/s&gt;\n&#39;, &#39;&lt;|user|&gt;\n&#39;, &#39;&lt;/s&gt;\n&lt;|user|&gt;\n&#39;]
    },
    &#39;OpenChat&#39; : {
        &#39;template&#39;: &#34;{{ bos_token }}{% for message in messages %}{{ &#39;GPT4 Correct &#39; + message[&#39;role&#39;].title() + &#39;: &#39; + message[&#39;content&#39;] + &#39;&lt;|end_of_turn|&gt;&#39;}}{% endfor %}{% if add_generation_prompt %}{{ &#39;GPT4 Correct Assistant:&#39; }}{% endif %}&#34;,
        &#39;eos_token&#39;: &#39;&lt;|end_of_turn|&gt;&#39;,
        &#39;bos_token&#39;: &#39;&lt;s&gt;&#39;,
        &#39;stop&#39;: [&#39;&lt;/s&gt;&#39;, &#39;&lt;|end_of_turn|&gt;&#39;, &#39;&lt;|end_of_turn|&gt;GPT4 Correct Assistant: &#39;]
    },
    &#39;Alpaca&#39; : {
        &#39;template&#39;: &#34;{% for message in messages %}{% if message[&#39;role&#39;] == &#39;system&#39; %}{% if message[&#39;content&#39;]%}{{&#39;### Instruction: &#39; + message[&#39;content&#39;]+&#39;\n&#39;}}{% endif %}{% elif message[&#39;role&#39;] == &#39;user&#39; %}{{&#39;### Input: &#39; + message[&#39;content&#39;]+&#39;\n&#39;}}{% elif message[&#39;role&#39;] == &#39;assistant&#39; %}{{&#39;### Response: &#39;  + message[&#39;content&#39;] + &#39;\n&#39;}}{% endif %}{% if loop.last and add_generation_prompt %}{{ &#39;### Response:&#39; }}{% endif %}{% endfor %}&#34;,
        &#39;eos_token&#39;: &#39;&lt;|end_of_turn|&gt;&#39;,
        &#39;bos_token&#39;: &#39;&lt;s&gt;&#39;,
        &#39;stop&#39;: [&#39;### Input: &#39;, &#39;\n### Input: &#39;, &#39;###&#39;]
    },
}

hidden_presets = {
    &#39;Llama2&#39; : {
        &#39;template&#39;: &#34;{% if messages[0][&#39;role&#39;] == &#39;system&#39; %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0][&#39;content&#39;] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if loop.index0 == 0 and system_message != false %}{% set content = &#39;&lt;&lt;SYS&gt;&gt;\\n&#39; + system_message + &#39;\\n&lt;&lt;/SYS&gt;&gt;\\n\\n&#39; + message[&#39;content&#39;] %}{% else %}{% set content = message[&#39;content&#39;] %}{% endif %}{% if message[&#39;role&#39;] == &#39;user&#39; %}{% if loop.index0 == 0 %}{{ &#39;[INST] &#39; + content.strip() + &#39; [/INST]&#39; }}{% else %}{{ bos_token + &#39;[INST] &#39; + content.strip() + &#39; [/INST]&#39; }}{% endif %}{% elif message[&#39;role&#39;] == &#39;system&#39; %}{{ &#39;&lt;&lt;SYS&gt;&gt;\\n&#39; + content.strip() + &#39;\\n&lt;&lt;/SYS&gt;&gt;\\n\\n&#39; }}{% elif message[&#39;role&#39;] == &#39;assistant&#39; %}{% if loop.index0 == 0 and system_message != false %}{% set prefix = &#39;[INST]&#39; + &#39;&lt;&lt;SYS&gt;&gt;\\n&#39; + system_message + &#39;\\n&lt;&lt;/SYS&gt;&gt;\\n\\n&#39; + &#39;[/INST] &#39; %}{% set content = message[&#39;content&#39;] %}{% elif loop.index0 == 0 %}{% set prefix = &#39;[INST][/INST] &#39; %}{% set content = message[&#39;content&#39;] %}{% else %}{% set prefix = &#39; &#39; %}{% endif %}{{ prefix  + content.strip() + &#39; &#39; + eos_token }}{% endif %}{% endfor %}&#34;,
        &#39;eos_token&#39;: &#39;&lt;/s&gt;&#39;,
        &#39;bos_token&#39;: &#39;&lt;s&gt;&#39;,
        &#39;stop&#39;: [&#39;&lt;/s&gt;&#39;, &#39;&lt;/s&gt;&lt;s&gt;&#39;, &#39;[INST]&#39;, &#39;&lt;s&gt;[INST]&#39;]
    }
}</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="llmflex.Prompts.prompt_template.PromptTemplate"><code class="flex name class">
<span>class <span class="ident">PromptTemplate</span></span>
<span>(</span><span>template: str, eos_token: Optional[str], bos_token: Optional[str], stop: Optional[List[str]] = None, force_real_template: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Class for storing prompt format presets.</p>
<p>Initialising the chat prompt class.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>template</code></strong> :&ensp;<code>str</code></dt>
<dd>Jinja2 template.</dd>
<dt><strong><code>eos_token</code></strong> :&ensp;<code>Optional[str]</code></dt>
<dd>EOS token string.</dd>
<dt><strong><code>bos_token</code></strong> :&ensp;<code>Optional[str]</code></dt>
<dd>BOS token string.</dd>
<dt><strong><code>stop</code></strong> :&ensp;<code>Optional[List[str]]</code>, optional</dt>
<dd>List of stop strings for the llm. If None is given, the EOS token string will be used. Defaults to None.</dd>
<dt><strong><code>force_real_template</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to render the given template. For most templates it has no effects. Only for some restrictive templates like llama2. Defaults to False.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PromptTemplate:
    &#34;&#34;&#34;Class for storing prompt format presets.
    &#34;&#34;&#34;
    def __init__(
            self,
            template: str,
            eos_token: Optional[str],
            bos_token: Optional[str],
            stop: Optional[List[str]] = None,
            force_real_template: bool = False
    ) -&gt; None:
        &#34;&#34;&#34;Initialising the chat prompt class.

        Args:
            template (str): Jinja2 template.
            eos_token (Optional[str]): EOS token string.
            bos_token (Optional[str]): BOS token string.
            stop (Optional[List[str]], optional): List of stop strings for the llm. If None is given, the EOS token string will be used. Defaults to None.
            force_real_template (bool, optional): Whether to render the given template. For most templates it has no effects. Only for some restrictive templates like llama2. Defaults to False.
        &#34;&#34;&#34;
        self._template = template
        self._eos_token = eos_token
        self._bos_token = bos_token
        self._stop = stop
        self._force_real_template = force_real_template

    @property
    def template(self) -&gt; str:
        return self._template
    
    @property
    def _hidden_template(self) -&gt; str:
        &#34;&#34;&#34;To fix issues with some default templates like llama 2.

        Returns:
            str: The actual template to be rendered.
        &#34;&#34;&#34;
        config = hidden_presets.get(self.template_name)
        return self.template if config is None else config[&#39;template&#39;]
    
    @property
    def rendered_template(self) -&gt; Environment:
        if not hasattr(self, &#39;_rendered_template&#39;):
            from jinja2 import BaseLoader
            template = self.template if self._force_real_template else self._hidden_template
            self._rendered_template = Environment(loader=BaseLoader).from_string(template)
        return self._rendered_template

    @property
    def eos_token(self) -&gt; Optional[str]:
        return self._eos_token
    
    @property
    def bos_token(self) -&gt; Optional[str]:
        return self._bos_token

    @property
    def stop(self) -&gt; List[str]:
        return self._stop if isinstance(self._stop, list) else [self.eos_token] if self.eos_token is not None else []
    
    @property
    def template_name(self) -&gt; str:
        &#34;&#34;&#34;Name of the template.

        Returns:
            str: Name of the template.
        &#34;&#34;&#34;
        if not hasattr(self, &#39;_template_name&#39;):
            self._template_name = &#39;Unititled template&#39;
            for k, v in presets.items():
                if self.template == v[&#39;template&#39;]:
                    self._template_name = k
                    break
        return self._template_name

    def format_history(self, history: Union[List[str], List[Tuple[str, str]]], return_list: bool = False) -&gt; Union[str, List[Dict[str, str]]]:
        &#34;&#34;&#34;Formatting a list of conversation history into a full string of conversation history or a list of messages for the Jinja template to render.

        Args:
            history (Union[List[str], List[Tuple[str, str]]]): List of conversation history. 
            return_list (bool, optional): Whether to return a list of messages for the Jinja template to render. Defaults to False.

        Returns:
            Union[str, List[Dict[str, str]]]: A full string of conversation history or a list of messages for the Jinja template to render.
        &#34;&#34;&#34;
        if len(history) == 0:
            return [] if return_list else &#39;&#39;
        elif not isinstance(history[0], str):
            body = list(map(lambda x: [dict(role=&#39;user&#39;, content=x[0]), dict(role=&#39;assistant&#39;, content=x[1])], history))
            body = sum(body, [])
        else:
            length = len(history)
            is_even = length % 2 == 0
            half = int(length / 2) if is_even else int((length + 1) / 2)
            roles = [&#39;user&#39;, &#39;assistant&#39;] * half
            roles = roles if is_even else roles[1:]
            body = list(map(lambda x: dict(role=x[0], content=x[1]), list(zip(roles, history))))
        if return_list:
            return body
        return self.rendered_template.render(messages=body, bos_token=self.bos_token, eos_token=self.eos_token, add_generation_prompt=False)

    def create_prompt(self, user: str, system: str = DEFAULT_SYSTEM_MESSAGE, history: Optional[Union[List[str], List[Tuple[str, str]]]] = None) -&gt; str:
        &#34;&#34;&#34;Creating the full chat prompt.

        Args:
            user (str): Latest user input.
            system (str, optional): System message. Defaults to DEFAULT_SYSTEM_MESSAGE.
            history (Optional[Union[List[str], List[Tuple[str, str]]]], optional): List of conversation history. Defaults to None.

        Returns:
            str: The full prompt.
        &#34;&#34;&#34;
        head = [dict(role=&#39;system&#39;, content=system)] if system.strip(&#39; \n\r\t&#39;) != &#39;&#39; else []
        history = [] if history is None else history
        body = self.format_history(history=history, return_list=True)
        tail = [dict(role=&#39;user&#39;, content=user)]
        prompt = head + body + tail
        return self.rendered_template.render(messages=prompt, bos_token=self.bos_token, eos_token=self.eos_token, add_generation_prompt=True)
    
    def create_custom_prompt(self, messages: List[Dict[str, str]], add_generation_prompt: bool = True) -&gt; str:
        &#34;&#34;&#34;Creating a custom prompt with your given list of messages. Each message should contain a dictionary with the key &#34;role&#34; and &#34;content&#34;.

        Args:
            messages (List[Dict[str, str]]): List of messages. Each message should contain a dictionary with the key &#34;role&#34; and &#34;content&#34;.
            add_generation_prompt (bool, optional): Whether to add the assistant tokens at the end of the prompt. Defaults to True.

        Returns:
            str: The full prompt given your messages.
        &#34;&#34;&#34;
        return self.rendered_template.render(messages=messages, bos_token=self.bos_token, eos_token=self.eos_token, add_generation_prompt=add_generation_prompt)
    
    @classmethod
    def from_dict(cls, format_dict: Dict[str, Any], template_name: Optional[str] = None) -&gt; PromptTemplate:
        &#34;&#34;&#34;Initialise the prompt template from a dictionary.

        Args:
            format_dict (Dict[str, Any]): Dictionary of the prompt format.
            template_name (Optional[str], optional): Name of the template. Defaults to None.

        Returns:
            PromptTemplate: The initialised PromptTemplate instance.
        &#34;&#34;&#34;
        template = cls(**format_dict)
        if template_name is not None:
            template._template_name = template_name
        return template
    
    @classmethod
    def from_json(cls, file_dir: str) -&gt; PromptTemplate:
        &#34;&#34;&#34;Initialise the prompt template from a json file.

        Args:
            file_dir (str): json file path of the prompt format.

        Returns:
            PromptTemplatet: The initialised PromptTemplate instance.
        &#34;&#34;&#34;
        from ..utils import read_json
        return cls.from_dict(read_json(file_dir=file_dir), template_name=file_dir)
    
    @classmethod
    def from_preset(cls, style: Literal[&#39;Default&#39;, &#39;Llama2&#39;, &#39;Vicuna&#39;, &#39;ChatML&#39;, &#39;Zephyr&#39;, &#39;OpenChat&#39;, &#39;Alpaca&#39;], force_real_template: bool = False) -&gt; PromptTemplate:
        &#34;&#34;&#34;Initialise the prompt template from a preset.

        Args:
            style (Literal[&amp;#39;Default&amp;#39;, &amp;#39;Llama2&amp;#39;, &amp;#39;Vicuna&amp;#39;, &amp;#39;ChatML&amp;#39;, &amp;#39;Zephyr&amp;#39;, &amp;#39;OpenChat&amp;#39;, &amp;#39;Alpaca&amp;#39;]): Format of the prompt.

        Returns:
            PromptTemplate: The initialised PromptTemplate instance.
        &#34;&#34;&#34;
        from copy import deepcopy
        preset = deepcopy(presets[style])
        preset[&#39;force_real_template&#39;] = force_real_template
        return cls.from_dict(preset, template_name=style)
    
    def to_dict(self) -&gt; Dict[str, Any]:
        &#34;&#34;&#34;Export the class as a dictionary.

        Returns:
            Dict[str, Any]: Prompt format as a dictionary.
        &#34;&#34;&#34;
        return dict(
            template = self.template,
            eos_token = self.eos_token,
            bos_token = self.bos_token,
            stop = self.stop if self.stop is not None else [self.eos_token]
        )</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="llmflex.Prompts.prompt_template.PromptTemplate.from_dict"><code class="name flex">
<span>def <span class="ident">from_dict</span></span>(<span>format_dict: Dict[str, Any], template_name: Optional[str] = None) ‑> <a title="llmflex.Prompts.prompt_template.PromptTemplate" href="#llmflex.Prompts.prompt_template.PromptTemplate">PromptTemplate</a></span>
</code></dt>
<dd>
<div class="desc"><p>Initialise the prompt template from a dictionary.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>format_dict</code></strong> :&ensp;<code>Dict[str, Any]</code></dt>
<dd>Dictionary of the prompt format.</dd>
<dt><strong><code>template_name</code></strong> :&ensp;<code>Optional[str]</code>, optional</dt>
<dd>Name of the template. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="llmflex.Prompts.prompt_template.PromptTemplate" href="#llmflex.Prompts.prompt_template.PromptTemplate">PromptTemplate</a></code></dt>
<dd>The initialised PromptTemplate instance.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def from_dict(cls, format_dict: Dict[str, Any], template_name: Optional[str] = None) -&gt; PromptTemplate:
    &#34;&#34;&#34;Initialise the prompt template from a dictionary.

    Args:
        format_dict (Dict[str, Any]): Dictionary of the prompt format.
        template_name (Optional[str], optional): Name of the template. Defaults to None.

    Returns:
        PromptTemplate: The initialised PromptTemplate instance.
    &#34;&#34;&#34;
    template = cls(**format_dict)
    if template_name is not None:
        template._template_name = template_name
    return template</code></pre>
</details>
</dd>
<dt id="llmflex.Prompts.prompt_template.PromptTemplate.from_json"><code class="name flex">
<span>def <span class="ident">from_json</span></span>(<span>file_dir: str) ‑> <a title="llmflex.Prompts.prompt_template.PromptTemplate" href="#llmflex.Prompts.prompt_template.PromptTemplate">PromptTemplate</a></span>
</code></dt>
<dd>
<div class="desc"><p>Initialise the prompt template from a json file.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>file_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>json file path of the prompt format.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>PromptTemplatet</code></dt>
<dd>The initialised PromptTemplate instance.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def from_json(cls, file_dir: str) -&gt; PromptTemplate:
    &#34;&#34;&#34;Initialise the prompt template from a json file.

    Args:
        file_dir (str): json file path of the prompt format.

    Returns:
        PromptTemplatet: The initialised PromptTemplate instance.
    &#34;&#34;&#34;
    from ..utils import read_json
    return cls.from_dict(read_json(file_dir=file_dir), template_name=file_dir)</code></pre>
</details>
</dd>
<dt id="llmflex.Prompts.prompt_template.PromptTemplate.from_preset"><code class="name flex">
<span>def <span class="ident">from_preset</span></span>(<span>style: "Literal['Default', 'Llama2', 'Vicuna', 'ChatML', 'Zephyr', 'OpenChat', 'Alpaca']", force_real_template: bool = False) ‑> <a title="llmflex.Prompts.prompt_template.PromptTemplate" href="#llmflex.Prompts.prompt_template.PromptTemplate">PromptTemplate</a></span>
</code></dt>
<dd>
<div class="desc"><p>Initialise the prompt template from a preset.</p>
<h2 id="args">Args</h2>
<p>style (Literal[&#39;Default&#39;, &#39;Llama2&#39;, &#39;Vicuna&#39;, &#39;ChatML&#39;, &#39;Zephyr&#39;, &#39;OpenChat&#39;, &#39;Alpaca&#39;]): Format of the prompt.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="llmflex.Prompts.prompt_template.PromptTemplate" href="#llmflex.Prompts.prompt_template.PromptTemplate">PromptTemplate</a></code></dt>
<dd>The initialised PromptTemplate instance.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def from_preset(cls, style: Literal[&#39;Default&#39;, &#39;Llama2&#39;, &#39;Vicuna&#39;, &#39;ChatML&#39;, &#39;Zephyr&#39;, &#39;OpenChat&#39;, &#39;Alpaca&#39;], force_real_template: bool = False) -&gt; PromptTemplate:
    &#34;&#34;&#34;Initialise the prompt template from a preset.

    Args:
        style (Literal[&amp;#39;Default&amp;#39;, &amp;#39;Llama2&amp;#39;, &amp;#39;Vicuna&amp;#39;, &amp;#39;ChatML&amp;#39;, &amp;#39;Zephyr&amp;#39;, &amp;#39;OpenChat&amp;#39;, &amp;#39;Alpaca&amp;#39;]): Format of the prompt.

    Returns:
        PromptTemplate: The initialised PromptTemplate instance.
    &#34;&#34;&#34;
    from copy import deepcopy
    preset = deepcopy(presets[style])
    preset[&#39;force_real_template&#39;] = force_real_template
    return cls.from_dict(preset, template_name=style)</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="llmflex.Prompts.prompt_template.PromptTemplate.bos_token"><code class="name">var <span class="ident">bos_token</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def bos_token(self) -&gt; Optional[str]:
    return self._bos_token</code></pre>
</details>
</dd>
<dt id="llmflex.Prompts.prompt_template.PromptTemplate.eos_token"><code class="name">var <span class="ident">eos_token</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def eos_token(self) -&gt; Optional[str]:
    return self._eos_token</code></pre>
</details>
</dd>
<dt id="llmflex.Prompts.prompt_template.PromptTemplate.rendered_template"><code class="name">var <span class="ident">rendered_template</span> : jinja2.environment.Environment</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def rendered_template(self) -&gt; Environment:
    if not hasattr(self, &#39;_rendered_template&#39;):
        from jinja2 import BaseLoader
        template = self.template if self._force_real_template else self._hidden_template
        self._rendered_template = Environment(loader=BaseLoader).from_string(template)
    return self._rendered_template</code></pre>
</details>
</dd>
<dt id="llmflex.Prompts.prompt_template.PromptTemplate.stop"><code class="name">var <span class="ident">stop</span> : List[str]</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def stop(self) -&gt; List[str]:
    return self._stop if isinstance(self._stop, list) else [self.eos_token] if self.eos_token is not None else []</code></pre>
</details>
</dd>
<dt id="llmflex.Prompts.prompt_template.PromptTemplate.template"><code class="name">var <span class="ident">template</span> : str</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def template(self) -&gt; str:
    return self._template</code></pre>
</details>
</dd>
<dt id="llmflex.Prompts.prompt_template.PromptTemplate.template_name"><code class="name">var <span class="ident">template_name</span> : str</code></dt>
<dd>
<div class="desc"><p>Name of the template.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>Name of the template.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def template_name(self) -&gt; str:
    &#34;&#34;&#34;Name of the template.

    Returns:
        str: Name of the template.
    &#34;&#34;&#34;
    if not hasattr(self, &#39;_template_name&#39;):
        self._template_name = &#39;Unititled template&#39;
        for k, v in presets.items():
            if self.template == v[&#39;template&#39;]:
                self._template_name = k
                break
    return self._template_name</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="llmflex.Prompts.prompt_template.PromptTemplate.create_custom_prompt"><code class="name flex">
<span>def <span class="ident">create_custom_prompt</span></span>(<span>self, messages: List[Dict[str, str]], add_generation_prompt: bool = True) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Creating a custom prompt with your given list of messages. Each message should contain a dictionary with the key "role" and "content".</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>messages</code></strong> :&ensp;<code>List[Dict[str, str]]</code></dt>
<dd>List of messages. Each message should contain a dictionary with the key "role" and "content".</dd>
<dt><strong><code>add_generation_prompt</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to add the assistant tokens at the end of the prompt. Defaults to True.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>The full prompt given your messages.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_custom_prompt(self, messages: List[Dict[str, str]], add_generation_prompt: bool = True) -&gt; str:
    &#34;&#34;&#34;Creating a custom prompt with your given list of messages. Each message should contain a dictionary with the key &#34;role&#34; and &#34;content&#34;.

    Args:
        messages (List[Dict[str, str]]): List of messages. Each message should contain a dictionary with the key &#34;role&#34; and &#34;content&#34;.
        add_generation_prompt (bool, optional): Whether to add the assistant tokens at the end of the prompt. Defaults to True.

    Returns:
        str: The full prompt given your messages.
    &#34;&#34;&#34;
    return self.rendered_template.render(messages=messages, bos_token=self.bos_token, eos_token=self.eos_token, add_generation_prompt=add_generation_prompt)</code></pre>
</details>
</dd>
<dt id="llmflex.Prompts.prompt_template.PromptTemplate.create_prompt"><code class="name flex">
<span>def <span class="ident">create_prompt</span></span>(<span>self, user: str, system: str = 'This is a conversation between a human user and a helpful AI assistant.', history: Optional[Union[List[str], List[Tuple[str, str]]]] = None) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Creating the full chat prompt.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>user</code></strong> :&ensp;<code>str</code></dt>
<dd>Latest user input.</dd>
<dt><strong><code>system</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>System message. Defaults to DEFAULT_SYSTEM_MESSAGE.</dd>
<dt><strong><code>history</code></strong> :&ensp;<code>Optional[Union[List[str], List[Tuple[str, str]]]]</code>, optional</dt>
<dd>List of conversation history. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>The full prompt.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_prompt(self, user: str, system: str = DEFAULT_SYSTEM_MESSAGE, history: Optional[Union[List[str], List[Tuple[str, str]]]] = None) -&gt; str:
    &#34;&#34;&#34;Creating the full chat prompt.

    Args:
        user (str): Latest user input.
        system (str, optional): System message. Defaults to DEFAULT_SYSTEM_MESSAGE.
        history (Optional[Union[List[str], List[Tuple[str, str]]]], optional): List of conversation history. Defaults to None.

    Returns:
        str: The full prompt.
    &#34;&#34;&#34;
    head = [dict(role=&#39;system&#39;, content=system)] if system.strip(&#39; \n\r\t&#39;) != &#39;&#39; else []
    history = [] if history is None else history
    body = self.format_history(history=history, return_list=True)
    tail = [dict(role=&#39;user&#39;, content=user)]
    prompt = head + body + tail
    return self.rendered_template.render(messages=prompt, bos_token=self.bos_token, eos_token=self.eos_token, add_generation_prompt=True)</code></pre>
</details>
</dd>
<dt id="llmflex.Prompts.prompt_template.PromptTemplate.format_history"><code class="name flex">
<span>def <span class="ident">format_history</span></span>(<span>self, history: Union[List[str], List[Tuple[str, str]]], return_list: bool = False) ‑> Union[str, List[Dict[str, str]]]</span>
</code></dt>
<dd>
<div class="desc"><p>Formatting a list of conversation history into a full string of conversation history or a list of messages for the Jinja template to render.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>history</code></strong> :&ensp;<code>Union[List[str], List[Tuple[str, str]]]</code></dt>
<dd>List of conversation history. </dd>
<dt><strong><code>return_list</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to return a list of messages for the Jinja template to render. Defaults to False.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Union[str, List[Dict[str, str]]]</code></dt>
<dd>A full string of conversation history or a list of messages for the Jinja template to render.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def format_history(self, history: Union[List[str], List[Tuple[str, str]]], return_list: bool = False) -&gt; Union[str, List[Dict[str, str]]]:
    &#34;&#34;&#34;Formatting a list of conversation history into a full string of conversation history or a list of messages for the Jinja template to render.

    Args:
        history (Union[List[str], List[Tuple[str, str]]]): List of conversation history. 
        return_list (bool, optional): Whether to return a list of messages for the Jinja template to render. Defaults to False.

    Returns:
        Union[str, List[Dict[str, str]]]: A full string of conversation history or a list of messages for the Jinja template to render.
    &#34;&#34;&#34;
    if len(history) == 0:
        return [] if return_list else &#39;&#39;
    elif not isinstance(history[0], str):
        body = list(map(lambda x: [dict(role=&#39;user&#39;, content=x[0]), dict(role=&#39;assistant&#39;, content=x[1])], history))
        body = sum(body, [])
    else:
        length = len(history)
        is_even = length % 2 == 0
        half = int(length / 2) if is_even else int((length + 1) / 2)
        roles = [&#39;user&#39;, &#39;assistant&#39;] * half
        roles = roles if is_even else roles[1:]
        body = list(map(lambda x: dict(role=x[0], content=x[1]), list(zip(roles, history))))
    if return_list:
        return body
    return self.rendered_template.render(messages=body, bos_token=self.bos_token, eos_token=self.eos_token, add_generation_prompt=False)</code></pre>
</details>
</dd>
<dt id="llmflex.Prompts.prompt_template.PromptTemplate.to_dict"><code class="name flex">
<span>def <span class="ident">to_dict</span></span>(<span>self) ‑> Dict[str, Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Export the class as a dictionary.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Dict[str, Any]</code></dt>
<dd>Prompt format as a dictionary.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_dict(self) -&gt; Dict[str, Any]:
    &#34;&#34;&#34;Export the class as a dictionary.

    Returns:
        Dict[str, Any]: Prompt format as a dictionary.
    &#34;&#34;&#34;
    return dict(
        template = self.template,
        eos_token = self.eos_token,
        bos_token = self.bos_token,
        stop = self.stop if self.stop is not None else [self.eos_token]
    )</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="llmflex.Prompts" href="index.html">llmflex.Prompts</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="llmflex.Prompts.prompt_template.PromptTemplate" href="#llmflex.Prompts.prompt_template.PromptTemplate">PromptTemplate</a></code></h4>
<ul class="">
<li><code><a title="llmflex.Prompts.prompt_template.PromptTemplate.bos_token" href="#llmflex.Prompts.prompt_template.PromptTemplate.bos_token">bos_token</a></code></li>
<li><code><a title="llmflex.Prompts.prompt_template.PromptTemplate.create_custom_prompt" href="#llmflex.Prompts.prompt_template.PromptTemplate.create_custom_prompt">create_custom_prompt</a></code></li>
<li><code><a title="llmflex.Prompts.prompt_template.PromptTemplate.create_prompt" href="#llmflex.Prompts.prompt_template.PromptTemplate.create_prompt">create_prompt</a></code></li>
<li><code><a title="llmflex.Prompts.prompt_template.PromptTemplate.eos_token" href="#llmflex.Prompts.prompt_template.PromptTemplate.eos_token">eos_token</a></code></li>
<li><code><a title="llmflex.Prompts.prompt_template.PromptTemplate.format_history" href="#llmflex.Prompts.prompt_template.PromptTemplate.format_history">format_history</a></code></li>
<li><code><a title="llmflex.Prompts.prompt_template.PromptTemplate.from_dict" href="#llmflex.Prompts.prompt_template.PromptTemplate.from_dict">from_dict</a></code></li>
<li><code><a title="llmflex.Prompts.prompt_template.PromptTemplate.from_json" href="#llmflex.Prompts.prompt_template.PromptTemplate.from_json">from_json</a></code></li>
<li><code><a title="llmflex.Prompts.prompt_template.PromptTemplate.from_preset" href="#llmflex.Prompts.prompt_template.PromptTemplate.from_preset">from_preset</a></code></li>
<li><code><a title="llmflex.Prompts.prompt_template.PromptTemplate.rendered_template" href="#llmflex.Prompts.prompt_template.PromptTemplate.rendered_template">rendered_template</a></code></li>
<li><code><a title="llmflex.Prompts.prompt_template.PromptTemplate.stop" href="#llmflex.Prompts.prompt_template.PromptTemplate.stop">stop</a></code></li>
<li><code><a title="llmflex.Prompts.prompt_template.PromptTemplate.template" href="#llmflex.Prompts.prompt_template.PromptTemplate.template">template</a></code></li>
<li><code><a title="llmflex.Prompts.prompt_template.PromptTemplate.template_name" href="#llmflex.Prompts.prompt_template.PromptTemplate.template_name">template_name</a></code></li>
<li><code><a title="llmflex.Prompts.prompt_template.PromptTemplate.to_dict" href="#llmflex.Prompts.prompt_template.PromptTemplate.to_dict">to_dict</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>