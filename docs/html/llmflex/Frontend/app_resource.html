<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>llmflex.Frontend.app_resource API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>llmflex.Frontend.app_resource</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from ..Models.Factory.llm_factory import LlmFactory
from ..Models.Cores.base_core import BaseLLM
from ..Embeddings.base_embeddings import BaseEmbeddingsToolkit
from ..TextSplitters.base_text_splitter import BaseTextSplitter
from ..Rankers.base_ranker import BaseRanker
from ..Tools.tool_utils import BaseTool, ToolSelector, normalise_tool_name
from ..Prompts.prompt_template import presets, PromptTemplate
from ..Memory.long_short_memory import LongShortTermChatMemory
from ..Memory.base_memory import list_titles, list_chat_ids, get_new_chat_id, get_title_from_id, get_dir_from_id
from typing import List, Type, Dict, Any, Union, Optional

class AppBackend:
    &#34;&#34;&#34;Resources for the App to share.
    &#34;&#34;&#34;
    def __init__(self, config: Dict[str, Any]) -&gt; None:
        &#34;&#34;&#34;Initialise the backend resourses.
        Args:
            config (Dict[str, Any]): Configuration of all the resources.
        &#34;&#34;&#34;
        self._config = config
        self._init_llm_factory()
        self._init_ranker()
        self._init_text_splitter()
        self._init_embeddings()
        self._init_tools()
        
    @property
    def config(self) -&gt; Dict[str, Any]:
        &#34;&#34;&#34;Configuration of all the resources.ry_

        Returns:
            Dict[str, Any]: Configuration of all the resources.
        &#34;&#34;&#34;
        return self._config
    
    @property
    def factory(self) -&gt; LlmFactory:
        &#34;&#34;&#34;LLM factory.

        Returns:
            LlmFactory: LLM factory.
        &#34;&#34;&#34;
        return self._factory
    
    @property
    def llm(self) -&gt; BaseLLM:
        &#34;&#34;&#34;LLM.

        Returns:
            BaseLLM: LLM.
        &#34;&#34;&#34;
        if not hasattr(self, &#39;_llm&#39;):
            self._llm = self.factory()
        return self._llm
    
    @property
    def ranker(self) -&gt; BaseRanker:
        &#34;&#34;&#34;Reranker.

        Returns:
            BaseRanker: Reranker.
        &#34;&#34;&#34;
        return self._ranker
    
    @property
    def text_splitter(self) -&gt; BaseTextSplitter:
        &#34;&#34;&#34;Text splitter.

        Returns:
            BaseTextSplitter: Text splitter.
        &#34;&#34;&#34;
        return self._text_splitter
    
    @property
    def embeddings(self) -&gt; BaseEmbeddingsToolkit:
        &#34;&#34;&#34;Embeddings toolkit.

        Returns:
            BaseEmbeddingsToolkit: Embeddings toolkit.
        &#34;&#34;&#34;
        return self._embeddings
    
    @property
    def tool_selector(self) -&gt; ToolSelector:
        &#34;&#34;&#34;Tool selector.

        Returns:
            ToolSelector: Tool selector.
        &#34;&#34;&#34;
        return self._tool_selector
    
    @property
    def prompt_template(self) -&gt; PromptTemplate:
        &#34;&#34;&#34;Prompt template.

        Returns:
            PromptTemplate: Prompt template.
        &#34;&#34;&#34;
        if not hasattr(self, &#39;_prompt_template&#39;):
            self._prompt_template = self.factory.prompt_template
        return self._prompt_template

    @property
    def memory(self) -&gt; LongShortTermChatMemory:
        &#34;&#34;&#34;Current chat memory.

        Returns:
            LongShortTermChatMemory: Current chat memory.
        &#34;&#34;&#34;
        if not hasattr(self, &#39;_memory&#39;):
            self.create_memory()
        return self._memory
    
    @property
    def generation_config(self) -&gt; Dict[str, float]:
        &#34;&#34;&#34;Text generation config.

        Returns:
            Dict[str, float]: Text generation config.
        &#34;&#34;&#34;
        if not hasattr(self, &#39;_generation_config&#39;):
            self._generation_config = dict(
            temperature = 0.8,
            max_new_tokens = 1024,
            top_p  = 0.95,
            top_k = 40,
            repetition_penalty = 1.1
        )
        return self._generation_config

    @property
    def memory_config(self) -&gt; Dict[str, float]:
        if not hasattr(self, &#39;_memory_config&#39;):
            self._memory_config = dict(
                recent_token_limit = 600, 
                relevant_token_limit= 500,
                relevance_score_threshold = 0.8, 
                similarity_score_threshold = 0.5
            )
        return self._memory_config

    @property
    def tool_status(self) -&gt; Dict[str, bool]:
        &#34;&#34;&#34;Whether each tool is on or off.

        Returns:
            Dict[str, bool]: Whether each tool is on or off.
        &#34;&#34;&#34;
        from copy import deepcopy
        import gc
        status = deepcopy(self.tool_selector._enabled)
        status.pop(&#39;direct_response&#39;)
        gc.collect()
        return status

    @property
    def has_tools(self) -&gt; bool:
        &#34;&#34;&#34;Whether any tools exist in the tool selector.

        Returns:
            bool: Whether any tools exist in the tool selector.
        &#34;&#34;&#34;
        if not hasattr(self, &#39;_has_tools&#39;):
            self._has_tools = len(self.tool_status) &gt; 0
        return self._has_tools

    def _init_llm_factory(self) -&gt; None:
        &#34;&#34;&#34;Initialising the llm factory.
        &#34;&#34;&#34;
        self._factory = LlmFactory(**self.config[&#39;model&#39;])

    def _init_ranker(self) -&gt; None:
        &#34;&#34;&#34;Initialise the reranker.
        &#34;&#34;&#34;
        from .. import Rankers
        from copy import deepcopy
        config = deepcopy(self.config.get(&#39;ranker&#39;, dict(class_name=&#39;FlashrankRanker&#39;)))
        class_name = config.pop(&#39;class_name&#39;, &#39;FlashrankRanker&#39;)
        rclass = Rankers.__dict__[class_name]
        self._ranker = rclass(**config)

    def _init_text_splitter(self) -&gt; None:
        &#34;&#34;&#34;Initialise the text splitter.
        &#34;&#34;&#34;
        from .. import TextSplitters
        from copy import deepcopy
        config = deepcopy(self.config.get(&#39;text_splitter&#39;, dict(class_name=&#39;SentenceTokenTextSplitter&#39;, count_token_fn=&#39;default&#39;)))
        class_name = config.pop(&#39;class_name&#39;, &#39;SentenceTokenTextSplitter&#39;)
        if config.get(&#39;count_token_fn&#39;) == &#39;default&#39;:
            config[&#39;count_token_fn&#39;] = self.llm.get_num_tokens
        rclass = TextSplitters.__dict__[class_name]
        self._text_splitter = rclass(**config)      

    def _init_embeddings(self) -&gt; None:
        &#34;&#34;&#34;Initialising the embeddings toolkit.
        &#34;&#34;&#34;
        from .. import Embeddings
        from copy import deepcopy
        config = deepcopy(self.config.get(&#39;embeddings&#39;, dict(class_name=&#39;HuggingfaceEmbeddingsToolkit&#39;, model_id=&#39;&#39;)))
        class_name = config.pop(&#39;class_name&#39;, &#39;HuggingfaceEmbeddingsToolkit&#39;)
        if ((class_name == &#39;HuggingfaceEmbeddingsToolkit&#39;) &amp; (&#39;model_id&#39; not in config.keys())):
            config[&#39;model_id&#39;] = &#39;&#39;
        eclass = Embeddings.__dict__[class_name]
        self._embeddings= eclass(**config)

    def _init_tools(self) -&gt; None:
        &#34;&#34;&#34;Initialising tool selector.
        &#34;&#34;&#34;
        from ..Tools import tool_classes
        from copy import deepcopy
        from inspect import isclass
        config = deepcopy(self.config.get(&#39;tools&#39;, []))
        tool_classes =  list(map(lambda x: tool_classes.__dict__.get(x[&#39;class_name&#39;]), config))
        tool_names = []
        tool_instances = []
        for i, cf in enumerate(config):
            tclass = tool_classes[i]
            tool_name = normalise_tool_name(cf.pop(&#39;class_name&#39;))
            if isclass(tclass):
                if issubclass(tclass, BaseTool):
                    if cf.get(&#39;llm&#39;) == &#39;default&#39;:
                        cf[&#39;llm&#39;] = self.llm
                    if cf.get(&#39;embeddings&#39;) == &#39;default&#39;:
                        cf[&#39;embeddings&#39;] = self.embeddings
                    if cf.get(&#39;ranker&#39;) == &#39;default&#39;:
                        cf[&#39;ranker&#39;] = self.ranker
                    if cf.get(&#39;text_splitter&#39;) == &#39;default&#39;:
                        cf[&#39;text_splitter&#39;] = self.text_splitter
                    if cf.get(&#39;count_token_fn&#39;) == &#39;default&#39;:
                        cf[&#39;count_token_fn&#39;] = self.llm.get_num_tokens
                    tool = tclass(**cf)
                    if tool.name not in tool_names:
                        tool_instances.append(tool)
                        tool_names.append(tool.name)
            elif tool_name not in tool_names:
                tool_instances.append(tclass)
                tool_names.append(tool_name)
        self._tool_selector = ToolSelector(tools=tool_instances)
        self.tool_selector.turn_off_tools(self.tool_selector.enabled_tools)

    def switch_memory(self, chat_id: str) -&gt; None:
        &#34;&#34;&#34;Switch to the memory given the chat ID.

        Args:
            chat_id (str): Chat ID.
        &#34;&#34;&#34;
        if chat_id in list_chat_ids():
            self._memory = LongShortTermChatMemory(chat_id=chat_id, 
                    embeddings=self.embeddings, 
                    llm=self.llm, 
                    ranker=self.ranker, 
                    text_splitter=self.text_splitter,
                    from_exist=True)
            
    def create_memory(self) -&gt; None:
        &#34;&#34;&#34;Create a new chat memory.
        &#34;&#34;&#34;
        self._memory = LongShortTermChatMemory(chat_id=get_new_chat_id(), 
                embeddings=self.embeddings, 
                llm=self.llm, 
                ranker=self.ranker, 
                text_splitter=self.text_splitter)

    def drop_memory(self, chat_id: str) -&gt; None:
        &#34;&#34;&#34;Delete the chat memory give the chat ID.

        Args:
            chat_id (str): Chat ID.
        &#34;&#34;&#34;
        import shutil
        if chat_id in list_chat_ids():
            if chat_id == self.memory.chat_id:
                self.create_memory()
            chat_dir = get_dir_from_id(chat_id=chat_id)
            shutil.rmtree(chat_dir)

    def set_generation_config(self, temperature: Optional[float] = None, max_new_tokens: Optional[int] = None, 
                top_p: Optional[float] = None, top_k: Optional[int] = None, repetition_penalty: Optional[float] = None) -&gt; None:
        &#34;&#34;&#34;Update the LLM generation config. If None is given to any arguments, the argument will not change.

        Args:
            temperature (Optional[float], optional): Set how &#34;creative&#34; the model is, the smaller it is, the more static of the output. Defaults to None.
            max_new_tokens (Optional[int], optional): Maximum number of tokens to generate by the llm. Defaults to None.
            top_p (Optional[float], optional): While sampling the next token, only consider the tokens above this p value. Defaults to None.
            top_k (Optional[int], optional): While sampling the next token, only consider the top &#34;top_k&#34; tokens. Defaults to None.
            repetition_penalty (Optional[float], optional): The value to penalise the model for generating repetitive text. Defaults to None.
        &#34;&#34;&#34;
        args = [temperature, max_new_tokens, top_p, top_k, repetition_penalty]
        arg_names = [&#39;temperature&#39;, &#39;max_new_tokens&#39;, &#39;top_p&#39;, &#39;top_k&#39;, &#39;repetition_penalty&#39;]
        for i, arg in enumerate(args):
            if arg is not None:
                self._generation_config[arg_names[i]] = arg

    def set_memory_config(self, recent_token_limit: Optional[int] = None, relevant_token_limit: Optional[int] = None, 
                relevance_score_threshold: Optional[float] = None, similarity_score_threshold: Optional[float] = None) -&gt; None:
        &#34;&#34;&#34;Update the memory config. If None is given to any arguments, the argument will not change.

        Args:
        recent_token_limit (Optional[int], optional): Token limit for the most recent conversation history. Defaults to None.
        relevant_token_limit (Optional[int], optional): Token limit for the relevant contents from older conversation history. Defaults to None.
        relevance_score_threshold (Optional[float], optional): Score threshold for the reranker for relevant conversation history content extraction. Defaults to None.
        similarity_score_threshold (Optional[float], optional): Score threshold for the vector database search for relevant conversation history content extraction. Defaults to None.
        &#34;&#34;&#34;
        args = [recent_token_limit, relevant_token_limit, relevance_score_threshold, similarity_score_threshold]
        arg_names = [&#39;recent_token_limit&#39;, &#39;relevant_token_limit&#39;, &#39;relevance_score_threshold&#39;, &#39;similarity_score_threshold&#39;]
        for i, arg in enumerate(args):
            if arg is not None:
                self._memory_config[arg_names[i]] = arg

    def set_system_message(self, system: str) -&gt; None:
        &#34;&#34;&#34;Update the system message of the current conversation.

        Args:
            system (str): Update the system message of the current conversation.
        &#34;&#34;&#34;
        self.memory.update_system_message(system=system)

    def set_prompt_template(self, preset: str) -&gt; None:
        &#34;&#34;&#34;Updating prompt template.

        Args:
            preset (str): Preset name of the prompt template.
        &#34;&#34;&#34;
        if preset in presets.keys():
            self._prompt_template = PromptTemplate.from_preset(style=preset)

    def toggle_tool(self, tool_name: str) -&gt; None:
        &#34;&#34;&#34;Toggle the on/off status of the given tool.

        Args:
            tool_name (str): Tool to toggle.
        &#34;&#34;&#34;
        if tool_name in self.tool_status.keys():
            self.tool_selector._enabled[tool_name] = not self.tool_selector._enabled[tool_name]</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="llmflex.Frontend.app_resource.AppBackend"><code class="flex name class">
<span>class <span class="ident">AppBackend</span></span>
<span>(</span><span>config: Dict[str, Any])</span>
</code></dt>
<dd>
<div class="desc"><p>Resources for the App to share.</p>
<p>Initialise the backend resourses.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>config</code></strong> :&ensp;<code>Dict[str, Any]</code></dt>
<dd>Configuration of all the resources.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AppBackend:
    &#34;&#34;&#34;Resources for the App to share.
    &#34;&#34;&#34;
    def __init__(self, config: Dict[str, Any]) -&gt; None:
        &#34;&#34;&#34;Initialise the backend resourses.
        Args:
            config (Dict[str, Any]): Configuration of all the resources.
        &#34;&#34;&#34;
        self._config = config
        self._init_llm_factory()
        self._init_ranker()
        self._init_text_splitter()
        self._init_embeddings()
        self._init_tools()
        
    @property
    def config(self) -&gt; Dict[str, Any]:
        &#34;&#34;&#34;Configuration of all the resources.ry_

        Returns:
            Dict[str, Any]: Configuration of all the resources.
        &#34;&#34;&#34;
        return self._config
    
    @property
    def factory(self) -&gt; LlmFactory:
        &#34;&#34;&#34;LLM factory.

        Returns:
            LlmFactory: LLM factory.
        &#34;&#34;&#34;
        return self._factory
    
    @property
    def llm(self) -&gt; BaseLLM:
        &#34;&#34;&#34;LLM.

        Returns:
            BaseLLM: LLM.
        &#34;&#34;&#34;
        if not hasattr(self, &#39;_llm&#39;):
            self._llm = self.factory()
        return self._llm
    
    @property
    def ranker(self) -&gt; BaseRanker:
        &#34;&#34;&#34;Reranker.

        Returns:
            BaseRanker: Reranker.
        &#34;&#34;&#34;
        return self._ranker
    
    @property
    def text_splitter(self) -&gt; BaseTextSplitter:
        &#34;&#34;&#34;Text splitter.

        Returns:
            BaseTextSplitter: Text splitter.
        &#34;&#34;&#34;
        return self._text_splitter
    
    @property
    def embeddings(self) -&gt; BaseEmbeddingsToolkit:
        &#34;&#34;&#34;Embeddings toolkit.

        Returns:
            BaseEmbeddingsToolkit: Embeddings toolkit.
        &#34;&#34;&#34;
        return self._embeddings
    
    @property
    def tool_selector(self) -&gt; ToolSelector:
        &#34;&#34;&#34;Tool selector.

        Returns:
            ToolSelector: Tool selector.
        &#34;&#34;&#34;
        return self._tool_selector
    
    @property
    def prompt_template(self) -&gt; PromptTemplate:
        &#34;&#34;&#34;Prompt template.

        Returns:
            PromptTemplate: Prompt template.
        &#34;&#34;&#34;
        if not hasattr(self, &#39;_prompt_template&#39;):
            self._prompt_template = self.factory.prompt_template
        return self._prompt_template

    @property
    def memory(self) -&gt; LongShortTermChatMemory:
        &#34;&#34;&#34;Current chat memory.

        Returns:
            LongShortTermChatMemory: Current chat memory.
        &#34;&#34;&#34;
        if not hasattr(self, &#39;_memory&#39;):
            self.create_memory()
        return self._memory
    
    @property
    def generation_config(self) -&gt; Dict[str, float]:
        &#34;&#34;&#34;Text generation config.

        Returns:
            Dict[str, float]: Text generation config.
        &#34;&#34;&#34;
        if not hasattr(self, &#39;_generation_config&#39;):
            self._generation_config = dict(
            temperature = 0.8,
            max_new_tokens = 1024,
            top_p  = 0.95,
            top_k = 40,
            repetition_penalty = 1.1
        )
        return self._generation_config

    @property
    def memory_config(self) -&gt; Dict[str, float]:
        if not hasattr(self, &#39;_memory_config&#39;):
            self._memory_config = dict(
                recent_token_limit = 600, 
                relevant_token_limit= 500,
                relevance_score_threshold = 0.8, 
                similarity_score_threshold = 0.5
            )
        return self._memory_config

    @property
    def tool_status(self) -&gt; Dict[str, bool]:
        &#34;&#34;&#34;Whether each tool is on or off.

        Returns:
            Dict[str, bool]: Whether each tool is on or off.
        &#34;&#34;&#34;
        from copy import deepcopy
        import gc
        status = deepcopy(self.tool_selector._enabled)
        status.pop(&#39;direct_response&#39;)
        gc.collect()
        return status

    @property
    def has_tools(self) -&gt; bool:
        &#34;&#34;&#34;Whether any tools exist in the tool selector.

        Returns:
            bool: Whether any tools exist in the tool selector.
        &#34;&#34;&#34;
        if not hasattr(self, &#39;_has_tools&#39;):
            self._has_tools = len(self.tool_status) &gt; 0
        return self._has_tools

    def _init_llm_factory(self) -&gt; None:
        &#34;&#34;&#34;Initialising the llm factory.
        &#34;&#34;&#34;
        self._factory = LlmFactory(**self.config[&#39;model&#39;])

    def _init_ranker(self) -&gt; None:
        &#34;&#34;&#34;Initialise the reranker.
        &#34;&#34;&#34;
        from .. import Rankers
        from copy import deepcopy
        config = deepcopy(self.config.get(&#39;ranker&#39;, dict(class_name=&#39;FlashrankRanker&#39;)))
        class_name = config.pop(&#39;class_name&#39;, &#39;FlashrankRanker&#39;)
        rclass = Rankers.__dict__[class_name]
        self._ranker = rclass(**config)

    def _init_text_splitter(self) -&gt; None:
        &#34;&#34;&#34;Initialise the text splitter.
        &#34;&#34;&#34;
        from .. import TextSplitters
        from copy import deepcopy
        config = deepcopy(self.config.get(&#39;text_splitter&#39;, dict(class_name=&#39;SentenceTokenTextSplitter&#39;, count_token_fn=&#39;default&#39;)))
        class_name = config.pop(&#39;class_name&#39;, &#39;SentenceTokenTextSplitter&#39;)
        if config.get(&#39;count_token_fn&#39;) == &#39;default&#39;:
            config[&#39;count_token_fn&#39;] = self.llm.get_num_tokens
        rclass = TextSplitters.__dict__[class_name]
        self._text_splitter = rclass(**config)      

    def _init_embeddings(self) -&gt; None:
        &#34;&#34;&#34;Initialising the embeddings toolkit.
        &#34;&#34;&#34;
        from .. import Embeddings
        from copy import deepcopy
        config = deepcopy(self.config.get(&#39;embeddings&#39;, dict(class_name=&#39;HuggingfaceEmbeddingsToolkit&#39;, model_id=&#39;&#39;)))
        class_name = config.pop(&#39;class_name&#39;, &#39;HuggingfaceEmbeddingsToolkit&#39;)
        if ((class_name == &#39;HuggingfaceEmbeddingsToolkit&#39;) &amp; (&#39;model_id&#39; not in config.keys())):
            config[&#39;model_id&#39;] = &#39;&#39;
        eclass = Embeddings.__dict__[class_name]
        self._embeddings= eclass(**config)

    def _init_tools(self) -&gt; None:
        &#34;&#34;&#34;Initialising tool selector.
        &#34;&#34;&#34;
        from ..Tools import tool_classes
        from copy import deepcopy
        from inspect import isclass
        config = deepcopy(self.config.get(&#39;tools&#39;, []))
        tool_classes =  list(map(lambda x: tool_classes.__dict__.get(x[&#39;class_name&#39;]), config))
        tool_names = []
        tool_instances = []
        for i, cf in enumerate(config):
            tclass = tool_classes[i]
            tool_name = normalise_tool_name(cf.pop(&#39;class_name&#39;))
            if isclass(tclass):
                if issubclass(tclass, BaseTool):
                    if cf.get(&#39;llm&#39;) == &#39;default&#39;:
                        cf[&#39;llm&#39;] = self.llm
                    if cf.get(&#39;embeddings&#39;) == &#39;default&#39;:
                        cf[&#39;embeddings&#39;] = self.embeddings
                    if cf.get(&#39;ranker&#39;) == &#39;default&#39;:
                        cf[&#39;ranker&#39;] = self.ranker
                    if cf.get(&#39;text_splitter&#39;) == &#39;default&#39;:
                        cf[&#39;text_splitter&#39;] = self.text_splitter
                    if cf.get(&#39;count_token_fn&#39;) == &#39;default&#39;:
                        cf[&#39;count_token_fn&#39;] = self.llm.get_num_tokens
                    tool = tclass(**cf)
                    if tool.name not in tool_names:
                        tool_instances.append(tool)
                        tool_names.append(tool.name)
            elif tool_name not in tool_names:
                tool_instances.append(tclass)
                tool_names.append(tool_name)
        self._tool_selector = ToolSelector(tools=tool_instances)
        self.tool_selector.turn_off_tools(self.tool_selector.enabled_tools)

    def switch_memory(self, chat_id: str) -&gt; None:
        &#34;&#34;&#34;Switch to the memory given the chat ID.

        Args:
            chat_id (str): Chat ID.
        &#34;&#34;&#34;
        if chat_id in list_chat_ids():
            self._memory = LongShortTermChatMemory(chat_id=chat_id, 
                    embeddings=self.embeddings, 
                    llm=self.llm, 
                    ranker=self.ranker, 
                    text_splitter=self.text_splitter,
                    from_exist=True)
            
    def create_memory(self) -&gt; None:
        &#34;&#34;&#34;Create a new chat memory.
        &#34;&#34;&#34;
        self._memory = LongShortTermChatMemory(chat_id=get_new_chat_id(), 
                embeddings=self.embeddings, 
                llm=self.llm, 
                ranker=self.ranker, 
                text_splitter=self.text_splitter)

    def drop_memory(self, chat_id: str) -&gt; None:
        &#34;&#34;&#34;Delete the chat memory give the chat ID.

        Args:
            chat_id (str): Chat ID.
        &#34;&#34;&#34;
        import shutil
        if chat_id in list_chat_ids():
            if chat_id == self.memory.chat_id:
                self.create_memory()
            chat_dir = get_dir_from_id(chat_id=chat_id)
            shutil.rmtree(chat_dir)

    def set_generation_config(self, temperature: Optional[float] = None, max_new_tokens: Optional[int] = None, 
                top_p: Optional[float] = None, top_k: Optional[int] = None, repetition_penalty: Optional[float] = None) -&gt; None:
        &#34;&#34;&#34;Update the LLM generation config. If None is given to any arguments, the argument will not change.

        Args:
            temperature (Optional[float], optional): Set how &#34;creative&#34; the model is, the smaller it is, the more static of the output. Defaults to None.
            max_new_tokens (Optional[int], optional): Maximum number of tokens to generate by the llm. Defaults to None.
            top_p (Optional[float], optional): While sampling the next token, only consider the tokens above this p value. Defaults to None.
            top_k (Optional[int], optional): While sampling the next token, only consider the top &#34;top_k&#34; tokens. Defaults to None.
            repetition_penalty (Optional[float], optional): The value to penalise the model for generating repetitive text. Defaults to None.
        &#34;&#34;&#34;
        args = [temperature, max_new_tokens, top_p, top_k, repetition_penalty]
        arg_names = [&#39;temperature&#39;, &#39;max_new_tokens&#39;, &#39;top_p&#39;, &#39;top_k&#39;, &#39;repetition_penalty&#39;]
        for i, arg in enumerate(args):
            if arg is not None:
                self._generation_config[arg_names[i]] = arg

    def set_memory_config(self, recent_token_limit: Optional[int] = None, relevant_token_limit: Optional[int] = None, 
                relevance_score_threshold: Optional[float] = None, similarity_score_threshold: Optional[float] = None) -&gt; None:
        &#34;&#34;&#34;Update the memory config. If None is given to any arguments, the argument will not change.

        Args:
        recent_token_limit (Optional[int], optional): Token limit for the most recent conversation history. Defaults to None.
        relevant_token_limit (Optional[int], optional): Token limit for the relevant contents from older conversation history. Defaults to None.
        relevance_score_threshold (Optional[float], optional): Score threshold for the reranker for relevant conversation history content extraction. Defaults to None.
        similarity_score_threshold (Optional[float], optional): Score threshold for the vector database search for relevant conversation history content extraction. Defaults to None.
        &#34;&#34;&#34;
        args = [recent_token_limit, relevant_token_limit, relevance_score_threshold, similarity_score_threshold]
        arg_names = [&#39;recent_token_limit&#39;, &#39;relevant_token_limit&#39;, &#39;relevance_score_threshold&#39;, &#39;similarity_score_threshold&#39;]
        for i, arg in enumerate(args):
            if arg is not None:
                self._memory_config[arg_names[i]] = arg

    def set_system_message(self, system: str) -&gt; None:
        &#34;&#34;&#34;Update the system message of the current conversation.

        Args:
            system (str): Update the system message of the current conversation.
        &#34;&#34;&#34;
        self.memory.update_system_message(system=system)

    def set_prompt_template(self, preset: str) -&gt; None:
        &#34;&#34;&#34;Updating prompt template.

        Args:
            preset (str): Preset name of the prompt template.
        &#34;&#34;&#34;
        if preset in presets.keys():
            self._prompt_template = PromptTemplate.from_preset(style=preset)

    def toggle_tool(self, tool_name: str) -&gt; None:
        &#34;&#34;&#34;Toggle the on/off status of the given tool.

        Args:
            tool_name (str): Tool to toggle.
        &#34;&#34;&#34;
        if tool_name in self.tool_status.keys():
            self.tool_selector._enabled[tool_name] = not self.tool_selector._enabled[tool_name]</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="llmflex.Frontend.app_resource.AppBackend.config"><code class="name">var <span class="ident">config</span> : Dict[str, Any]</code></dt>
<dd>
<div class="desc"><p>Configuration of all the resources.ry_</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Dict[str, Any]</code></dt>
<dd>Configuration of all the resources.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def config(self) -&gt; Dict[str, Any]:
    &#34;&#34;&#34;Configuration of all the resources.ry_

    Returns:
        Dict[str, Any]: Configuration of all the resources.
    &#34;&#34;&#34;
    return self._config</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.app_resource.AppBackend.embeddings"><code class="name">var <span class="ident">embeddings</span> : <a title="llmflex.Embeddings.base_embeddings.BaseEmbeddingsToolkit" href="../Embeddings/base_embeddings.html#llmflex.Embeddings.base_embeddings.BaseEmbeddingsToolkit">BaseEmbeddingsToolkit</a></code></dt>
<dd>
<div class="desc"><p>Embeddings toolkit.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>BaseEmbeddingsToolkit</code></dt>
<dd>Embeddings toolkit.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def embeddings(self) -&gt; BaseEmbeddingsToolkit:
    &#34;&#34;&#34;Embeddings toolkit.

    Returns:
        BaseEmbeddingsToolkit: Embeddings toolkit.
    &#34;&#34;&#34;
    return self._embeddings</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.app_resource.AppBackend.factory"><code class="name">var <span class="ident">factory</span> : <a title="llmflex.Models.Factory.llm_factory.LlmFactory" href="../Models/Factory/llm_factory.html#llmflex.Models.Factory.llm_factory.LlmFactory">LlmFactory</a></code></dt>
<dd>
<div class="desc"><p>LLM factory.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>LlmFactory</code></dt>
<dd>LLM factory.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def factory(self) -&gt; LlmFactory:
    &#34;&#34;&#34;LLM factory.

    Returns:
        LlmFactory: LLM factory.
    &#34;&#34;&#34;
    return self._factory</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.app_resource.AppBackend.generation_config"><code class="name">var <span class="ident">generation_config</span> : Dict[str, float]</code></dt>
<dd>
<div class="desc"><p>Text generation config.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Dict[str, float]</code></dt>
<dd>Text generation config.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def generation_config(self) -&gt; Dict[str, float]:
    &#34;&#34;&#34;Text generation config.

    Returns:
        Dict[str, float]: Text generation config.
    &#34;&#34;&#34;
    if not hasattr(self, &#39;_generation_config&#39;):
        self._generation_config = dict(
        temperature = 0.8,
        max_new_tokens = 1024,
        top_p  = 0.95,
        top_k = 40,
        repetition_penalty = 1.1
    )
    return self._generation_config</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.app_resource.AppBackend.has_tools"><code class="name">var <span class="ident">has_tools</span> : bool</code></dt>
<dd>
<div class="desc"><p>Whether any tools exist in the tool selector.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd>Whether any tools exist in the tool selector.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def has_tools(self) -&gt; bool:
    &#34;&#34;&#34;Whether any tools exist in the tool selector.

    Returns:
        bool: Whether any tools exist in the tool selector.
    &#34;&#34;&#34;
    if not hasattr(self, &#39;_has_tools&#39;):
        self._has_tools = len(self.tool_status) &gt; 0
    return self._has_tools</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.app_resource.AppBackend.llm"><code class="name">var <span class="ident">llm</span> : <a title="llmflex.Models.Cores.base_core.BaseLLM" href="../Models/Cores/base_core.html#llmflex.Models.Cores.base_core.BaseLLM">BaseLLM</a></code></dt>
<dd>
<div class="desc"><p>LLM.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>BaseLLM</code></dt>
<dd>LLM.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def llm(self) -&gt; BaseLLM:
    &#34;&#34;&#34;LLM.

    Returns:
        BaseLLM: LLM.
    &#34;&#34;&#34;
    if not hasattr(self, &#39;_llm&#39;):
        self._llm = self.factory()
    return self._llm</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.app_resource.AppBackend.memory"><code class="name">var <span class="ident">memory</span> : <a title="llmflex.Memory.long_short_memory.LongShortTermChatMemory" href="../Memory/long_short_memory.html#llmflex.Memory.long_short_memory.LongShortTermChatMemory">LongShortTermChatMemory</a></code></dt>
<dd>
<div class="desc"><p>Current chat memory.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>LongShortTermChatMemory</code></dt>
<dd>Current chat memory.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def memory(self) -&gt; LongShortTermChatMemory:
    &#34;&#34;&#34;Current chat memory.

    Returns:
        LongShortTermChatMemory: Current chat memory.
    &#34;&#34;&#34;
    if not hasattr(self, &#39;_memory&#39;):
        self.create_memory()
    return self._memory</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.app_resource.AppBackend.memory_config"><code class="name">var <span class="ident">memory_config</span> : Dict[str, float]</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def memory_config(self) -&gt; Dict[str, float]:
    if not hasattr(self, &#39;_memory_config&#39;):
        self._memory_config = dict(
            recent_token_limit = 600, 
            relevant_token_limit= 500,
            relevance_score_threshold = 0.8, 
            similarity_score_threshold = 0.5
        )
    return self._memory_config</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.app_resource.AppBackend.prompt_template"><code class="name">var <span class="ident">prompt_template</span> : <a title="llmflex.Prompts.prompt_template.PromptTemplate" href="../Prompts/prompt_template.html#llmflex.Prompts.prompt_template.PromptTemplate">PromptTemplate</a></code></dt>
<dd>
<div class="desc"><p>Prompt template.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>PromptTemplate</code></dt>
<dd>Prompt template.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def prompt_template(self) -&gt; PromptTemplate:
    &#34;&#34;&#34;Prompt template.

    Returns:
        PromptTemplate: Prompt template.
    &#34;&#34;&#34;
    if not hasattr(self, &#39;_prompt_template&#39;):
        self._prompt_template = self.factory.prompt_template
    return self._prompt_template</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.app_resource.AppBackend.ranker"><code class="name">var <span class="ident">ranker</span> : <a title="llmflex.Rankers.base_ranker.BaseRanker" href="../Rankers/base_ranker.html#llmflex.Rankers.base_ranker.BaseRanker">BaseRanker</a></code></dt>
<dd>
<div class="desc"><p>Reranker.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>BaseRanker</code></dt>
<dd>Reranker.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def ranker(self) -&gt; BaseRanker:
    &#34;&#34;&#34;Reranker.

    Returns:
        BaseRanker: Reranker.
    &#34;&#34;&#34;
    return self._ranker</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.app_resource.AppBackend.text_splitter"><code class="name">var <span class="ident">text_splitter</span> : <a title="llmflex.TextSplitters.base_text_splitter.BaseTextSplitter" href="../TextSplitters/base_text_splitter.html#llmflex.TextSplitters.base_text_splitter.BaseTextSplitter">BaseTextSplitter</a></code></dt>
<dd>
<div class="desc"><p>Text splitter.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>BaseTextSplitter</code></dt>
<dd>Text splitter.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def text_splitter(self) -&gt; BaseTextSplitter:
    &#34;&#34;&#34;Text splitter.

    Returns:
        BaseTextSplitter: Text splitter.
    &#34;&#34;&#34;
    return self._text_splitter</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.app_resource.AppBackend.tool_selector"><code class="name">var <span class="ident">tool_selector</span> : <a title="llmflex.Tools.tool_utils.ToolSelector" href="../Tools/tool_utils.html#llmflex.Tools.tool_utils.ToolSelector">ToolSelector</a></code></dt>
<dd>
<div class="desc"><p>Tool selector.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ToolSelector</code></dt>
<dd>Tool selector.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def tool_selector(self) -&gt; ToolSelector:
    &#34;&#34;&#34;Tool selector.

    Returns:
        ToolSelector: Tool selector.
    &#34;&#34;&#34;
    return self._tool_selector</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.app_resource.AppBackend.tool_status"><code class="name">var <span class="ident">tool_status</span> : Dict[str, bool]</code></dt>
<dd>
<div class="desc"><p>Whether each tool is on or off.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Dict[str, bool]</code></dt>
<dd>Whether each tool is on or off.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def tool_status(self) -&gt; Dict[str, bool]:
    &#34;&#34;&#34;Whether each tool is on or off.

    Returns:
        Dict[str, bool]: Whether each tool is on or off.
    &#34;&#34;&#34;
    from copy import deepcopy
    import gc
    status = deepcopy(self.tool_selector._enabled)
    status.pop(&#39;direct_response&#39;)
    gc.collect()
    return status</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="llmflex.Frontend.app_resource.AppBackend.create_memory"><code class="name flex">
<span>def <span class="ident">create_memory</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Create a new chat memory.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_memory(self) -&gt; None:
    &#34;&#34;&#34;Create a new chat memory.
    &#34;&#34;&#34;
    self._memory = LongShortTermChatMemory(chat_id=get_new_chat_id(), 
            embeddings=self.embeddings, 
            llm=self.llm, 
            ranker=self.ranker, 
            text_splitter=self.text_splitter)</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.app_resource.AppBackend.drop_memory"><code class="name flex">
<span>def <span class="ident">drop_memory</span></span>(<span>self, chat_id: str) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Delete the chat memory give the chat ID.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>chat_id</code></strong> :&ensp;<code>str</code></dt>
<dd>Chat ID.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def drop_memory(self, chat_id: str) -&gt; None:
    &#34;&#34;&#34;Delete the chat memory give the chat ID.

    Args:
        chat_id (str): Chat ID.
    &#34;&#34;&#34;
    import shutil
    if chat_id in list_chat_ids():
        if chat_id == self.memory.chat_id:
            self.create_memory()
        chat_dir = get_dir_from_id(chat_id=chat_id)
        shutil.rmtree(chat_dir)</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.app_resource.AppBackend.set_generation_config"><code class="name flex">
<span>def <span class="ident">set_generation_config</span></span>(<span>self, temperature: Optional[float] = None, max_new_tokens: Optional[int] = None, top_p: Optional[float] = None, top_k: Optional[int] = None, repetition_penalty: Optional[float] = None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Update the LLM generation config. If None is given to any arguments, the argument will not change.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>temperature</code></strong> :&ensp;<code>Optional[float]</code>, optional</dt>
<dd>Set how "creative" the model is, the smaller it is, the more static of the output. Defaults to None.</dd>
<dt><strong><code>max_new_tokens</code></strong> :&ensp;<code>Optional[int]</code>, optional</dt>
<dd>Maximum number of tokens to generate by the llm. Defaults to None.</dd>
<dt><strong><code>top_p</code></strong> :&ensp;<code>Optional[float]</code>, optional</dt>
<dd>While sampling the next token, only consider the tokens above this p value. Defaults to None.</dd>
<dt><strong><code>top_k</code></strong> :&ensp;<code>Optional[int]</code>, optional</dt>
<dd>While sampling the next token, only consider the top "top_k" tokens. Defaults to None.</dd>
<dt><strong><code>repetition_penalty</code></strong> :&ensp;<code>Optional[float]</code>, optional</dt>
<dd>The value to penalise the model for generating repetitive text. Defaults to None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_generation_config(self, temperature: Optional[float] = None, max_new_tokens: Optional[int] = None, 
            top_p: Optional[float] = None, top_k: Optional[int] = None, repetition_penalty: Optional[float] = None) -&gt; None:
    &#34;&#34;&#34;Update the LLM generation config. If None is given to any arguments, the argument will not change.

    Args:
        temperature (Optional[float], optional): Set how &#34;creative&#34; the model is, the smaller it is, the more static of the output. Defaults to None.
        max_new_tokens (Optional[int], optional): Maximum number of tokens to generate by the llm. Defaults to None.
        top_p (Optional[float], optional): While sampling the next token, only consider the tokens above this p value. Defaults to None.
        top_k (Optional[int], optional): While sampling the next token, only consider the top &#34;top_k&#34; tokens. Defaults to None.
        repetition_penalty (Optional[float], optional): The value to penalise the model for generating repetitive text. Defaults to None.
    &#34;&#34;&#34;
    args = [temperature, max_new_tokens, top_p, top_k, repetition_penalty]
    arg_names = [&#39;temperature&#39;, &#39;max_new_tokens&#39;, &#39;top_p&#39;, &#39;top_k&#39;, &#39;repetition_penalty&#39;]
    for i, arg in enumerate(args):
        if arg is not None:
            self._generation_config[arg_names[i]] = arg</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.app_resource.AppBackend.set_memory_config"><code class="name flex">
<span>def <span class="ident">set_memory_config</span></span>(<span>self, recent_token_limit: Optional[int] = None, relevant_token_limit: Optional[int] = None, relevance_score_threshold: Optional[float] = None, similarity_score_threshold: Optional[float] = None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Update the memory config. If None is given to any arguments, the argument will not change.</p>
<p>Args:
recent_token_limit (Optional[int], optional): Token limit for the most recent conversation history. Defaults to None.
relevant_token_limit (Optional[int], optional): Token limit for the relevant contents from older conversation history. Defaults to None.
relevance_score_threshold (Optional[float], optional): Score threshold for the reranker for relevant conversation history content extraction. Defaults to None.
similarity_score_threshold (Optional[float], optional): Score threshold for the vector database search for relevant conversation history content extraction. Defaults to None.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_memory_config(self, recent_token_limit: Optional[int] = None, relevant_token_limit: Optional[int] = None, 
            relevance_score_threshold: Optional[float] = None, similarity_score_threshold: Optional[float] = None) -&gt; None:
    &#34;&#34;&#34;Update the memory config. If None is given to any arguments, the argument will not change.

    Args:
    recent_token_limit (Optional[int], optional): Token limit for the most recent conversation history. Defaults to None.
    relevant_token_limit (Optional[int], optional): Token limit for the relevant contents from older conversation history. Defaults to None.
    relevance_score_threshold (Optional[float], optional): Score threshold for the reranker for relevant conversation history content extraction. Defaults to None.
    similarity_score_threshold (Optional[float], optional): Score threshold for the vector database search for relevant conversation history content extraction. Defaults to None.
    &#34;&#34;&#34;
    args = [recent_token_limit, relevant_token_limit, relevance_score_threshold, similarity_score_threshold]
    arg_names = [&#39;recent_token_limit&#39;, &#39;relevant_token_limit&#39;, &#39;relevance_score_threshold&#39;, &#39;similarity_score_threshold&#39;]
    for i, arg in enumerate(args):
        if arg is not None:
            self._memory_config[arg_names[i]] = arg</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.app_resource.AppBackend.set_prompt_template"><code class="name flex">
<span>def <span class="ident">set_prompt_template</span></span>(<span>self, preset: str) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Updating prompt template.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>preset</code></strong> :&ensp;<code>str</code></dt>
<dd>Preset name of the prompt template.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_prompt_template(self, preset: str) -&gt; None:
    &#34;&#34;&#34;Updating prompt template.

    Args:
        preset (str): Preset name of the prompt template.
    &#34;&#34;&#34;
    if preset in presets.keys():
        self._prompt_template = PromptTemplate.from_preset(style=preset)</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.app_resource.AppBackend.set_system_message"><code class="name flex">
<span>def <span class="ident">set_system_message</span></span>(<span>self, system: str) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Update the system message of the current conversation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>system</code></strong> :&ensp;<code>str</code></dt>
<dd>Update the system message of the current conversation.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_system_message(self, system: str) -&gt; None:
    &#34;&#34;&#34;Update the system message of the current conversation.

    Args:
        system (str): Update the system message of the current conversation.
    &#34;&#34;&#34;
    self.memory.update_system_message(system=system)</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.app_resource.AppBackend.switch_memory"><code class="name flex">
<span>def <span class="ident">switch_memory</span></span>(<span>self, chat_id: str) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Switch to the memory given the chat ID.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>chat_id</code></strong> :&ensp;<code>str</code></dt>
<dd>Chat ID.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def switch_memory(self, chat_id: str) -&gt; None:
    &#34;&#34;&#34;Switch to the memory given the chat ID.

    Args:
        chat_id (str): Chat ID.
    &#34;&#34;&#34;
    if chat_id in list_chat_ids():
        self._memory = LongShortTermChatMemory(chat_id=chat_id, 
                embeddings=self.embeddings, 
                llm=self.llm, 
                ranker=self.ranker, 
                text_splitter=self.text_splitter,
                from_exist=True)</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.app_resource.AppBackend.toggle_tool"><code class="name flex">
<span>def <span class="ident">toggle_tool</span></span>(<span>self, tool_name: str) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Toggle the on/off status of the given tool.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tool_name</code></strong> :&ensp;<code>str</code></dt>
<dd>Tool to toggle.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def toggle_tool(self, tool_name: str) -&gt; None:
    &#34;&#34;&#34;Toggle the on/off status of the given tool.

    Args:
        tool_name (str): Tool to toggle.
    &#34;&#34;&#34;
    if tool_name in self.tool_status.keys():
        self.tool_selector._enabled[tool_name] = not self.tool_selector._enabled[tool_name]</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="llmflex.Frontend" href="index.html">llmflex.Frontend</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="llmflex.Frontend.app_resource.AppBackend" href="#llmflex.Frontend.app_resource.AppBackend">AppBackend</a></code></h4>
<ul class="">
<li><code><a title="llmflex.Frontend.app_resource.AppBackend.config" href="#llmflex.Frontend.app_resource.AppBackend.config">config</a></code></li>
<li><code><a title="llmflex.Frontend.app_resource.AppBackend.create_memory" href="#llmflex.Frontend.app_resource.AppBackend.create_memory">create_memory</a></code></li>
<li><code><a title="llmflex.Frontend.app_resource.AppBackend.drop_memory" href="#llmflex.Frontend.app_resource.AppBackend.drop_memory">drop_memory</a></code></li>
<li><code><a title="llmflex.Frontend.app_resource.AppBackend.embeddings" href="#llmflex.Frontend.app_resource.AppBackend.embeddings">embeddings</a></code></li>
<li><code><a title="llmflex.Frontend.app_resource.AppBackend.factory" href="#llmflex.Frontend.app_resource.AppBackend.factory">factory</a></code></li>
<li><code><a title="llmflex.Frontend.app_resource.AppBackend.generation_config" href="#llmflex.Frontend.app_resource.AppBackend.generation_config">generation_config</a></code></li>
<li><code><a title="llmflex.Frontend.app_resource.AppBackend.has_tools" href="#llmflex.Frontend.app_resource.AppBackend.has_tools">has_tools</a></code></li>
<li><code><a title="llmflex.Frontend.app_resource.AppBackend.llm" href="#llmflex.Frontend.app_resource.AppBackend.llm">llm</a></code></li>
<li><code><a title="llmflex.Frontend.app_resource.AppBackend.memory" href="#llmflex.Frontend.app_resource.AppBackend.memory">memory</a></code></li>
<li><code><a title="llmflex.Frontend.app_resource.AppBackend.memory_config" href="#llmflex.Frontend.app_resource.AppBackend.memory_config">memory_config</a></code></li>
<li><code><a title="llmflex.Frontend.app_resource.AppBackend.prompt_template" href="#llmflex.Frontend.app_resource.AppBackend.prompt_template">prompt_template</a></code></li>
<li><code><a title="llmflex.Frontend.app_resource.AppBackend.ranker" href="#llmflex.Frontend.app_resource.AppBackend.ranker">ranker</a></code></li>
<li><code><a title="llmflex.Frontend.app_resource.AppBackend.set_generation_config" href="#llmflex.Frontend.app_resource.AppBackend.set_generation_config">set_generation_config</a></code></li>
<li><code><a title="llmflex.Frontend.app_resource.AppBackend.set_memory_config" href="#llmflex.Frontend.app_resource.AppBackend.set_memory_config">set_memory_config</a></code></li>
<li><code><a title="llmflex.Frontend.app_resource.AppBackend.set_prompt_template" href="#llmflex.Frontend.app_resource.AppBackend.set_prompt_template">set_prompt_template</a></code></li>
<li><code><a title="llmflex.Frontend.app_resource.AppBackend.set_system_message" href="#llmflex.Frontend.app_resource.AppBackend.set_system_message">set_system_message</a></code></li>
<li><code><a title="llmflex.Frontend.app_resource.AppBackend.switch_memory" href="#llmflex.Frontend.app_resource.AppBackend.switch_memory">switch_memory</a></code></li>
<li><code><a title="llmflex.Frontend.app_resource.AppBackend.text_splitter" href="#llmflex.Frontend.app_resource.AppBackend.text_splitter">text_splitter</a></code></li>
<li><code><a title="llmflex.Frontend.app_resource.AppBackend.toggle_tool" href="#llmflex.Frontend.app_resource.AppBackend.toggle_tool">toggle_tool</a></code></li>
<li><code><a title="llmflex.Frontend.app_resource.AppBackend.tool_selector" href="#llmflex.Frontend.app_resource.AppBackend.tool_selector">tool_selector</a></code></li>
<li><code><a title="llmflex.Frontend.app_resource.AppBackend.tool_status" href="#llmflex.Frontend.app_resource.AppBackend.tool_status">tool_status</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>