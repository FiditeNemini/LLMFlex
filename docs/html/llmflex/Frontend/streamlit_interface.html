<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>llmflex.Frontend.streamlit_interface API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>llmflex.Frontend.streamlit_interface</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from ..Models.Factory.llm_factory import LlmFactory
from ..Embeddings.base_embeddings import BaseEmbeddingsToolkit
from ..Embeddings.huggingface_embeddings import HuggingfaceEmbeddingsToolkit
from ..Embeddings.api_embeddings import APIEmbeddingsToolkit
from ..Tools.base_tool import BaseTool
from ..utils import PACKAGE_DISPLAY_NAME
import streamlit as st
from typing import Dict, Any, Union, List, Tuple, Type, Optional, Literal, Iterator


class InterfaceState:

    def __init__(self, model: LlmFactory, embeddings: Type[BaseEmbeddingsToolkit], tools: List[Type[BaseTool]] = []) -&gt; None:
        &#34;&#34;&#34;Initialise the backend of the Streamlit interface.

        Args:
            model (LlmFactory): LLM factory.
            embeddings (Type[BaseEmbeddingsToolkit]): Embeddings toolkit.
            tools (List[Type[BaseTool]], optional): List of tools. Defaults to [].
        &#34;&#34;&#34;
        from ..Memory.assistant_long_term_memory import AssistantLongTermChatMemory
        from ..Tools.tool_selection import ToolSelector
        from ..TextSplitters.sentence_token_text_splitter import SentenceTokenTextSplitter
        from ..Prompts.prompt_template import DEFAULT_SYSTEM_MESSAGE
        self.model = model
        self.embeddings = embeddings
        self.text_splitter = SentenceTokenTextSplitter(count_token_fn=model().get_num_tokens, chunk_size=200, chunk_overlap=40)
        self.memory = AssistantLongTermChatMemory(title=&#39;Untitled 0&#39;, embeddings=self.embeddings, text_splitter=self.text_splitter, from_exist=False)
        self.system = DEFAULT_SYSTEM_MESSAGE
        self.template = self.model.prompt_template
        self.llm = self.model(stop=self.template.stop + [&#39;#####&#39;])
        self.short_limit = 600
        self.long_limit = 500
        self.score_threshold = 0.5
        self.tool_selector = ToolSelector(tools, model=self.model, embeddings=self.embeddings) if len(tools) &gt; 0 else None

    @property
    def titles(self) -&gt; List[str]:
        &#34;&#34;&#34;All existing chat titles.

        Returns:
            List[str]: All existing chat titles.
        &#34;&#34;&#34;
        from ..Memory.base_memory import list_titles
        return list_titles()
    
    @property
    def current_title(self) -&gt; str:
        &#34;&#34;&#34;Current memory chat title.

        Returns:
            str: Current memory chat title.
        &#34;&#34;&#34;
        return self.memory.title
      
    @property
    def history(self) -&gt; List[List[str]]:
        &#34;&#34;&#34;Current conversation history.

        Returns:
            List[List[str]]: Current conversation history.
        &#34;&#34;&#34;
        return self.memory.history
       
    @property
    def presets(self) -&gt; List[str]:
        &#34;&#34;&#34;List of prompt templates presets.

        Returns:
            List[str]: List of prompt templates presets.
        &#34;&#34;&#34;
        from ..Prompts.prompt_template import presets
        return list(presets.keys())

class StreamlitInterface:

    def __init__(self, model_kwargs: Dict[str, Any], 
                 embeddings_kwargs: Dict[str, Any],
                 tool_kwargs: List[Dict[str, Any]] = [],
                 auth: Optional[Tuple[str, str]] = None, 
                 debug: bool = False) -&gt; None:
        if not hasattr(st.session_state, &#39;backend&#39;):
            model = LlmFactory(**model_kwargs)
            embeddings = embeddings_loader(embeddings_kwargs)
            tools = list(map(lambda x: tool_loader(x, embeddings, model), tool_kwargs))
            st.session_state.backend = InterfaceState(model, embeddings, tools=tools)
        self.debug = debug
        self._auth = auth
        if auth is None:
            st.session_state.islogin = True
        self.sidebar_ratio = [5, 1]

    @property
    def backend(self) -&gt; InterfaceState:
        return st.session_state.backend
        
    @property
    def islogin(self) -&gt; bool:
        if not hasattr(st.session_state, &#39;islogin&#39;):
            st.session_state.islogin = False
        return st.session_state.islogin
    
    @property
    def mobile(self) -&gt; bool:
        if not hasattr(st.session_state, &#39;mobile&#39;):
            st.session_state.mobile = False
        return st.session_state.mobile

    @property
    def login_wrong(self) -&gt; bool:
        if not hasattr(st.session_state, &#39;login_wrong&#39;):
            st.session_state.login_wrong = False
        return st.session_state.login_wrong

    @property
    def conversation_delete(self) -&gt; bool:
        if not hasattr(st.session_state, &#39;conversation_delete&#39;):
            st.session_state.conversation_delete = False
        return st.session_state.conversation_delete

    @property
    def generating(self) -&gt; bool:
        &#34;&#34;&#34;Whether chatbot is generating.&#34;&#34;&#34;
        if not hasattr(st.session_state, &#39;generating&#39;):
            st.session_state.generating = False
        return st.session_state.generating
    
    @property
    def experimental(self) -&gt; bool:
        if not hasattr(st.session_state, &#39;allow_ai_start&#39;):
            st.session_state.allow_ai_start = False
        return st.session_state.allow_ai_start

    @property
    def history_dict(self) -&gt; List[Dict[str, Any]]:
        if not hasattr(st.session_state, &#39;history&#39;):
            self.refresh_history()
        elif st.session_state.history[&#39;current_title&#39;] != self.backend.current_title:
            self.refresh_history()
        return st.session_state.history[&#39;history_dict&#39;]
    
    @property
    def generation_config(self) -&gt; Dict[str, str]:
        if not hasattr(st.session_state, &#39;generation_config&#39;):
            st.session_state.generation_config = dict(gen_type=&#39;none&#39;)
        return st.session_state.generation_config
    
    @property
    def ai_start_text(self) -&gt; str:
        if not hasattr(st.session_state, &#39;ai_start_text&#39;):
            st.session_state.ai_start_text = &#39;&#39;
        return st.session_state.ai_start_text

    @property
    def generation_time_info(self) -&gt; str:
        template = &#39;  \nGeneration time taken: &#39;
        if not hasattr(st.session_state, &#39;generation_time_info&#39;):
            st.session_state.generation_time_info = &#39;--&#39;
        if type(st.session_state.generation_time_info) == str:
            return template + st.session_state.generation_time_info
        else:
            return template + f&#39;{round(st.session_state.generation_time_info, 2)}&#39; + &#39;s&#39;

    @property
    def tool_states(self) -&gt; Dict[str, bool]:
        if not hasattr(st.session_state, &#39;tool_states&#39;):
            st.session_state.tool_states = dict()
            for tool in self.backend.tool_selector.tools:
                st.session_state.tool_states[tool.name] = True
        return st.session_state.tool_states

    def get_tool(self, user_input: str) -&gt; Optional[Type[BaseTool]]:
        if self.backend.tool_selector is None:
            return None
        if sum(list(self.tool_states.values())) == 0:
            return None
        self.backend.tool_selector.set_score_threshold(self.tool_threshold)
        history = self.backend.memory.get_token_memory(llm=self.backend.llm, token_limit=self.backend.short_limit)
        system = self.backend.system
        tool = self.backend.tool_selector.get_tool(user_input=user_input, history=history, system=system)
        print(f&#39;Tool: {self.backend.tool_selector.last_tool}\nScore: {self.backend.tool_selector.last_score}&#39;)
        if tool is not None:
            if not self.tool_states[tool.name]:
                print(tool.name + &#39; disabled.&#39;)
                tool = None
        return tool
    
    def run_tool(self, tool: BaseTool, user_input: str) -&gt; Iterator[Union[str, Tuple[str, str], Iterator[str]]]:
        recent_history = self.backend.memory.get_token_memory(llm=self.backend.llm, token_limit=self.backend.short_limit)
        return tool.run_with_chat(tool_input=user_input, llm=self.backend.llm, stream=True, history=recent_history)

    def login_with_cred(self, user: str, password: str) -&gt; None:
        if ((user == self._auth[0]) &amp; (password == self._auth[1])):
            print(user, password)
            st.session_state.islogin = True
        else:
            st.session_state.login_wrong = True
        st.rerun()

    def toggle_generating(self) -&gt; None:
        st.session_state.generating = not self.generating
        # print(f&#39;Generating = {self.generating}&#39;)

    def toggle_mobile(self) -&gt; None:
        st.session_state.mobile = not self.mobile

    def toggle_tool(self, tool_name: str) -&gt; None:
        if not self.generating:
            self.tool_states[tool_name] = not self.tool_states[tool_name]

    def toggle_conversation_delete(self) -&gt; None:
        st.session_state.conversation_delete = not self.conversation_delete

    def set_mobile(self) -&gt; None:
        if not self.generating:
            st.session_state.mobile = st.session_state.mobile_toggle

    def current_prompt_index(self) -&gt; int:
        from ..Prompts.prompt_template import presets
        template = self.backend.model.prompt_template.template_name
        for i, t in enumerate(self.backend.presets):
            if template == t:
                return i
        return 0
    
    def set_time_info(self, time: Union[str, float] = &#39;--&#39;) -&gt; None:
        st.session_state.generation_time_info = time     

    def add_chat(self, title: str) -&gt; None:
        if not self.generating:
            title = title.strip(&#39; \r\n\t&#39;)
            if title == &#39;&#39;:
                pass
            else:
                from ..Memory.assistant_long_term_memory import AssistantLongTermChatMemory
                from ..Prompts.prompt_template import DEFAULT_SYSTEM_MESSAGE
                self.backend.memory = AssistantLongTermChatMemory(title, embeddings=self.backend.embeddings, text_splitter=self.backend.text_splitter, from_exist=True)
                self.backend.system = self.backend.memory.vectordb._info.get(&#39;system&#39;, DEFAULT_SYSTEM_MESSAGE)
                self.backend.memory.vectordb._info[&#39;system&#39;] = self.backend.system
                self.backend.memory.save()
                st.rerun()

    def switch_chat(self, title: str) -&gt; None:
        if not self.generating:
            from ..Memory.assistant_long_term_memory import AssistantLongTermChatMemory
            from ..Prompts.prompt_template import DEFAULT_SYSTEM_MESSAGE
            st.session_state.generating = True
            self.set_time_info()
            if title == self.backend.current_title:
                pass
            else:
                print(f&#39;Switch to: {title}&#39;)
                from_exist = title != &#39;Untitled 0&#39;
                self.backend.memory = AssistantLongTermChatMemory(title=title, embeddings=self.backend.embeddings, text_splitter=self.backend.text_splitter, from_exist=from_exist)
                self.backend.system = self.backend.memory.info.get(&#39;system&#39;, DEFAULT_SYSTEM_MESSAGE)
                self.backend.memory.save()
            st.session_state.generating = False

    def delete_chat(self, title: str) -&gt; None:
        if not self.generating:
            switch = title == self.backend.current_title
            from shutil import rmtree
            if switch:
                rmtree(self.backend.memory.chat_dir)
                self.switch_chat(&#39;Untitled 0&#39;)
            else:
                from ..Memory.assistant_long_term_memory import AssistantLongTermChatMemory
                mem = AssistantLongTermChatMemory(title, embeddings=self.backend.embeddings, text_splitter=self.backend.text_splitter, from_exist=False)
                rmtree(mem.chat_dir)
        
    def toggle_exeperimental(self) -&gt; None:
        if not self.generating:
            st.session_state.allow_ai_start = not st.session_state.allow_ai_start

    def set_exeperimental(self) -&gt; None:
        if not self.generating:
            st.session_state.allow_ai_start = st.session_state.use_ai_start

    def set_prompt_template(self) -&gt; None:
        if not self.generating:
            from ..Prompts.prompt_template import PromptTemplate
            preset = st.session_state.prompt_format
            self.backend.template = PromptTemplate.from_preset(preset)

    def set_system_message(self, system: str) -&gt; None:
        if not self.generating:
            self.backend.system = system.strip(&#39; \n\r\t&#39;)
            self.backend.memory.vectordb._info[&#39;system&#39;] = self.backend.system
            self.backend.memory.save()

    def set_memory_settings(self, long: int, short: int, score: float) -&gt; None:
        if not self.generating:
            self.backend.short_limit = short
            self.backend.long_limit = long
            self.backend.score_threshold = score

    def set_llm_config(self, temperature: float, max_new_tokens: int, repetition_penalty: float, top_p: float, top_k: int) -&gt; None:
        if not self.generating:
            self.backend.llm = self.backend.model(temperature=temperature, max_new_tokens=max_new_tokens, repetition_penalty=repetition_penalty, top_p=top_p, top_k=top_k)

    def get_history(self, mem) -&gt; List[Dict[str, Any]]:
        history = list(map(lambda x: dict(
            user=x[&#39;metadata&#39;][&#39;user&#39;], 
            assistant=x[&#39;metadata&#39;][&#39;assistant&#39;], 
            order=x[&#39;metadata&#39;][&#39;order&#39;],
            tool_details=x[&#39;metadata&#39;].get(&#39;tool_details&#39;, None),
            footnote=x[&#39;metadata&#39;].get(&#39;footnote&#39;, None),
            tool_name=x[&#39;metadata&#39;].get(&#39;tool_name&#39;, None)
        ), mem._data))
        if len(history) == 0:
            return []
        count = max(list(map(lambda x: x[&#39;order&#39;], history))) + 1
        history = list(map(lambda x: list(filter(lambda y: y[&#39;order&#39;] == x, history))[0], range(count)))
        history.sort(key=lambda x: x[&#39;order&#39;], reverse=False)
        return history

    def refresh_history(self) -&gt; None:
        st.session_state.history = dict(
            current_title=self.backend.current_title,
            history_dict=self.get_history(self.backend.memory)
        )

    def save_interaction(self, user: str, assistant: str, **kwargs) -&gt; None:
        self.backend.memory.save_interaction(user_input=user, assistant_output=assistant, **kwargs)
        self.refresh_history()

    def input_template(self, user: Optional[str], ai_start: str) -&gt; Optional[Dict[str, Any]]:
        if user is None:
            return None
        user = user.strip(&#39; \n\r\t&#39;)
        if user == &#39;&#39;:
            return None
        input_dict = dict(
            user = user.strip(&#39; \n\r\t&#39;),
            assistant = ai_start,
            order = self.backend.memory.interaction_count,
            tool_details = None,
            footnote = None,
            tool_name = None
        )
        return input_dict

    def create_generation_config(self, gen_type: Literal[&#39;new&#39;, &#39;retry&#39;, &#39;continue&#39;, &#39;none&#39;], user_input: str, ai_start: Optional[str] = None) -&gt; None:
        gen_obj = dict(
            gen_type = gen_type,
            user_input = user_input.strip(&#39; \r\n\t&#39;),
            ai_start = self.ai_start if ai_start is None else ai_start
        )
        st.session_state.generation_config = gen_obj

    def get_generation_iterator(self) -&gt; Iterator[str]:
        from ..Memory.assistant_long_term_memory import create_long_assistant_memory_prompt
        config = self.generation_config
        prompt = create_long_assistant_memory_prompt(
            user=config[&#39;user_input&#39;],
            prompt_template=self.backend.template,
            llm=self.backend.llm,
            memory=self.backend.memory,
            system=self.backend.system,
            short_token_limit=self.backend.short_limit,
            long_token_limit=self.backend.long_limit,
            score_threshold=self.backend.score_threshold
        ) + config[&#39;ai_start&#39;]
        print(f&#39;Number of input tokens: {self.backend.llm.get_num_tokens(prompt)}&#39;)
        def generator():
            yield config[&#39;ai_start&#39;]
            for i in self.backend.llm.stream(prompt):
                yield i
        return generator()

    def retry_response(self, cont: bool = False, ai_start: str = &#39;&#39;) -&gt; None:
        if not self.generating:
            if len(self.history_dict) == 0:
                return None
            print(ai_start)
            st.session_state.ai_start_text = ai_start
            history = self.get_history(self.backend.memory)
            user = history[-1][&#39;user&#39;]
            self.create_generation_config(&#39;continue&#39; if cont else &#39;retry&#39;, user, history[-1][&#39;assistant&#39;] if cont else ai_start)
            input_dict = self.input_template(user=user, ai_start=history[-1][&#39;assistant&#39;] if cont else ai_start)
            self.backend.memory.remove_last_interaction()
            self.refresh_history()
            self.history_dict.append(input_dict)
            self.toggle_generating()

    def remove_last(self) -&gt; None:
        if not self.generating:
            self.backend.memory.remove_last_interaction()
            self.refresh_history()

    ##### Defining the interface
        
    def login(self) -&gt; None:
        login_form = st.form(key=&#39;login&#39;)
        with login_form:
            user = st.text_input(label=&#39;Username:&#39;, placeholder=&#39;Your username...&#39;)
            password = st.text_input(label=&#39;Password&#39;, placeholder=&#39;Your password...&#39;, type=&#39;password&#39;)
            if st.form_submit_button(label=&#39;Login&#39;):
                print(&#39;Triggered by btn.&#39;)
                self.login_with_cred(user=user, password=password)
            if self.login_wrong:
                st.warning(&#39;Incorrect credentials. Please try again.&#39;)

    def sidebar(self) -&gt; None:
        &#34;&#34;&#34;Sidebar of the webapp.
        &#34;&#34;&#34;
        app_summary = [&#39;Powered by:&#39;, 
                       f&#39;* LLM: {self.backend.model.model_id}&#39;, 
                       f&#39;* Embedding model: {self.backend.embeddings.name}&#39;, 
                       &#39;&#39;, 
                       &#39;Current conversation:&#39;, 
                       f&#39;* {self.backend.current_title}&#39;,
                       &#39;&#39;,
                       &#39;Current prompt format:&#39;,
                       f&#39;* {self.backend.template.template_name}&#39;]
        st.header(PACKAGE_DISPLAY_NAME.upper(), help=&#39;  \n&#39;.join(app_summary), divider=&#34;grey&#34;)
        st.subheader(&#39;:left_speech_bubble: Conversations&#39;)
        self.new_chat_form()
        with st.expander(label=&#39;Previous conversations&#39;):
            self.conversations()
        self.settings()
        if self.debug:
            st.subheader(&#39;Debug&#39;)
            self.test_buttons()

    def new_chat_form(self) -&gt; None:
        with st.form(key=&#39;new_chat_form&#39;, border=False, clear_on_submit=True):
            cols = st.columns(self.sidebar_ratio)
            with cols[0]:
                self.new_title = st.text_input(label=&#39;new_title&#39;, max_chars=40, placeholder=&#39;New conversation title here...&#39;, label_visibility=&#39;collapsed&#39;, disabled=self.generating)
            with cols[1]:
                if st.form_submit_button(label=&#39;:heavy_plus_sign:&#39;, disabled=self.generating):
                    self.add_chat(title=self.new_title)

    def conversations(self) -&gt; None:
        &#34;&#34;&#34;List of conversations.&#34;&#34;&#34;
        for title in self.backend.titles:
            if self.conversation_delete:
                cols = st.columns(self.sidebar_ratio)
                with cols[0]:
                    btn_type = &#39;primary&#39; if self.backend.current_title == title else &#39;secondary&#39;
                    st.button(label=title.title(), key=f&#39;{title}_select&#39;, disabled=self.generating, 
                            use_container_width=True, type=btn_type, on_click=self.switch_chat, kwargs=dict(title=title))
                with cols[1]:
                    st.button(label=&#39;:heavy_minus_sign:&#39;, key=f&#39;{title}_delete&#39;, disabled=self.generating, on_click=self.delete_chat, kwargs=dict(title=title))
            else:
                btn_type = &#39;primary&#39; if self.backend.current_title == title else &#39;secondary&#39;
                st.button(label=title.title(), key=f&#39;{title}_select&#39;, disabled=self.generating, 
                        use_container_width=True, type=btn_type, on_click=self.switch_chat, kwargs=dict(title=title))
        cols = st.columns(self.sidebar_ratio)
        with cols[0]:
            st.toggle(label=&#39;:wastebasket:&#39;, key=f&#39;conv_delete&#39;, disabled=self.generating, on_change=self.toggle_conversation_delete, help=&#39;Select conversations to remove.&#39;)

    def test_buttons(self) -&gt; None:
        cols = st.columns([1, 1])
        with cols[0]:
            st.button(&#39;Test print&#39;, key=&#39;test&#39;, on_click=lambda: print(self.ai_start))

        with cols[1]:
            st.button(&#39;gen_toggle&#39;, key=&#39;toggle_gen&#39;, on_click=lambda: self.toggle_generating())

    def settings(self) -&gt; None:
        &#34;&#34;&#34;Settings of the webapp.&#34;&#34;&#34;
        st.subheader(&#39;:gear: Settings&#39;)
        st.toggle(label=&#39;Experimental&#39;, 
                    value=self.experimental,
                    disabled=self.generating,
                    on_change=self.set_exeperimental,
                    key=&#39;use_ai_start&#39;, 
                    help=&#39;More features such as retrying, adding response starting message etc.&#39;)
        st.toggle(label=&#39;Mobile&#39;, 
                    value=self.mobile,
                    disabled=self.generating,
                    on_change=self.set_mobile,
                    key=&#39;mobile_toggle&#39;, 
                    help=&#39;Better layout for mobile device.&#39;)
        with st.expander(label=&#39;Prompt format settings&#39;):
            self.prompt_template_settings()
        with st.expander(label=&#39;System message settings&#39;):
            self.system_prompt_settings()
        with st.expander(label=&#39;Memory settings&#39;):
            self.memory_settings()
        with st.expander(label=&#39;Model settings&#39;):
            self.llm_settings()
        self.tool_settings()

    def prompt_template_settings(self) -&gt; None:
        &#34;&#34;&#34;Prompt template settings.&#34;&#34;&#34;
        from ..Prompts.prompt_template import presets
        format = st.selectbox(label=&#39;prompt_formats&#39;, 
                     label_visibility=&#39;collapsed&#39;,
                     key=&#39;prompt_format&#39;,
                     options=list(presets.keys()), 
                     disabled=self.generating, 
                     index=self.current_prompt_index(),
                     on_change=self.set_prompt_template)
        
    def system_prompt_settings(self) -&gt; None:
        &#34;&#34;&#34;System prompt settings.&#34;&#34;&#34;
        self.system_text = st.text_area(label=&#39;System message&#39;, height=250, key=&#39;system_msg&#39;,label_visibility=&#39;collapsed&#39;, value=self.backend.system, disabled=self.generating)
        st.markdown(f&#39;System message token count: {self.backend.llm.get_num_tokens(self.backend.system)}&#39;)
        st.button(label=&#39;:floppy_disk:&#39;, key=&#39;system_save&#39;, disabled=self.generating, use_container_width=True, on_click=self.set_system_message, kwargs=dict(system=self.system_text))

    def memory_settings(self) -&gt; None:
        &#34;&#34;&#34;Memory token limit settings.&#34;&#34;&#34;
        self.short_limit_slidder = st.slider(&#39;Short term memory token limit&#39;, min_value=0, max_value=6000, step=1, value=self.backend.short_limit, disabled=self.generating)
        self.long_limit_slidder = st.slider(&#39;Long term memory token limit&#39;, min_value=0, max_value=6000, step=1, value=self.backend.long_limit, disabled=self.generating)
        self.score_threshold_slidder = st.slider(&#39;Score threshold for long term memory&#39;, min_value=0.0, max_value=1.0, step=0.01, value=self.backend.score_threshold, disabled=self.generating)
        summary = [
            &#39;Current settings:&#39;,
            f&#39;Short term memory token limit: {self.backend.short_limit}&#39;,
            f&#39;Long term memory token limit: {self.backend.long_limit}&#39;,
            f&#39;Score threshold: {self.backend.score_threshold}&#39;

        ]
        st.markdown(&#39;  \n&#39;.join(summary))
        st.button(label=&#39;:floppy_disk:&#39;, key=&#39;memory_token_save&#39;, disabled=self.generating, 
                  use_container_width=True,
                  on_click=self.set_memory_settings,
                  kwargs=dict(short=self.short_limit_slidder,
                              long=self.long_limit_slidder,
                              score=self.score_threshold_slidder))

    def llm_settings(self) -&gt; None:
        &#34;&#34;&#34;LLM generation settings.&#34;&#34;&#34;
        self.temperature_slidder = st.slider(&#39;Temparature&#39;, min_value=0.0, max_value=2.0, step=0.01, value=self.backend.llm.generation_config[&#39;temperature&#39;], disabled=self.generating)
        self.max_new_token_slidder = st.slider(&#39;Maximum number of new tokens&#39;, min_value=0, max_value=4096, step=1, value=self.backend.llm.generation_config[&#39;max_new_tokens&#39;], disabled=self.generating)
        self.repetition_slidder = st.slider(&#39;Repetition penalty&#39;, min_value=1.0, max_value=2.0, step=0.01, value=self.backend.llm.generation_config[&#39;repetition_penalty&#39;], disabled=self.generating)
        self.topp_slidder = st.slider(&#39;Top P&#39;, min_value=0.0, max_value=1.0, step=0.01, value=self.backend.llm.generation_config[&#39;top_p&#39;], disabled=self.generating)
        self.topk_slidder = st.slider(&#39;Top K&#39;, min_value=0, max_value=30000, step=1, value=self.backend.llm.generation_config[&#39;top_k&#39;], disabled=self.generating)
        summary = [
            &#39;Current settings:&#39;,
            f&#34;Temperature: {self.backend.llm.generation_config[&#39;temperature&#39;]}&#34;,
            f&#34;Max new tokens: {self.backend.llm.generation_config[&#39;max_new_tokens&#39;]}&#34;,
            f&#34;Repetition penalty: {self.backend.llm.generation_config[&#39;repetition_penalty&#39;]}&#34;,
            f&#34;Top P: {self.backend.llm.generation_config[&#39;top_p&#39;]}&#34;,
            f&#34;Top K: {self.backend.llm.generation_config[&#39;top_k&#39;]}&#34;,
        ]
        st.markdown(&#39;  \n&#39;.join(summary))
        st.button(label=&#39;:floppy_disk:&#39;, 
                  key=&#39;llm_config_save&#39;, 
                  disabled=self.generating, 
                  use_container_width=True,
                  on_click=self.set_llm_config,
                  kwargs=dict(temperature=self.temperature_slidder,
                              max_new_tokens=self.max_new_token_slidder,
                              repetition_penalty=self.repetition_slidder,
                              top_p=self.topp_slidder,
                              top_k=self.topk_slidder))
        
    def tool_settings(self) -&gt; None:
        if self.backend.tool_selector is not None:
            with st.expander(&#39;Tools settings&#39;):
                for i in self.backend.tool_selector.tools:
                    st.toggle(label=f&#39;{i.pretty_name}&#39;, value=self.tool_states[i.name], on_change=lambda: self.toggle_tool(i.name),
                              disabled=self.generating)
                self.tool_threshold = st.slider(label=&#39;Tool trigger threshold&#39;, min_value=0.0, max_value=1.0, step=0.005, 
                                                value=self.backend.tool_selector.score_threshold, disabled=self.generating)

    def chatbot(self) -&gt; None:
        self.conversation_history()
        
        self.ai_start_textbox()
        self.user_input_box()

    def conversation_history(self) -&gt; None:
        history = self.history_dict
        last = len(history) - 1
        gen_type = self.generation_config[&#39;gen_type&#39;]
        for i, ex in enumerate(history):
            with st.chat_message(name=&#39;user&#39;):
                st.markdown(ex[&#39;user&#39;], help=f&#39;Number of tokens: {self.backend.llm.get_num_tokens(ex[&#34;user&#34;])}&#39;)
            with st.chat_message(name=&#39;assistant&#39;):
                self.assistant_response(ex, i, last)
            if ((gen_type != &#39;none&#39;) &amp; (i == last)):
                st.session_state.generation_config = dict(gen_type=&#39;none&#39;)
                st.session_state.retry_or_continue = False
                self.toggle_generating()
                st.rerun()

    def assistant_response(self, ex: Dict[str, Any], i: int, last: int) -&gt; None:
        if ex[&#39;tool_details&#39;] is not None:
            with st.status(label=f&#34;:hammer_and_pick: __{ex[&#39;tool_name&#39;]}__&#34;, state=&#39;complete&#39;):
                st.code(ex[&#39;tool_details&#39;][1], language=&#39;plaintext&#39;)
        md = ex[&#39;assistant&#39;]
        if ex[&#39;footnote&#39;] is not None:
            md += &#39;\n\n---\n&#39; + ex[&#39;footnote&#39;]
        if ((not self.generating) | (i!=last)):
            help_info = f&#39;Number of tokens: {self.backend.llm.get_num_tokens(ex[&#34;assistant&#34;])}&#39;
            if i==last:
                help_info += self.generation_time_info
            st.markdown(md, help=help_info)
        else:
            # st.button(label=&#39;:black_square_for_stop:&#39;, help=&#39;Stop response generation&#39;)
            with st.spinner(&#39;Thinking....&#39;):
                from time import perf_counter
                start = perf_counter()
                tool = self.get_tool(user_input=ex[&#39;user&#39;])
                if tool is None:
                    placeholder = st.empty()
                    self.output = &#39;&#39;
                    for i in self.get_generation_iterator():
                        self.output += i
                        placeholder.markdown(self.output.strip(&#39; \r\n\t&#39;))
                    self.history_dict[-1][&#39;assistant&#39;] = self.output.strip(&#39; \r\n\t&#39;)
                    self.save_interaction(user=self.history_dict[-1][&#39;user&#39;], assistant=self.history_dict[-1][&#39;assistant&#39;])
                    end = perf_counter() - start
                    self.set_time_info(end)
                    print(f&#39;Number of output tokens: {self.backend.llm.get_num_tokens(self.output)}&#39;)
                else:
                    tool_name = tool.pretty_name
                    tool_details = None
                    footnote = None
                    toolholder = st.empty()
                    placeholder = st.empty()
                    md_text = &#39;&#39;
                    for chunk in self.run_tool(tool=tool, user_input=ex[&#39;user&#39;]):
                        if isinstance(chunk, tuple):
                            tool_details = chunk
                            with toolholder.status(label=f&#34;:hammer_and_pick: Running __{tool_name}__...&#34;, state=&#39;running&#39;):
                                st.text(tool_details[1])
                        elif isinstance(chunk, str):
                            footnote = chunk
                            md_text += &#39;\n\n---\n&#39; + chunk
                            placeholder.markdown(md_text)
                        else:
                            with toolholder.status(label=f&#34;:hammer_and_pick: __{tool_name}__&#34;, state=&#39;complete&#39;):
                                st.text(tool_details[1])
                            output = &#39;&#39;
                            for i in chunk:
                                output += i
                                md_text = output.strip(&#39; \n\r\t&#39;)
                                placeholder.markdown(md_text)
                            end = perf_counter() - start
                            self.set_time_info(end)
                            print(f&#39;Number of output tokens: {self.backend.llm.get_num_tokens(output)}&#39;)
                    self.save_interaction(user=ex[&#39;user&#39;], assistant=output.strip(&#39; \r\n\t&#39;), tool_details=tool_details, footnote=footnote, tool_name=tool_name)
                    

                            
                
            
    def experimental_buttons(self) -&gt; None:
        cols = st.columns([1, 1, 1])
        with cols[0]:
            st.button(&#39;:arrows_counterclockwise:&#39;, use_container_width=True, help=&#39;Re-generate response&#39;, disabled=self.generating, 
                      on_click=self.retry_response, kwargs=dict(cont=False, ai_start=self.ai_start))
        with cols[1]:
            st.button(&#39;:fast_forward:&#39;, use_container_width=True, help=&#39;Continue generating response&#39;, disabled=self.generating,
                      on_click=self.retry_response, kwargs=dict(cont=True, ai_start=self.ai_start))
        with cols[2]:
            st.button(&#39;:wastebasket:&#39;, use_container_width=True, help=&#39;Remove the latest question and response&#39;, disabled=self.generating, on_click=self.remove_last)

    def ai_start_textbox(self) -&gt; None:
        if ((self.experimental) &amp; (not self.generating)):
            with st.container(border=False):
                self.ai_start = st.text_area(label=&#39;AI start&#39;, placeholder=&#39;Start of the chatbot response here...&#39;, value=self.ai_start_text, height=1, label_visibility=&#39;collapsed&#39;)
                if self.mobile:
                    with st.expander(&#39;:gear: Extra options&#39;):
                        self.experimental_buttons()
                else:
                    self.experimental_buttons()
            
        else:
            self.ai_start = &#39;&#39;

    def user_input_box(self) -&gt; None:
        self.user_input = st.chat_input(placeholder=&#39;Your message...&#39;, disabled=self.generating)
        if self.user_input:
            if self.user_input.strip(&#39; \r\n\t&#39;) != &#39;&#39;:
                input_dict = self.input_template(user=self.user_input, ai_start=self.ai_start)
                self.history_dict.append(input_dict)
                self.create_generation_config(&#39;new&#39;, self.user_input)
                self.toggle_generating()
                st.rerun()
            
    def launch(self) -&gt; None:
        if self.islogin:
            if not self.mobile:
                st.set_page_config(layout=&#39;wide&#39;)
            else:
                st.set_page_config(layout=&#39;centered&#39;)
            with st.sidebar:
                self.sidebar()
            self.chatbot()
        else:
            self.login()

def embeddings_loader(embeddings_kwargs: Dict[str, Any]) -&gt; BaseEmbeddingsToolkit:
    &#34;&#34;&#34;Load the embeddings given the kwargs.

    Args:
        embeddings_kwargs (Dict[str, Any]): Kwargs to initialise the embeddings toolkit.

    Returns:
        BaseEmbeddingsToolkit: The embeddings toolkit.
    &#34;&#34;&#34;
    mapper = {&#34;HuggingfaceEmbeddingsToolkit&#34;: HuggingfaceEmbeddingsToolkit, &#39;APIEmbeddingsToolkit&#39;: APIEmbeddingsToolkit}
    model_key = embeddings_kwargs.pop(&#39;embeddings_class&#39;, &#34;HuggingfaceEmbeddingsToolkit&#34;)
    return mapper.get(model_key)(**embeddings_kwargs)

def tool_loader(tool_kwargs: Dict[str, Any], embeddings: Type[BaseEmbeddingsToolkit], model: LlmFactory) -&gt; BaseTool:
    &#34;&#34;&#34;Load the embeddings given the kwargs.

    Args:
        tool_kwargs (Dict[str, Any]): Kwargs to initialise the tool.
        embeddings (Type[BaseEmbeddingsToolkit]): Embeddings toolkit for the tool if needed.
        model (LlmFactory): LlmFactory for the tool if needed.

    Returns:
        BaseTool: The tool.
    &#34;&#34;&#34;
    from ..Tools.web_search_tool import WebSearchTool
    mapper = {&#39;WebSearchTool&#39;: WebSearchTool}
    tool_class = mapper.get(tool_kwargs.pop(&#39;tool_class&#39;, None))
    if tool_class == None:
        raise ValueError(&#39;&#34;tool_class&#34; must be given.&#39;)
    if tool_kwargs.pop(&#39;model&#39;, False):
        tool_kwargs[&#39;model&#39;] = model
    if tool_kwargs.pop(&#39;embeddings&#39;, False):
        tool_kwargs[&#39;embeddings&#39;] = embeddings
    return tool_class(**tool_kwargs)

def create_streamlit_script(model_kwargs: Dict[str, Any], 
                 embeddings_kwargs: Dict[str, Any],
                 tool_kwargs: List[Dict[str, Any]] = [],
                 auth: Optional[Tuple[str, str]] = None, 
                 debug: bool = False) -&gt; str:
    &#34;&#34;&#34;Create the script to run the streamlit interface.

    Args:
        model_kwargs (Dict[str, Any]): Kwargs to initialise the LLM factory.
        embeddings_kwargs (Dict[str, Any]): Kwargs to initialise the embeddings toolkit.
        tool_kwargs (List[Dict[str, Any]], optional): List of kwargs to initialise the tools. Defaults to [].
        auth (Optional[Tuple[str, str]], optional): Tuple of username and password. Defaults to None.
        debug (bool, optional): Whether to display the debug buttons. Defaults to False.

    Returns:
        str: The streamlit script as a string.
    &#34;&#34;&#34;
    from ..utils import PACKAGE_NAME
    script = [f&#39;from {PACKAGE_NAME}.Frontend.streamlit_interface import StreamlitInterface&#39;, &#39;&#39;]
    script.append(f&#39;model = {str(model_kwargs)}&#39;)
    script.append(f&#39;embeddings = {str(embeddings_kwargs)}&#39;)
    script.append(f&#39;tools = {str(tool_kwargs)}&#39;)
    script.append(f&#39;auth = {auth}&#39;)
    script.append(f&#39;debug = {debug}&#39;)
    script.append(&#39;&#39;)
    script.append(&#39;app = StreamlitInterface(model, embeddings, tools, auth, debug)\napp.launch()&#39;)
    return &#39;\n&#39;.join(script)

def run_streamlit_interface(model_kwargs: Dict[str, Any], 
                 embeddings_kwargs: Dict[str, Any],
                 tool_kwargs: List[Dict[str, Any]] = [],
                 auth: Optional[Tuple[str, str]] = None, 
                 debug: bool = False,
                 app_name: str = PACKAGE_DISPLAY_NAME) -&gt; None:
    &#34;&#34;&#34;Run the streamlit interface.

    Args:
        model_kwargs (Dict[str, Any]): Kwargs to initialise the LLM factory.
        embeddings_kwargs (Dict[str, Any]): Kwargs to initialise the embeddings toolkit.
        tool_kwargs (List[Dict[str, Any]], optional): List of kwargs to initialise the tools. Defaults to [].
        auth (Optional[Tuple[str, str]], optional): Tuple of username and password. Defaults to None.
        debug (bool, optional): Whether to display the debug buttons. Defaults to False.
        app_name (str, optional): name of the streamlit script created. Defaults to PACKAGE_DISPLAY_NAME.
    &#34;&#34;&#34;
    import subprocess
    import os
    from ..utils import get_config
    script_dir = os.path.join(get_config()[&#39;package_home&#39;], &#39;.streamlit_scripts&#39;,f&#39;{app_name}.py&#39;)
    os.makedirs(os.path.dirname(script_dir), exist_ok=True)
    with open(script_dir, &#39;w&#39;) as f:
        f.write(create_streamlit_script(model_kwargs, embeddings_kwargs, tool_kwargs, auth, debug))
    os.system(&#39;streamlit run &#39;+ script_dir)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="llmflex.Frontend.streamlit_interface.create_streamlit_script"><code class="name flex">
<span>def <span class="ident">create_streamlit_script</span></span>(<span>model_kwargs: Dict[str, Any], embeddings_kwargs: Dict[str, Any], tool_kwargs: List[Dict[str, Any]] = [], auth: Optional[Tuple[str, str]] = None, debug: bool = False) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Create the script to run the streamlit interface.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model_kwargs</code></strong> :&ensp;<code>Dict[str, Any]</code></dt>
<dd>Kwargs to initialise the LLM factory.</dd>
<dt><strong><code>embeddings_kwargs</code></strong> :&ensp;<code>Dict[str, Any]</code></dt>
<dd>Kwargs to initialise the embeddings toolkit.</dd>
<dt><strong><code>tool_kwargs</code></strong> :&ensp;<code>List[Dict[str, Any]]</code>, optional</dt>
<dd>List of kwargs to initialise the tools. Defaults to [].</dd>
<dt><strong><code>auth</code></strong> :&ensp;<code>Optional[Tuple[str, str]]</code>, optional</dt>
<dd>Tuple of username and password. Defaults to None.</dd>
<dt><strong><code>debug</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to display the debug buttons. Defaults to False.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>The streamlit script as a string.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_streamlit_script(model_kwargs: Dict[str, Any], 
                 embeddings_kwargs: Dict[str, Any],
                 tool_kwargs: List[Dict[str, Any]] = [],
                 auth: Optional[Tuple[str, str]] = None, 
                 debug: bool = False) -&gt; str:
    &#34;&#34;&#34;Create the script to run the streamlit interface.

    Args:
        model_kwargs (Dict[str, Any]): Kwargs to initialise the LLM factory.
        embeddings_kwargs (Dict[str, Any]): Kwargs to initialise the embeddings toolkit.
        tool_kwargs (List[Dict[str, Any]], optional): List of kwargs to initialise the tools. Defaults to [].
        auth (Optional[Tuple[str, str]], optional): Tuple of username and password. Defaults to None.
        debug (bool, optional): Whether to display the debug buttons. Defaults to False.

    Returns:
        str: The streamlit script as a string.
    &#34;&#34;&#34;
    from ..utils import PACKAGE_NAME
    script = [f&#39;from {PACKAGE_NAME}.Frontend.streamlit_interface import StreamlitInterface&#39;, &#39;&#39;]
    script.append(f&#39;model = {str(model_kwargs)}&#39;)
    script.append(f&#39;embeddings = {str(embeddings_kwargs)}&#39;)
    script.append(f&#39;tools = {str(tool_kwargs)}&#39;)
    script.append(f&#39;auth = {auth}&#39;)
    script.append(f&#39;debug = {debug}&#39;)
    script.append(&#39;&#39;)
    script.append(&#39;app = StreamlitInterface(model, embeddings, tools, auth, debug)\napp.launch()&#39;)
    return &#39;\n&#39;.join(script)</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.embeddings_loader"><code class="name flex">
<span>def <span class="ident">embeddings_loader</span></span>(<span>embeddings_kwargs: Dict[str, Any]) ‑> <a title="llmflex.Embeddings.base_embeddings.BaseEmbeddingsToolkit" href="../Embeddings/base_embeddings.html#llmflex.Embeddings.base_embeddings.BaseEmbeddingsToolkit">BaseEmbeddingsToolkit</a></span>
</code></dt>
<dd>
<div class="desc"><p>Load the embeddings given the kwargs.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>embeddings_kwargs</code></strong> :&ensp;<code>Dict[str, Any]</code></dt>
<dd>Kwargs to initialise the embeddings toolkit.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>BaseEmbeddingsToolkit</code></dt>
<dd>The embeddings toolkit.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def embeddings_loader(embeddings_kwargs: Dict[str, Any]) -&gt; BaseEmbeddingsToolkit:
    &#34;&#34;&#34;Load the embeddings given the kwargs.

    Args:
        embeddings_kwargs (Dict[str, Any]): Kwargs to initialise the embeddings toolkit.

    Returns:
        BaseEmbeddingsToolkit: The embeddings toolkit.
    &#34;&#34;&#34;
    mapper = {&#34;HuggingfaceEmbeddingsToolkit&#34;: HuggingfaceEmbeddingsToolkit, &#39;APIEmbeddingsToolkit&#39;: APIEmbeddingsToolkit}
    model_key = embeddings_kwargs.pop(&#39;embeddings_class&#39;, &#34;HuggingfaceEmbeddingsToolkit&#34;)
    return mapper.get(model_key)(**embeddings_kwargs)</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.run_streamlit_interface"><code class="name flex">
<span>def <span class="ident">run_streamlit_interface</span></span>(<span>model_kwargs: Dict[str, Any], embeddings_kwargs: Dict[str, Any], tool_kwargs: List[Dict[str, Any]] = [], auth: Optional[Tuple[str, str]] = None, debug: bool = False, app_name: str = 'LLMFlex') ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Run the streamlit interface.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model_kwargs</code></strong> :&ensp;<code>Dict[str, Any]</code></dt>
<dd>Kwargs to initialise the LLM factory.</dd>
<dt><strong><code>embeddings_kwargs</code></strong> :&ensp;<code>Dict[str, Any]</code></dt>
<dd>Kwargs to initialise the embeddings toolkit.</dd>
<dt><strong><code>tool_kwargs</code></strong> :&ensp;<code>List[Dict[str, Any]]</code>, optional</dt>
<dd>List of kwargs to initialise the tools. Defaults to [].</dd>
<dt><strong><code>auth</code></strong> :&ensp;<code>Optional[Tuple[str, str]]</code>, optional</dt>
<dd>Tuple of username and password. Defaults to None.</dd>
<dt><strong><code>debug</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to display the debug buttons. Defaults to False.</dd>
<dt><strong><code>app_name</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>name of the streamlit script created. Defaults to PACKAGE_DISPLAY_NAME.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_streamlit_interface(model_kwargs: Dict[str, Any], 
                 embeddings_kwargs: Dict[str, Any],
                 tool_kwargs: List[Dict[str, Any]] = [],
                 auth: Optional[Tuple[str, str]] = None, 
                 debug: bool = False,
                 app_name: str = PACKAGE_DISPLAY_NAME) -&gt; None:
    &#34;&#34;&#34;Run the streamlit interface.

    Args:
        model_kwargs (Dict[str, Any]): Kwargs to initialise the LLM factory.
        embeddings_kwargs (Dict[str, Any]): Kwargs to initialise the embeddings toolkit.
        tool_kwargs (List[Dict[str, Any]], optional): List of kwargs to initialise the tools. Defaults to [].
        auth (Optional[Tuple[str, str]], optional): Tuple of username and password. Defaults to None.
        debug (bool, optional): Whether to display the debug buttons. Defaults to False.
        app_name (str, optional): name of the streamlit script created. Defaults to PACKAGE_DISPLAY_NAME.
    &#34;&#34;&#34;
    import subprocess
    import os
    from ..utils import get_config
    script_dir = os.path.join(get_config()[&#39;package_home&#39;], &#39;.streamlit_scripts&#39;,f&#39;{app_name}.py&#39;)
    os.makedirs(os.path.dirname(script_dir), exist_ok=True)
    with open(script_dir, &#39;w&#39;) as f:
        f.write(create_streamlit_script(model_kwargs, embeddings_kwargs, tool_kwargs, auth, debug))
    os.system(&#39;streamlit run &#39;+ script_dir)</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.tool_loader"><code class="name flex">
<span>def <span class="ident">tool_loader</span></span>(<span>tool_kwargs: Dict[str, Any], embeddings: Type[<a title="llmflex.Embeddings.base_embeddings.BaseEmbeddingsToolkit" href="../Embeddings/base_embeddings.html#llmflex.Embeddings.base_embeddings.BaseEmbeddingsToolkit">BaseEmbeddingsToolkit</a>], model: <a title="llmflex.Models.Factory.llm_factory.LlmFactory" href="../Models/Factory/llm_factory.html#llmflex.Models.Factory.llm_factory.LlmFactory">LlmFactory</a>) ‑> <a title="llmflex.Tools.base_tool.BaseTool" href="../Tools/base_tool.html#llmflex.Tools.base_tool.BaseTool">BaseTool</a></span>
</code></dt>
<dd>
<div class="desc"><p>Load the embeddings given the kwargs.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tool_kwargs</code></strong> :&ensp;<code>Dict[str, Any]</code></dt>
<dd>Kwargs to initialise the tool.</dd>
<dt><strong><code>embeddings</code></strong> :&ensp;<code>Type[BaseEmbeddingsToolkit]</code></dt>
<dd>Embeddings toolkit for the tool if needed.</dd>
<dt><strong><code>model</code></strong> :&ensp;<code>LlmFactory</code></dt>
<dd>LlmFactory for the tool if needed.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>BaseTool</code></dt>
<dd>The tool.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tool_loader(tool_kwargs: Dict[str, Any], embeddings: Type[BaseEmbeddingsToolkit], model: LlmFactory) -&gt; BaseTool:
    &#34;&#34;&#34;Load the embeddings given the kwargs.

    Args:
        tool_kwargs (Dict[str, Any]): Kwargs to initialise the tool.
        embeddings (Type[BaseEmbeddingsToolkit]): Embeddings toolkit for the tool if needed.
        model (LlmFactory): LlmFactory for the tool if needed.

    Returns:
        BaseTool: The tool.
    &#34;&#34;&#34;
    from ..Tools.web_search_tool import WebSearchTool
    mapper = {&#39;WebSearchTool&#39;: WebSearchTool}
    tool_class = mapper.get(tool_kwargs.pop(&#39;tool_class&#39;, None))
    if tool_class == None:
        raise ValueError(&#39;&#34;tool_class&#34; must be given.&#39;)
    if tool_kwargs.pop(&#39;model&#39;, False):
        tool_kwargs[&#39;model&#39;] = model
    if tool_kwargs.pop(&#39;embeddings&#39;, False):
        tool_kwargs[&#39;embeddings&#39;] = embeddings
    return tool_class(**tool_kwargs)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="llmflex.Frontend.streamlit_interface.InterfaceState"><code class="flex name class">
<span>class <span class="ident">InterfaceState</span></span>
<span>(</span><span>model: <a title="llmflex.Models.Factory.llm_factory.LlmFactory" href="../Models/Factory/llm_factory.html#llmflex.Models.Factory.llm_factory.LlmFactory">LlmFactory</a>, embeddings: Type[<a title="llmflex.Embeddings.base_embeddings.BaseEmbeddingsToolkit" href="../Embeddings/base_embeddings.html#llmflex.Embeddings.base_embeddings.BaseEmbeddingsToolkit">BaseEmbeddingsToolkit</a>], tools: List[Type[<a title="llmflex.Tools.base_tool.BaseTool" href="../Tools/base_tool.html#llmflex.Tools.base_tool.BaseTool">BaseTool</a>]] = [])</span>
</code></dt>
<dd>
<div class="desc"><p>Initialise the backend of the Streamlit interface.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>LlmFactory</code></dt>
<dd>LLM factory.</dd>
<dt><strong><code>embeddings</code></strong> :&ensp;<code>Type[BaseEmbeddingsToolkit]</code></dt>
<dd>Embeddings toolkit.</dd>
<dt><strong><code>tools</code></strong> :&ensp;<code>List[Type[BaseTool]]</code>, optional</dt>
<dd>List of tools. Defaults to [].</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class InterfaceState:

    def __init__(self, model: LlmFactory, embeddings: Type[BaseEmbeddingsToolkit], tools: List[Type[BaseTool]] = []) -&gt; None:
        &#34;&#34;&#34;Initialise the backend of the Streamlit interface.

        Args:
            model (LlmFactory): LLM factory.
            embeddings (Type[BaseEmbeddingsToolkit]): Embeddings toolkit.
            tools (List[Type[BaseTool]], optional): List of tools. Defaults to [].
        &#34;&#34;&#34;
        from ..Memory.assistant_long_term_memory import AssistantLongTermChatMemory
        from ..Tools.tool_selection import ToolSelector
        from ..TextSplitters.sentence_token_text_splitter import SentenceTokenTextSplitter
        from ..Prompts.prompt_template import DEFAULT_SYSTEM_MESSAGE
        self.model = model
        self.embeddings = embeddings
        self.text_splitter = SentenceTokenTextSplitter(count_token_fn=model().get_num_tokens, chunk_size=200, chunk_overlap=40)
        self.memory = AssistantLongTermChatMemory(title=&#39;Untitled 0&#39;, embeddings=self.embeddings, text_splitter=self.text_splitter, from_exist=False)
        self.system = DEFAULT_SYSTEM_MESSAGE
        self.template = self.model.prompt_template
        self.llm = self.model(stop=self.template.stop + [&#39;#####&#39;])
        self.short_limit = 600
        self.long_limit = 500
        self.score_threshold = 0.5
        self.tool_selector = ToolSelector(tools, model=self.model, embeddings=self.embeddings) if len(tools) &gt; 0 else None

    @property
    def titles(self) -&gt; List[str]:
        &#34;&#34;&#34;All existing chat titles.

        Returns:
            List[str]: All existing chat titles.
        &#34;&#34;&#34;
        from ..Memory.base_memory import list_titles
        return list_titles()
    
    @property
    def current_title(self) -&gt; str:
        &#34;&#34;&#34;Current memory chat title.

        Returns:
            str: Current memory chat title.
        &#34;&#34;&#34;
        return self.memory.title
      
    @property
    def history(self) -&gt; List[List[str]]:
        &#34;&#34;&#34;Current conversation history.

        Returns:
            List[List[str]]: Current conversation history.
        &#34;&#34;&#34;
        return self.memory.history
       
    @property
    def presets(self) -&gt; List[str]:
        &#34;&#34;&#34;List of prompt templates presets.

        Returns:
            List[str]: List of prompt templates presets.
        &#34;&#34;&#34;
        from ..Prompts.prompt_template import presets
        return list(presets.keys())</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="llmflex.Frontend.streamlit_interface.InterfaceState.current_title"><code class="name">var <span class="ident">current_title</span> : str</code></dt>
<dd>
<div class="desc"><p>Current memory chat title.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>Current memory chat title.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def current_title(self) -&gt; str:
    &#34;&#34;&#34;Current memory chat title.

    Returns:
        str: Current memory chat title.
    &#34;&#34;&#34;
    return self.memory.title</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.InterfaceState.history"><code class="name">var <span class="ident">history</span> : List[List[str]]</code></dt>
<dd>
<div class="desc"><p>Current conversation history.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>List[List[str]]</code></dt>
<dd>Current conversation history.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def history(self) -&gt; List[List[str]]:
    &#34;&#34;&#34;Current conversation history.

    Returns:
        List[List[str]]: Current conversation history.
    &#34;&#34;&#34;
    return self.memory.history</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.InterfaceState.presets"><code class="name">var <span class="ident">presets</span> : List[str]</code></dt>
<dd>
<div class="desc"><p>List of prompt templates presets.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>List[str]</code></dt>
<dd>List of prompt templates presets.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def presets(self) -&gt; List[str]:
    &#34;&#34;&#34;List of prompt templates presets.

    Returns:
        List[str]: List of prompt templates presets.
    &#34;&#34;&#34;
    from ..Prompts.prompt_template import presets
    return list(presets.keys())</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.InterfaceState.titles"><code class="name">var <span class="ident">titles</span> : List[str]</code></dt>
<dd>
<div class="desc"><p>All existing chat titles.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>List[str]</code></dt>
<dd>All existing chat titles.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def titles(self) -&gt; List[str]:
    &#34;&#34;&#34;All existing chat titles.

    Returns:
        List[str]: All existing chat titles.
    &#34;&#34;&#34;
    from ..Memory.base_memory import list_titles
    return list_titles()</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface"><code class="flex name class">
<span>class <span class="ident">StreamlitInterface</span></span>
<span>(</span><span>model_kwargs: Dict[str, Any], embeddings_kwargs: Dict[str, Any], tool_kwargs: List[Dict[str, Any]] = [], auth: Optional[Tuple[str, str]] = None, debug: bool = False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class StreamlitInterface:

    def __init__(self, model_kwargs: Dict[str, Any], 
                 embeddings_kwargs: Dict[str, Any],
                 tool_kwargs: List[Dict[str, Any]] = [],
                 auth: Optional[Tuple[str, str]] = None, 
                 debug: bool = False) -&gt; None:
        if not hasattr(st.session_state, &#39;backend&#39;):
            model = LlmFactory(**model_kwargs)
            embeddings = embeddings_loader(embeddings_kwargs)
            tools = list(map(lambda x: tool_loader(x, embeddings, model), tool_kwargs))
            st.session_state.backend = InterfaceState(model, embeddings, tools=tools)
        self.debug = debug
        self._auth = auth
        if auth is None:
            st.session_state.islogin = True
        self.sidebar_ratio = [5, 1]

    @property
    def backend(self) -&gt; InterfaceState:
        return st.session_state.backend
        
    @property
    def islogin(self) -&gt; bool:
        if not hasattr(st.session_state, &#39;islogin&#39;):
            st.session_state.islogin = False
        return st.session_state.islogin
    
    @property
    def mobile(self) -&gt; bool:
        if not hasattr(st.session_state, &#39;mobile&#39;):
            st.session_state.mobile = False
        return st.session_state.mobile

    @property
    def login_wrong(self) -&gt; bool:
        if not hasattr(st.session_state, &#39;login_wrong&#39;):
            st.session_state.login_wrong = False
        return st.session_state.login_wrong

    @property
    def conversation_delete(self) -&gt; bool:
        if not hasattr(st.session_state, &#39;conversation_delete&#39;):
            st.session_state.conversation_delete = False
        return st.session_state.conversation_delete

    @property
    def generating(self) -&gt; bool:
        &#34;&#34;&#34;Whether chatbot is generating.&#34;&#34;&#34;
        if not hasattr(st.session_state, &#39;generating&#39;):
            st.session_state.generating = False
        return st.session_state.generating
    
    @property
    def experimental(self) -&gt; bool:
        if not hasattr(st.session_state, &#39;allow_ai_start&#39;):
            st.session_state.allow_ai_start = False
        return st.session_state.allow_ai_start

    @property
    def history_dict(self) -&gt; List[Dict[str, Any]]:
        if not hasattr(st.session_state, &#39;history&#39;):
            self.refresh_history()
        elif st.session_state.history[&#39;current_title&#39;] != self.backend.current_title:
            self.refresh_history()
        return st.session_state.history[&#39;history_dict&#39;]
    
    @property
    def generation_config(self) -&gt; Dict[str, str]:
        if not hasattr(st.session_state, &#39;generation_config&#39;):
            st.session_state.generation_config = dict(gen_type=&#39;none&#39;)
        return st.session_state.generation_config
    
    @property
    def ai_start_text(self) -&gt; str:
        if not hasattr(st.session_state, &#39;ai_start_text&#39;):
            st.session_state.ai_start_text = &#39;&#39;
        return st.session_state.ai_start_text

    @property
    def generation_time_info(self) -&gt; str:
        template = &#39;  \nGeneration time taken: &#39;
        if not hasattr(st.session_state, &#39;generation_time_info&#39;):
            st.session_state.generation_time_info = &#39;--&#39;
        if type(st.session_state.generation_time_info) == str:
            return template + st.session_state.generation_time_info
        else:
            return template + f&#39;{round(st.session_state.generation_time_info, 2)}&#39; + &#39;s&#39;

    @property
    def tool_states(self) -&gt; Dict[str, bool]:
        if not hasattr(st.session_state, &#39;tool_states&#39;):
            st.session_state.tool_states = dict()
            for tool in self.backend.tool_selector.tools:
                st.session_state.tool_states[tool.name] = True
        return st.session_state.tool_states

    def get_tool(self, user_input: str) -&gt; Optional[Type[BaseTool]]:
        if self.backend.tool_selector is None:
            return None
        if sum(list(self.tool_states.values())) == 0:
            return None
        self.backend.tool_selector.set_score_threshold(self.tool_threshold)
        history = self.backend.memory.get_token_memory(llm=self.backend.llm, token_limit=self.backend.short_limit)
        system = self.backend.system
        tool = self.backend.tool_selector.get_tool(user_input=user_input, history=history, system=system)
        print(f&#39;Tool: {self.backend.tool_selector.last_tool}\nScore: {self.backend.tool_selector.last_score}&#39;)
        if tool is not None:
            if not self.tool_states[tool.name]:
                print(tool.name + &#39; disabled.&#39;)
                tool = None
        return tool
    
    def run_tool(self, tool: BaseTool, user_input: str) -&gt; Iterator[Union[str, Tuple[str, str], Iterator[str]]]:
        recent_history = self.backend.memory.get_token_memory(llm=self.backend.llm, token_limit=self.backend.short_limit)
        return tool.run_with_chat(tool_input=user_input, llm=self.backend.llm, stream=True, history=recent_history)

    def login_with_cred(self, user: str, password: str) -&gt; None:
        if ((user == self._auth[0]) &amp; (password == self._auth[1])):
            print(user, password)
            st.session_state.islogin = True
        else:
            st.session_state.login_wrong = True
        st.rerun()

    def toggle_generating(self) -&gt; None:
        st.session_state.generating = not self.generating
        # print(f&#39;Generating = {self.generating}&#39;)

    def toggle_mobile(self) -&gt; None:
        st.session_state.mobile = not self.mobile

    def toggle_tool(self, tool_name: str) -&gt; None:
        if not self.generating:
            self.tool_states[tool_name] = not self.tool_states[tool_name]

    def toggle_conversation_delete(self) -&gt; None:
        st.session_state.conversation_delete = not self.conversation_delete

    def set_mobile(self) -&gt; None:
        if not self.generating:
            st.session_state.mobile = st.session_state.mobile_toggle

    def current_prompt_index(self) -&gt; int:
        from ..Prompts.prompt_template import presets
        template = self.backend.model.prompt_template.template_name
        for i, t in enumerate(self.backend.presets):
            if template == t:
                return i
        return 0
    
    def set_time_info(self, time: Union[str, float] = &#39;--&#39;) -&gt; None:
        st.session_state.generation_time_info = time     

    def add_chat(self, title: str) -&gt; None:
        if not self.generating:
            title = title.strip(&#39; \r\n\t&#39;)
            if title == &#39;&#39;:
                pass
            else:
                from ..Memory.assistant_long_term_memory import AssistantLongTermChatMemory
                from ..Prompts.prompt_template import DEFAULT_SYSTEM_MESSAGE
                self.backend.memory = AssistantLongTermChatMemory(title, embeddings=self.backend.embeddings, text_splitter=self.backend.text_splitter, from_exist=True)
                self.backend.system = self.backend.memory.vectordb._info.get(&#39;system&#39;, DEFAULT_SYSTEM_MESSAGE)
                self.backend.memory.vectordb._info[&#39;system&#39;] = self.backend.system
                self.backend.memory.save()
                st.rerun()

    def switch_chat(self, title: str) -&gt; None:
        if not self.generating:
            from ..Memory.assistant_long_term_memory import AssistantLongTermChatMemory
            from ..Prompts.prompt_template import DEFAULT_SYSTEM_MESSAGE
            st.session_state.generating = True
            self.set_time_info()
            if title == self.backend.current_title:
                pass
            else:
                print(f&#39;Switch to: {title}&#39;)
                from_exist = title != &#39;Untitled 0&#39;
                self.backend.memory = AssistantLongTermChatMemory(title=title, embeddings=self.backend.embeddings, text_splitter=self.backend.text_splitter, from_exist=from_exist)
                self.backend.system = self.backend.memory.info.get(&#39;system&#39;, DEFAULT_SYSTEM_MESSAGE)
                self.backend.memory.save()
            st.session_state.generating = False

    def delete_chat(self, title: str) -&gt; None:
        if not self.generating:
            switch = title == self.backend.current_title
            from shutil import rmtree
            if switch:
                rmtree(self.backend.memory.chat_dir)
                self.switch_chat(&#39;Untitled 0&#39;)
            else:
                from ..Memory.assistant_long_term_memory import AssistantLongTermChatMemory
                mem = AssistantLongTermChatMemory(title, embeddings=self.backend.embeddings, text_splitter=self.backend.text_splitter, from_exist=False)
                rmtree(mem.chat_dir)
        
    def toggle_exeperimental(self) -&gt; None:
        if not self.generating:
            st.session_state.allow_ai_start = not st.session_state.allow_ai_start

    def set_exeperimental(self) -&gt; None:
        if not self.generating:
            st.session_state.allow_ai_start = st.session_state.use_ai_start

    def set_prompt_template(self) -&gt; None:
        if not self.generating:
            from ..Prompts.prompt_template import PromptTemplate
            preset = st.session_state.prompt_format
            self.backend.template = PromptTemplate.from_preset(preset)

    def set_system_message(self, system: str) -&gt; None:
        if not self.generating:
            self.backend.system = system.strip(&#39; \n\r\t&#39;)
            self.backend.memory.vectordb._info[&#39;system&#39;] = self.backend.system
            self.backend.memory.save()

    def set_memory_settings(self, long: int, short: int, score: float) -&gt; None:
        if not self.generating:
            self.backend.short_limit = short
            self.backend.long_limit = long
            self.backend.score_threshold = score

    def set_llm_config(self, temperature: float, max_new_tokens: int, repetition_penalty: float, top_p: float, top_k: int) -&gt; None:
        if not self.generating:
            self.backend.llm = self.backend.model(temperature=temperature, max_new_tokens=max_new_tokens, repetition_penalty=repetition_penalty, top_p=top_p, top_k=top_k)

    def get_history(self, mem) -&gt; List[Dict[str, Any]]:
        history = list(map(lambda x: dict(
            user=x[&#39;metadata&#39;][&#39;user&#39;], 
            assistant=x[&#39;metadata&#39;][&#39;assistant&#39;], 
            order=x[&#39;metadata&#39;][&#39;order&#39;],
            tool_details=x[&#39;metadata&#39;].get(&#39;tool_details&#39;, None),
            footnote=x[&#39;metadata&#39;].get(&#39;footnote&#39;, None),
            tool_name=x[&#39;metadata&#39;].get(&#39;tool_name&#39;, None)
        ), mem._data))
        if len(history) == 0:
            return []
        count = max(list(map(lambda x: x[&#39;order&#39;], history))) + 1
        history = list(map(lambda x: list(filter(lambda y: y[&#39;order&#39;] == x, history))[0], range(count)))
        history.sort(key=lambda x: x[&#39;order&#39;], reverse=False)
        return history

    def refresh_history(self) -&gt; None:
        st.session_state.history = dict(
            current_title=self.backend.current_title,
            history_dict=self.get_history(self.backend.memory)
        )

    def save_interaction(self, user: str, assistant: str, **kwargs) -&gt; None:
        self.backend.memory.save_interaction(user_input=user, assistant_output=assistant, **kwargs)
        self.refresh_history()

    def input_template(self, user: Optional[str], ai_start: str) -&gt; Optional[Dict[str, Any]]:
        if user is None:
            return None
        user = user.strip(&#39; \n\r\t&#39;)
        if user == &#39;&#39;:
            return None
        input_dict = dict(
            user = user.strip(&#39; \n\r\t&#39;),
            assistant = ai_start,
            order = self.backend.memory.interaction_count,
            tool_details = None,
            footnote = None,
            tool_name = None
        )
        return input_dict

    def create_generation_config(self, gen_type: Literal[&#39;new&#39;, &#39;retry&#39;, &#39;continue&#39;, &#39;none&#39;], user_input: str, ai_start: Optional[str] = None) -&gt; None:
        gen_obj = dict(
            gen_type = gen_type,
            user_input = user_input.strip(&#39; \r\n\t&#39;),
            ai_start = self.ai_start if ai_start is None else ai_start
        )
        st.session_state.generation_config = gen_obj

    def get_generation_iterator(self) -&gt; Iterator[str]:
        from ..Memory.assistant_long_term_memory import create_long_assistant_memory_prompt
        config = self.generation_config
        prompt = create_long_assistant_memory_prompt(
            user=config[&#39;user_input&#39;],
            prompt_template=self.backend.template,
            llm=self.backend.llm,
            memory=self.backend.memory,
            system=self.backend.system,
            short_token_limit=self.backend.short_limit,
            long_token_limit=self.backend.long_limit,
            score_threshold=self.backend.score_threshold
        ) + config[&#39;ai_start&#39;]
        print(f&#39;Number of input tokens: {self.backend.llm.get_num_tokens(prompt)}&#39;)
        def generator():
            yield config[&#39;ai_start&#39;]
            for i in self.backend.llm.stream(prompt):
                yield i
        return generator()

    def retry_response(self, cont: bool = False, ai_start: str = &#39;&#39;) -&gt; None:
        if not self.generating:
            if len(self.history_dict) == 0:
                return None
            print(ai_start)
            st.session_state.ai_start_text = ai_start
            history = self.get_history(self.backend.memory)
            user = history[-1][&#39;user&#39;]
            self.create_generation_config(&#39;continue&#39; if cont else &#39;retry&#39;, user, history[-1][&#39;assistant&#39;] if cont else ai_start)
            input_dict = self.input_template(user=user, ai_start=history[-1][&#39;assistant&#39;] if cont else ai_start)
            self.backend.memory.remove_last_interaction()
            self.refresh_history()
            self.history_dict.append(input_dict)
            self.toggle_generating()

    def remove_last(self) -&gt; None:
        if not self.generating:
            self.backend.memory.remove_last_interaction()
            self.refresh_history()

    ##### Defining the interface
        
    def login(self) -&gt; None:
        login_form = st.form(key=&#39;login&#39;)
        with login_form:
            user = st.text_input(label=&#39;Username:&#39;, placeholder=&#39;Your username...&#39;)
            password = st.text_input(label=&#39;Password&#39;, placeholder=&#39;Your password...&#39;, type=&#39;password&#39;)
            if st.form_submit_button(label=&#39;Login&#39;):
                print(&#39;Triggered by btn.&#39;)
                self.login_with_cred(user=user, password=password)
            if self.login_wrong:
                st.warning(&#39;Incorrect credentials. Please try again.&#39;)

    def sidebar(self) -&gt; None:
        &#34;&#34;&#34;Sidebar of the webapp.
        &#34;&#34;&#34;
        app_summary = [&#39;Powered by:&#39;, 
                       f&#39;* LLM: {self.backend.model.model_id}&#39;, 
                       f&#39;* Embedding model: {self.backend.embeddings.name}&#39;, 
                       &#39;&#39;, 
                       &#39;Current conversation:&#39;, 
                       f&#39;* {self.backend.current_title}&#39;,
                       &#39;&#39;,
                       &#39;Current prompt format:&#39;,
                       f&#39;* {self.backend.template.template_name}&#39;]
        st.header(PACKAGE_DISPLAY_NAME.upper(), help=&#39;  \n&#39;.join(app_summary), divider=&#34;grey&#34;)
        st.subheader(&#39;:left_speech_bubble: Conversations&#39;)
        self.new_chat_form()
        with st.expander(label=&#39;Previous conversations&#39;):
            self.conversations()
        self.settings()
        if self.debug:
            st.subheader(&#39;Debug&#39;)
            self.test_buttons()

    def new_chat_form(self) -&gt; None:
        with st.form(key=&#39;new_chat_form&#39;, border=False, clear_on_submit=True):
            cols = st.columns(self.sidebar_ratio)
            with cols[0]:
                self.new_title = st.text_input(label=&#39;new_title&#39;, max_chars=40, placeholder=&#39;New conversation title here...&#39;, label_visibility=&#39;collapsed&#39;, disabled=self.generating)
            with cols[1]:
                if st.form_submit_button(label=&#39;:heavy_plus_sign:&#39;, disabled=self.generating):
                    self.add_chat(title=self.new_title)

    def conversations(self) -&gt; None:
        &#34;&#34;&#34;List of conversations.&#34;&#34;&#34;
        for title in self.backend.titles:
            if self.conversation_delete:
                cols = st.columns(self.sidebar_ratio)
                with cols[0]:
                    btn_type = &#39;primary&#39; if self.backend.current_title == title else &#39;secondary&#39;
                    st.button(label=title.title(), key=f&#39;{title}_select&#39;, disabled=self.generating, 
                            use_container_width=True, type=btn_type, on_click=self.switch_chat, kwargs=dict(title=title))
                with cols[1]:
                    st.button(label=&#39;:heavy_minus_sign:&#39;, key=f&#39;{title}_delete&#39;, disabled=self.generating, on_click=self.delete_chat, kwargs=dict(title=title))
            else:
                btn_type = &#39;primary&#39; if self.backend.current_title == title else &#39;secondary&#39;
                st.button(label=title.title(), key=f&#39;{title}_select&#39;, disabled=self.generating, 
                        use_container_width=True, type=btn_type, on_click=self.switch_chat, kwargs=dict(title=title))
        cols = st.columns(self.sidebar_ratio)
        with cols[0]:
            st.toggle(label=&#39;:wastebasket:&#39;, key=f&#39;conv_delete&#39;, disabled=self.generating, on_change=self.toggle_conversation_delete, help=&#39;Select conversations to remove.&#39;)

    def test_buttons(self) -&gt; None:
        cols = st.columns([1, 1])
        with cols[0]:
            st.button(&#39;Test print&#39;, key=&#39;test&#39;, on_click=lambda: print(self.ai_start))

        with cols[1]:
            st.button(&#39;gen_toggle&#39;, key=&#39;toggle_gen&#39;, on_click=lambda: self.toggle_generating())

    def settings(self) -&gt; None:
        &#34;&#34;&#34;Settings of the webapp.&#34;&#34;&#34;
        st.subheader(&#39;:gear: Settings&#39;)
        st.toggle(label=&#39;Experimental&#39;, 
                    value=self.experimental,
                    disabled=self.generating,
                    on_change=self.set_exeperimental,
                    key=&#39;use_ai_start&#39;, 
                    help=&#39;More features such as retrying, adding response starting message etc.&#39;)
        st.toggle(label=&#39;Mobile&#39;, 
                    value=self.mobile,
                    disabled=self.generating,
                    on_change=self.set_mobile,
                    key=&#39;mobile_toggle&#39;, 
                    help=&#39;Better layout for mobile device.&#39;)
        with st.expander(label=&#39;Prompt format settings&#39;):
            self.prompt_template_settings()
        with st.expander(label=&#39;System message settings&#39;):
            self.system_prompt_settings()
        with st.expander(label=&#39;Memory settings&#39;):
            self.memory_settings()
        with st.expander(label=&#39;Model settings&#39;):
            self.llm_settings()
        self.tool_settings()

    def prompt_template_settings(self) -&gt; None:
        &#34;&#34;&#34;Prompt template settings.&#34;&#34;&#34;
        from ..Prompts.prompt_template import presets
        format = st.selectbox(label=&#39;prompt_formats&#39;, 
                     label_visibility=&#39;collapsed&#39;,
                     key=&#39;prompt_format&#39;,
                     options=list(presets.keys()), 
                     disabled=self.generating, 
                     index=self.current_prompt_index(),
                     on_change=self.set_prompt_template)
        
    def system_prompt_settings(self) -&gt; None:
        &#34;&#34;&#34;System prompt settings.&#34;&#34;&#34;
        self.system_text = st.text_area(label=&#39;System message&#39;, height=250, key=&#39;system_msg&#39;,label_visibility=&#39;collapsed&#39;, value=self.backend.system, disabled=self.generating)
        st.markdown(f&#39;System message token count: {self.backend.llm.get_num_tokens(self.backend.system)}&#39;)
        st.button(label=&#39;:floppy_disk:&#39;, key=&#39;system_save&#39;, disabled=self.generating, use_container_width=True, on_click=self.set_system_message, kwargs=dict(system=self.system_text))

    def memory_settings(self) -&gt; None:
        &#34;&#34;&#34;Memory token limit settings.&#34;&#34;&#34;
        self.short_limit_slidder = st.slider(&#39;Short term memory token limit&#39;, min_value=0, max_value=6000, step=1, value=self.backend.short_limit, disabled=self.generating)
        self.long_limit_slidder = st.slider(&#39;Long term memory token limit&#39;, min_value=0, max_value=6000, step=1, value=self.backend.long_limit, disabled=self.generating)
        self.score_threshold_slidder = st.slider(&#39;Score threshold for long term memory&#39;, min_value=0.0, max_value=1.0, step=0.01, value=self.backend.score_threshold, disabled=self.generating)
        summary = [
            &#39;Current settings:&#39;,
            f&#39;Short term memory token limit: {self.backend.short_limit}&#39;,
            f&#39;Long term memory token limit: {self.backend.long_limit}&#39;,
            f&#39;Score threshold: {self.backend.score_threshold}&#39;

        ]
        st.markdown(&#39;  \n&#39;.join(summary))
        st.button(label=&#39;:floppy_disk:&#39;, key=&#39;memory_token_save&#39;, disabled=self.generating, 
                  use_container_width=True,
                  on_click=self.set_memory_settings,
                  kwargs=dict(short=self.short_limit_slidder,
                              long=self.long_limit_slidder,
                              score=self.score_threshold_slidder))

    def llm_settings(self) -&gt; None:
        &#34;&#34;&#34;LLM generation settings.&#34;&#34;&#34;
        self.temperature_slidder = st.slider(&#39;Temparature&#39;, min_value=0.0, max_value=2.0, step=0.01, value=self.backend.llm.generation_config[&#39;temperature&#39;], disabled=self.generating)
        self.max_new_token_slidder = st.slider(&#39;Maximum number of new tokens&#39;, min_value=0, max_value=4096, step=1, value=self.backend.llm.generation_config[&#39;max_new_tokens&#39;], disabled=self.generating)
        self.repetition_slidder = st.slider(&#39;Repetition penalty&#39;, min_value=1.0, max_value=2.0, step=0.01, value=self.backend.llm.generation_config[&#39;repetition_penalty&#39;], disabled=self.generating)
        self.topp_slidder = st.slider(&#39;Top P&#39;, min_value=0.0, max_value=1.0, step=0.01, value=self.backend.llm.generation_config[&#39;top_p&#39;], disabled=self.generating)
        self.topk_slidder = st.slider(&#39;Top K&#39;, min_value=0, max_value=30000, step=1, value=self.backend.llm.generation_config[&#39;top_k&#39;], disabled=self.generating)
        summary = [
            &#39;Current settings:&#39;,
            f&#34;Temperature: {self.backend.llm.generation_config[&#39;temperature&#39;]}&#34;,
            f&#34;Max new tokens: {self.backend.llm.generation_config[&#39;max_new_tokens&#39;]}&#34;,
            f&#34;Repetition penalty: {self.backend.llm.generation_config[&#39;repetition_penalty&#39;]}&#34;,
            f&#34;Top P: {self.backend.llm.generation_config[&#39;top_p&#39;]}&#34;,
            f&#34;Top K: {self.backend.llm.generation_config[&#39;top_k&#39;]}&#34;,
        ]
        st.markdown(&#39;  \n&#39;.join(summary))
        st.button(label=&#39;:floppy_disk:&#39;, 
                  key=&#39;llm_config_save&#39;, 
                  disabled=self.generating, 
                  use_container_width=True,
                  on_click=self.set_llm_config,
                  kwargs=dict(temperature=self.temperature_slidder,
                              max_new_tokens=self.max_new_token_slidder,
                              repetition_penalty=self.repetition_slidder,
                              top_p=self.topp_slidder,
                              top_k=self.topk_slidder))
        
    def tool_settings(self) -&gt; None:
        if self.backend.tool_selector is not None:
            with st.expander(&#39;Tools settings&#39;):
                for i in self.backend.tool_selector.tools:
                    st.toggle(label=f&#39;{i.pretty_name}&#39;, value=self.tool_states[i.name], on_change=lambda: self.toggle_tool(i.name),
                              disabled=self.generating)
                self.tool_threshold = st.slider(label=&#39;Tool trigger threshold&#39;, min_value=0.0, max_value=1.0, step=0.005, 
                                                value=self.backend.tool_selector.score_threshold, disabled=self.generating)

    def chatbot(self) -&gt; None:
        self.conversation_history()
        
        self.ai_start_textbox()
        self.user_input_box()

    def conversation_history(self) -&gt; None:
        history = self.history_dict
        last = len(history) - 1
        gen_type = self.generation_config[&#39;gen_type&#39;]
        for i, ex in enumerate(history):
            with st.chat_message(name=&#39;user&#39;):
                st.markdown(ex[&#39;user&#39;], help=f&#39;Number of tokens: {self.backend.llm.get_num_tokens(ex[&#34;user&#34;])}&#39;)
            with st.chat_message(name=&#39;assistant&#39;):
                self.assistant_response(ex, i, last)
            if ((gen_type != &#39;none&#39;) &amp; (i == last)):
                st.session_state.generation_config = dict(gen_type=&#39;none&#39;)
                st.session_state.retry_or_continue = False
                self.toggle_generating()
                st.rerun()

    def assistant_response(self, ex: Dict[str, Any], i: int, last: int) -&gt; None:
        if ex[&#39;tool_details&#39;] is not None:
            with st.status(label=f&#34;:hammer_and_pick: __{ex[&#39;tool_name&#39;]}__&#34;, state=&#39;complete&#39;):
                st.code(ex[&#39;tool_details&#39;][1], language=&#39;plaintext&#39;)
        md = ex[&#39;assistant&#39;]
        if ex[&#39;footnote&#39;] is not None:
            md += &#39;\n\n---\n&#39; + ex[&#39;footnote&#39;]
        if ((not self.generating) | (i!=last)):
            help_info = f&#39;Number of tokens: {self.backend.llm.get_num_tokens(ex[&#34;assistant&#34;])}&#39;
            if i==last:
                help_info += self.generation_time_info
            st.markdown(md, help=help_info)
        else:
            # st.button(label=&#39;:black_square_for_stop:&#39;, help=&#39;Stop response generation&#39;)
            with st.spinner(&#39;Thinking....&#39;):
                from time import perf_counter
                start = perf_counter()
                tool = self.get_tool(user_input=ex[&#39;user&#39;])
                if tool is None:
                    placeholder = st.empty()
                    self.output = &#39;&#39;
                    for i in self.get_generation_iterator():
                        self.output += i
                        placeholder.markdown(self.output.strip(&#39; \r\n\t&#39;))
                    self.history_dict[-1][&#39;assistant&#39;] = self.output.strip(&#39; \r\n\t&#39;)
                    self.save_interaction(user=self.history_dict[-1][&#39;user&#39;], assistant=self.history_dict[-1][&#39;assistant&#39;])
                    end = perf_counter() - start
                    self.set_time_info(end)
                    print(f&#39;Number of output tokens: {self.backend.llm.get_num_tokens(self.output)}&#39;)
                else:
                    tool_name = tool.pretty_name
                    tool_details = None
                    footnote = None
                    toolholder = st.empty()
                    placeholder = st.empty()
                    md_text = &#39;&#39;
                    for chunk in self.run_tool(tool=tool, user_input=ex[&#39;user&#39;]):
                        if isinstance(chunk, tuple):
                            tool_details = chunk
                            with toolholder.status(label=f&#34;:hammer_and_pick: Running __{tool_name}__...&#34;, state=&#39;running&#39;):
                                st.text(tool_details[1])
                        elif isinstance(chunk, str):
                            footnote = chunk
                            md_text += &#39;\n\n---\n&#39; + chunk
                            placeholder.markdown(md_text)
                        else:
                            with toolholder.status(label=f&#34;:hammer_and_pick: __{tool_name}__&#34;, state=&#39;complete&#39;):
                                st.text(tool_details[1])
                            output = &#39;&#39;
                            for i in chunk:
                                output += i
                                md_text = output.strip(&#39; \n\r\t&#39;)
                                placeholder.markdown(md_text)
                            end = perf_counter() - start
                            self.set_time_info(end)
                            print(f&#39;Number of output tokens: {self.backend.llm.get_num_tokens(output)}&#39;)
                    self.save_interaction(user=ex[&#39;user&#39;], assistant=output.strip(&#39; \r\n\t&#39;), tool_details=tool_details, footnote=footnote, tool_name=tool_name)
                    

                            
                
            
    def experimental_buttons(self) -&gt; None:
        cols = st.columns([1, 1, 1])
        with cols[0]:
            st.button(&#39;:arrows_counterclockwise:&#39;, use_container_width=True, help=&#39;Re-generate response&#39;, disabled=self.generating, 
                      on_click=self.retry_response, kwargs=dict(cont=False, ai_start=self.ai_start))
        with cols[1]:
            st.button(&#39;:fast_forward:&#39;, use_container_width=True, help=&#39;Continue generating response&#39;, disabled=self.generating,
                      on_click=self.retry_response, kwargs=dict(cont=True, ai_start=self.ai_start))
        with cols[2]:
            st.button(&#39;:wastebasket:&#39;, use_container_width=True, help=&#39;Remove the latest question and response&#39;, disabled=self.generating, on_click=self.remove_last)

    def ai_start_textbox(self) -&gt; None:
        if ((self.experimental) &amp; (not self.generating)):
            with st.container(border=False):
                self.ai_start = st.text_area(label=&#39;AI start&#39;, placeholder=&#39;Start of the chatbot response here...&#39;, value=self.ai_start_text, height=1, label_visibility=&#39;collapsed&#39;)
                if self.mobile:
                    with st.expander(&#39;:gear: Extra options&#39;):
                        self.experimental_buttons()
                else:
                    self.experimental_buttons()
            
        else:
            self.ai_start = &#39;&#39;

    def user_input_box(self) -&gt; None:
        self.user_input = st.chat_input(placeholder=&#39;Your message...&#39;, disabled=self.generating)
        if self.user_input:
            if self.user_input.strip(&#39; \r\n\t&#39;) != &#39;&#39;:
                input_dict = self.input_template(user=self.user_input, ai_start=self.ai_start)
                self.history_dict.append(input_dict)
                self.create_generation_config(&#39;new&#39;, self.user_input)
                self.toggle_generating()
                st.rerun()
            
    def launch(self) -&gt; None:
        if self.islogin:
            if not self.mobile:
                st.set_page_config(layout=&#39;wide&#39;)
            else:
                st.set_page_config(layout=&#39;centered&#39;)
            with st.sidebar:
                self.sidebar()
            self.chatbot()
        else:
            self.login()</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.ai_start_text"><code class="name">var <span class="ident">ai_start_text</span> : str</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def ai_start_text(self) -&gt; str:
    if not hasattr(st.session_state, &#39;ai_start_text&#39;):
        st.session_state.ai_start_text = &#39;&#39;
    return st.session_state.ai_start_text</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.backend"><code class="name">var <span class="ident">backend</span> : <a title="llmflex.Frontend.streamlit_interface.InterfaceState" href="#llmflex.Frontend.streamlit_interface.InterfaceState">InterfaceState</a></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def backend(self) -&gt; InterfaceState:
    return st.session_state.backend</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.conversation_delete"><code class="name">var <span class="ident">conversation_delete</span> : bool</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def conversation_delete(self) -&gt; bool:
    if not hasattr(st.session_state, &#39;conversation_delete&#39;):
        st.session_state.conversation_delete = False
    return st.session_state.conversation_delete</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.experimental"><code class="name">var <span class="ident">experimental</span> : bool</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def experimental(self) -&gt; bool:
    if not hasattr(st.session_state, &#39;allow_ai_start&#39;):
        st.session_state.allow_ai_start = False
    return st.session_state.allow_ai_start</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.generating"><code class="name">var <span class="ident">generating</span> : bool</code></dt>
<dd>
<div class="desc"><p>Whether chatbot is generating.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def generating(self) -&gt; bool:
    &#34;&#34;&#34;Whether chatbot is generating.&#34;&#34;&#34;
    if not hasattr(st.session_state, &#39;generating&#39;):
        st.session_state.generating = False
    return st.session_state.generating</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.generation_config"><code class="name">var <span class="ident">generation_config</span> : Dict[str, str]</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def generation_config(self) -&gt; Dict[str, str]:
    if not hasattr(st.session_state, &#39;generation_config&#39;):
        st.session_state.generation_config = dict(gen_type=&#39;none&#39;)
    return st.session_state.generation_config</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.generation_time_info"><code class="name">var <span class="ident">generation_time_info</span> : str</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def generation_time_info(self) -&gt; str:
    template = &#39;  \nGeneration time taken: &#39;
    if not hasattr(st.session_state, &#39;generation_time_info&#39;):
        st.session_state.generation_time_info = &#39;--&#39;
    if type(st.session_state.generation_time_info) == str:
        return template + st.session_state.generation_time_info
    else:
        return template + f&#39;{round(st.session_state.generation_time_info, 2)}&#39; + &#39;s&#39;</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.history_dict"><code class="name">var <span class="ident">history_dict</span> : List[Dict[str, Any]]</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def history_dict(self) -&gt; List[Dict[str, Any]]:
    if not hasattr(st.session_state, &#39;history&#39;):
        self.refresh_history()
    elif st.session_state.history[&#39;current_title&#39;] != self.backend.current_title:
        self.refresh_history()
    return st.session_state.history[&#39;history_dict&#39;]</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.islogin"><code class="name">var <span class="ident">islogin</span> : bool</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def islogin(self) -&gt; bool:
    if not hasattr(st.session_state, &#39;islogin&#39;):
        st.session_state.islogin = False
    return st.session_state.islogin</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.login_wrong"><code class="name">var <span class="ident">login_wrong</span> : bool</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def login_wrong(self) -&gt; bool:
    if not hasattr(st.session_state, &#39;login_wrong&#39;):
        st.session_state.login_wrong = False
    return st.session_state.login_wrong</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.mobile"><code class="name">var <span class="ident">mobile</span> : bool</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def mobile(self) -&gt; bool:
    if not hasattr(st.session_state, &#39;mobile&#39;):
        st.session_state.mobile = False
    return st.session_state.mobile</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.tool_states"><code class="name">var <span class="ident">tool_states</span> : Dict[str, bool]</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def tool_states(self) -&gt; Dict[str, bool]:
    if not hasattr(st.session_state, &#39;tool_states&#39;):
        st.session_state.tool_states = dict()
        for tool in self.backend.tool_selector.tools:
            st.session_state.tool_states[tool.name] = True
    return st.session_state.tool_states</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.add_chat"><code class="name flex">
<span>def <span class="ident">add_chat</span></span>(<span>self, title: str) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_chat(self, title: str) -&gt; None:
    if not self.generating:
        title = title.strip(&#39; \r\n\t&#39;)
        if title == &#39;&#39;:
            pass
        else:
            from ..Memory.assistant_long_term_memory import AssistantLongTermChatMemory
            from ..Prompts.prompt_template import DEFAULT_SYSTEM_MESSAGE
            self.backend.memory = AssistantLongTermChatMemory(title, embeddings=self.backend.embeddings, text_splitter=self.backend.text_splitter, from_exist=True)
            self.backend.system = self.backend.memory.vectordb._info.get(&#39;system&#39;, DEFAULT_SYSTEM_MESSAGE)
            self.backend.memory.vectordb._info[&#39;system&#39;] = self.backend.system
            self.backend.memory.save()
            st.rerun()</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.ai_start_textbox"><code class="name flex">
<span>def <span class="ident">ai_start_textbox</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ai_start_textbox(self) -&gt; None:
    if ((self.experimental) &amp; (not self.generating)):
        with st.container(border=False):
            self.ai_start = st.text_area(label=&#39;AI start&#39;, placeholder=&#39;Start of the chatbot response here...&#39;, value=self.ai_start_text, height=1, label_visibility=&#39;collapsed&#39;)
            if self.mobile:
                with st.expander(&#39;:gear: Extra options&#39;):
                    self.experimental_buttons()
            else:
                self.experimental_buttons()
        
    else:
        self.ai_start = &#39;&#39;</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.assistant_response"><code class="name flex">
<span>def <span class="ident">assistant_response</span></span>(<span>self, ex: Dict[str, Any], i: int, last: int) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def assistant_response(self, ex: Dict[str, Any], i: int, last: int) -&gt; None:
    if ex[&#39;tool_details&#39;] is not None:
        with st.status(label=f&#34;:hammer_and_pick: __{ex[&#39;tool_name&#39;]}__&#34;, state=&#39;complete&#39;):
            st.code(ex[&#39;tool_details&#39;][1], language=&#39;plaintext&#39;)
    md = ex[&#39;assistant&#39;]
    if ex[&#39;footnote&#39;] is not None:
        md += &#39;\n\n---\n&#39; + ex[&#39;footnote&#39;]
    if ((not self.generating) | (i!=last)):
        help_info = f&#39;Number of tokens: {self.backend.llm.get_num_tokens(ex[&#34;assistant&#34;])}&#39;
        if i==last:
            help_info += self.generation_time_info
        st.markdown(md, help=help_info)
    else:
        # st.button(label=&#39;:black_square_for_stop:&#39;, help=&#39;Stop response generation&#39;)
        with st.spinner(&#39;Thinking....&#39;):
            from time import perf_counter
            start = perf_counter()
            tool = self.get_tool(user_input=ex[&#39;user&#39;])
            if tool is None:
                placeholder = st.empty()
                self.output = &#39;&#39;
                for i in self.get_generation_iterator():
                    self.output += i
                    placeholder.markdown(self.output.strip(&#39; \r\n\t&#39;))
                self.history_dict[-1][&#39;assistant&#39;] = self.output.strip(&#39; \r\n\t&#39;)
                self.save_interaction(user=self.history_dict[-1][&#39;user&#39;], assistant=self.history_dict[-1][&#39;assistant&#39;])
                end = perf_counter() - start
                self.set_time_info(end)
                print(f&#39;Number of output tokens: {self.backend.llm.get_num_tokens(self.output)}&#39;)
            else:
                tool_name = tool.pretty_name
                tool_details = None
                footnote = None
                toolholder = st.empty()
                placeholder = st.empty()
                md_text = &#39;&#39;
                for chunk in self.run_tool(tool=tool, user_input=ex[&#39;user&#39;]):
                    if isinstance(chunk, tuple):
                        tool_details = chunk
                        with toolholder.status(label=f&#34;:hammer_and_pick: Running __{tool_name}__...&#34;, state=&#39;running&#39;):
                            st.text(tool_details[1])
                    elif isinstance(chunk, str):
                        footnote = chunk
                        md_text += &#39;\n\n---\n&#39; + chunk
                        placeholder.markdown(md_text)
                    else:
                        with toolholder.status(label=f&#34;:hammer_and_pick: __{tool_name}__&#34;, state=&#39;complete&#39;):
                            st.text(tool_details[1])
                        output = &#39;&#39;
                        for i in chunk:
                            output += i
                            md_text = output.strip(&#39; \n\r\t&#39;)
                            placeholder.markdown(md_text)
                        end = perf_counter() - start
                        self.set_time_info(end)
                        print(f&#39;Number of output tokens: {self.backend.llm.get_num_tokens(output)}&#39;)
                self.save_interaction(user=ex[&#39;user&#39;], assistant=output.strip(&#39; \r\n\t&#39;), tool_details=tool_details, footnote=footnote, tool_name=tool_name)</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.chatbot"><code class="name flex">
<span>def <span class="ident">chatbot</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def chatbot(self) -&gt; None:
    self.conversation_history()
    
    self.ai_start_textbox()
    self.user_input_box()</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.conversation_history"><code class="name flex">
<span>def <span class="ident">conversation_history</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def conversation_history(self) -&gt; None:
    history = self.history_dict
    last = len(history) - 1
    gen_type = self.generation_config[&#39;gen_type&#39;]
    for i, ex in enumerate(history):
        with st.chat_message(name=&#39;user&#39;):
            st.markdown(ex[&#39;user&#39;], help=f&#39;Number of tokens: {self.backend.llm.get_num_tokens(ex[&#34;user&#34;])}&#39;)
        with st.chat_message(name=&#39;assistant&#39;):
            self.assistant_response(ex, i, last)
        if ((gen_type != &#39;none&#39;) &amp; (i == last)):
            st.session_state.generation_config = dict(gen_type=&#39;none&#39;)
            st.session_state.retry_or_continue = False
            self.toggle_generating()
            st.rerun()</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.conversations"><code class="name flex">
<span>def <span class="ident">conversations</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>List of conversations.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def conversations(self) -&gt; None:
    &#34;&#34;&#34;List of conversations.&#34;&#34;&#34;
    for title in self.backend.titles:
        if self.conversation_delete:
            cols = st.columns(self.sidebar_ratio)
            with cols[0]:
                btn_type = &#39;primary&#39; if self.backend.current_title == title else &#39;secondary&#39;
                st.button(label=title.title(), key=f&#39;{title}_select&#39;, disabled=self.generating, 
                        use_container_width=True, type=btn_type, on_click=self.switch_chat, kwargs=dict(title=title))
            with cols[1]:
                st.button(label=&#39;:heavy_minus_sign:&#39;, key=f&#39;{title}_delete&#39;, disabled=self.generating, on_click=self.delete_chat, kwargs=dict(title=title))
        else:
            btn_type = &#39;primary&#39; if self.backend.current_title == title else &#39;secondary&#39;
            st.button(label=title.title(), key=f&#39;{title}_select&#39;, disabled=self.generating, 
                    use_container_width=True, type=btn_type, on_click=self.switch_chat, kwargs=dict(title=title))
    cols = st.columns(self.sidebar_ratio)
    with cols[0]:
        st.toggle(label=&#39;:wastebasket:&#39;, key=f&#39;conv_delete&#39;, disabled=self.generating, on_change=self.toggle_conversation_delete, help=&#39;Select conversations to remove.&#39;)</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.create_generation_config"><code class="name flex">
<span>def <span class="ident">create_generation_config</span></span>(<span>self, gen_type: Literal['new', 'retry', 'continue', 'none'], user_input: str, ai_start: Optional[str] = None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_generation_config(self, gen_type: Literal[&#39;new&#39;, &#39;retry&#39;, &#39;continue&#39;, &#39;none&#39;], user_input: str, ai_start: Optional[str] = None) -&gt; None:
    gen_obj = dict(
        gen_type = gen_type,
        user_input = user_input.strip(&#39; \r\n\t&#39;),
        ai_start = self.ai_start if ai_start is None else ai_start
    )
    st.session_state.generation_config = gen_obj</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.current_prompt_index"><code class="name flex">
<span>def <span class="ident">current_prompt_index</span></span>(<span>self) ‑> int</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def current_prompt_index(self) -&gt; int:
    from ..Prompts.prompt_template import presets
    template = self.backend.model.prompt_template.template_name
    for i, t in enumerate(self.backend.presets):
        if template == t:
            return i
    return 0</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.delete_chat"><code class="name flex">
<span>def <span class="ident">delete_chat</span></span>(<span>self, title: str) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_chat(self, title: str) -&gt; None:
    if not self.generating:
        switch = title == self.backend.current_title
        from shutil import rmtree
        if switch:
            rmtree(self.backend.memory.chat_dir)
            self.switch_chat(&#39;Untitled 0&#39;)
        else:
            from ..Memory.assistant_long_term_memory import AssistantLongTermChatMemory
            mem = AssistantLongTermChatMemory(title, embeddings=self.backend.embeddings, text_splitter=self.backend.text_splitter, from_exist=False)
            rmtree(mem.chat_dir)</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.experimental_buttons"><code class="name flex">
<span>def <span class="ident">experimental_buttons</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def experimental_buttons(self) -&gt; None:
    cols = st.columns([1, 1, 1])
    with cols[0]:
        st.button(&#39;:arrows_counterclockwise:&#39;, use_container_width=True, help=&#39;Re-generate response&#39;, disabled=self.generating, 
                  on_click=self.retry_response, kwargs=dict(cont=False, ai_start=self.ai_start))
    with cols[1]:
        st.button(&#39;:fast_forward:&#39;, use_container_width=True, help=&#39;Continue generating response&#39;, disabled=self.generating,
                  on_click=self.retry_response, kwargs=dict(cont=True, ai_start=self.ai_start))
    with cols[2]:
        st.button(&#39;:wastebasket:&#39;, use_container_width=True, help=&#39;Remove the latest question and response&#39;, disabled=self.generating, on_click=self.remove_last)</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.get_generation_iterator"><code class="name flex">
<span>def <span class="ident">get_generation_iterator</span></span>(<span>self) ‑> Iterator[str]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_generation_iterator(self) -&gt; Iterator[str]:
    from ..Memory.assistant_long_term_memory import create_long_assistant_memory_prompt
    config = self.generation_config
    prompt = create_long_assistant_memory_prompt(
        user=config[&#39;user_input&#39;],
        prompt_template=self.backend.template,
        llm=self.backend.llm,
        memory=self.backend.memory,
        system=self.backend.system,
        short_token_limit=self.backend.short_limit,
        long_token_limit=self.backend.long_limit,
        score_threshold=self.backend.score_threshold
    ) + config[&#39;ai_start&#39;]
    print(f&#39;Number of input tokens: {self.backend.llm.get_num_tokens(prompt)}&#39;)
    def generator():
        yield config[&#39;ai_start&#39;]
        for i in self.backend.llm.stream(prompt):
            yield i
    return generator()</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.get_history"><code class="name flex">
<span>def <span class="ident">get_history</span></span>(<span>self, mem) ‑> List[Dict[str, Any]]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_history(self, mem) -&gt; List[Dict[str, Any]]:
    history = list(map(lambda x: dict(
        user=x[&#39;metadata&#39;][&#39;user&#39;], 
        assistant=x[&#39;metadata&#39;][&#39;assistant&#39;], 
        order=x[&#39;metadata&#39;][&#39;order&#39;],
        tool_details=x[&#39;metadata&#39;].get(&#39;tool_details&#39;, None),
        footnote=x[&#39;metadata&#39;].get(&#39;footnote&#39;, None),
        tool_name=x[&#39;metadata&#39;].get(&#39;tool_name&#39;, None)
    ), mem._data))
    if len(history) == 0:
        return []
    count = max(list(map(lambda x: x[&#39;order&#39;], history))) + 1
    history = list(map(lambda x: list(filter(lambda y: y[&#39;order&#39;] == x, history))[0], range(count)))
    history.sort(key=lambda x: x[&#39;order&#39;], reverse=False)
    return history</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.get_tool"><code class="name flex">
<span>def <span class="ident">get_tool</span></span>(<span>self, user_input: str) ‑> Optional[Type[<a title="llmflex.Tools.base_tool.BaseTool" href="../Tools/base_tool.html#llmflex.Tools.base_tool.BaseTool">BaseTool</a>]]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_tool(self, user_input: str) -&gt; Optional[Type[BaseTool]]:
    if self.backend.tool_selector is None:
        return None
    if sum(list(self.tool_states.values())) == 0:
        return None
    self.backend.tool_selector.set_score_threshold(self.tool_threshold)
    history = self.backend.memory.get_token_memory(llm=self.backend.llm, token_limit=self.backend.short_limit)
    system = self.backend.system
    tool = self.backend.tool_selector.get_tool(user_input=user_input, history=history, system=system)
    print(f&#39;Tool: {self.backend.tool_selector.last_tool}\nScore: {self.backend.tool_selector.last_score}&#39;)
    if tool is not None:
        if not self.tool_states[tool.name]:
            print(tool.name + &#39; disabled.&#39;)
            tool = None
    return tool</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.input_template"><code class="name flex">
<span>def <span class="ident">input_template</span></span>(<span>self, user: Optional[str], ai_start: str) ‑> Optional[Dict[str, Any]]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def input_template(self, user: Optional[str], ai_start: str) -&gt; Optional[Dict[str, Any]]:
    if user is None:
        return None
    user = user.strip(&#39; \n\r\t&#39;)
    if user == &#39;&#39;:
        return None
    input_dict = dict(
        user = user.strip(&#39; \n\r\t&#39;),
        assistant = ai_start,
        order = self.backend.memory.interaction_count,
        tool_details = None,
        footnote = None,
        tool_name = None
    )
    return input_dict</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.launch"><code class="name flex">
<span>def <span class="ident">launch</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def launch(self) -&gt; None:
    if self.islogin:
        if not self.mobile:
            st.set_page_config(layout=&#39;wide&#39;)
        else:
            st.set_page_config(layout=&#39;centered&#39;)
        with st.sidebar:
            self.sidebar()
        self.chatbot()
    else:
        self.login()</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.llm_settings"><code class="name flex">
<span>def <span class="ident">llm_settings</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>LLM generation settings.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def llm_settings(self) -&gt; None:
    &#34;&#34;&#34;LLM generation settings.&#34;&#34;&#34;
    self.temperature_slidder = st.slider(&#39;Temparature&#39;, min_value=0.0, max_value=2.0, step=0.01, value=self.backend.llm.generation_config[&#39;temperature&#39;], disabled=self.generating)
    self.max_new_token_slidder = st.slider(&#39;Maximum number of new tokens&#39;, min_value=0, max_value=4096, step=1, value=self.backend.llm.generation_config[&#39;max_new_tokens&#39;], disabled=self.generating)
    self.repetition_slidder = st.slider(&#39;Repetition penalty&#39;, min_value=1.0, max_value=2.0, step=0.01, value=self.backend.llm.generation_config[&#39;repetition_penalty&#39;], disabled=self.generating)
    self.topp_slidder = st.slider(&#39;Top P&#39;, min_value=0.0, max_value=1.0, step=0.01, value=self.backend.llm.generation_config[&#39;top_p&#39;], disabled=self.generating)
    self.topk_slidder = st.slider(&#39;Top K&#39;, min_value=0, max_value=30000, step=1, value=self.backend.llm.generation_config[&#39;top_k&#39;], disabled=self.generating)
    summary = [
        &#39;Current settings:&#39;,
        f&#34;Temperature: {self.backend.llm.generation_config[&#39;temperature&#39;]}&#34;,
        f&#34;Max new tokens: {self.backend.llm.generation_config[&#39;max_new_tokens&#39;]}&#34;,
        f&#34;Repetition penalty: {self.backend.llm.generation_config[&#39;repetition_penalty&#39;]}&#34;,
        f&#34;Top P: {self.backend.llm.generation_config[&#39;top_p&#39;]}&#34;,
        f&#34;Top K: {self.backend.llm.generation_config[&#39;top_k&#39;]}&#34;,
    ]
    st.markdown(&#39;  \n&#39;.join(summary))
    st.button(label=&#39;:floppy_disk:&#39;, 
              key=&#39;llm_config_save&#39;, 
              disabled=self.generating, 
              use_container_width=True,
              on_click=self.set_llm_config,
              kwargs=dict(temperature=self.temperature_slidder,
                          max_new_tokens=self.max_new_token_slidder,
                          repetition_penalty=self.repetition_slidder,
                          top_p=self.topp_slidder,
                          top_k=self.topk_slidder))</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.login"><code class="name flex">
<span>def <span class="ident">login</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def login(self) -&gt; None:
    login_form = st.form(key=&#39;login&#39;)
    with login_form:
        user = st.text_input(label=&#39;Username:&#39;, placeholder=&#39;Your username...&#39;)
        password = st.text_input(label=&#39;Password&#39;, placeholder=&#39;Your password...&#39;, type=&#39;password&#39;)
        if st.form_submit_button(label=&#39;Login&#39;):
            print(&#39;Triggered by btn.&#39;)
            self.login_with_cred(user=user, password=password)
        if self.login_wrong:
            st.warning(&#39;Incorrect credentials. Please try again.&#39;)</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.login_with_cred"><code class="name flex">
<span>def <span class="ident">login_with_cred</span></span>(<span>self, user: str, password: str) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def login_with_cred(self, user: str, password: str) -&gt; None:
    if ((user == self._auth[0]) &amp; (password == self._auth[1])):
        print(user, password)
        st.session_state.islogin = True
    else:
        st.session_state.login_wrong = True
    st.rerun()</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.memory_settings"><code class="name flex">
<span>def <span class="ident">memory_settings</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Memory token limit settings.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def memory_settings(self) -&gt; None:
    &#34;&#34;&#34;Memory token limit settings.&#34;&#34;&#34;
    self.short_limit_slidder = st.slider(&#39;Short term memory token limit&#39;, min_value=0, max_value=6000, step=1, value=self.backend.short_limit, disabled=self.generating)
    self.long_limit_slidder = st.slider(&#39;Long term memory token limit&#39;, min_value=0, max_value=6000, step=1, value=self.backend.long_limit, disabled=self.generating)
    self.score_threshold_slidder = st.slider(&#39;Score threshold for long term memory&#39;, min_value=0.0, max_value=1.0, step=0.01, value=self.backend.score_threshold, disabled=self.generating)
    summary = [
        &#39;Current settings:&#39;,
        f&#39;Short term memory token limit: {self.backend.short_limit}&#39;,
        f&#39;Long term memory token limit: {self.backend.long_limit}&#39;,
        f&#39;Score threshold: {self.backend.score_threshold}&#39;

    ]
    st.markdown(&#39;  \n&#39;.join(summary))
    st.button(label=&#39;:floppy_disk:&#39;, key=&#39;memory_token_save&#39;, disabled=self.generating, 
              use_container_width=True,
              on_click=self.set_memory_settings,
              kwargs=dict(short=self.short_limit_slidder,
                          long=self.long_limit_slidder,
                          score=self.score_threshold_slidder))</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.new_chat_form"><code class="name flex">
<span>def <span class="ident">new_chat_form</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def new_chat_form(self) -&gt; None:
    with st.form(key=&#39;new_chat_form&#39;, border=False, clear_on_submit=True):
        cols = st.columns(self.sidebar_ratio)
        with cols[0]:
            self.new_title = st.text_input(label=&#39;new_title&#39;, max_chars=40, placeholder=&#39;New conversation title here...&#39;, label_visibility=&#39;collapsed&#39;, disabled=self.generating)
        with cols[1]:
            if st.form_submit_button(label=&#39;:heavy_plus_sign:&#39;, disabled=self.generating):
                self.add_chat(title=self.new_title)</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.prompt_template_settings"><code class="name flex">
<span>def <span class="ident">prompt_template_settings</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Prompt template settings.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prompt_template_settings(self) -&gt; None:
    &#34;&#34;&#34;Prompt template settings.&#34;&#34;&#34;
    from ..Prompts.prompt_template import presets
    format = st.selectbox(label=&#39;prompt_formats&#39;, 
                 label_visibility=&#39;collapsed&#39;,
                 key=&#39;prompt_format&#39;,
                 options=list(presets.keys()), 
                 disabled=self.generating, 
                 index=self.current_prompt_index(),
                 on_change=self.set_prompt_template)</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.refresh_history"><code class="name flex">
<span>def <span class="ident">refresh_history</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def refresh_history(self) -&gt; None:
    st.session_state.history = dict(
        current_title=self.backend.current_title,
        history_dict=self.get_history(self.backend.memory)
    )</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.remove_last"><code class="name flex">
<span>def <span class="ident">remove_last</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_last(self) -&gt; None:
    if not self.generating:
        self.backend.memory.remove_last_interaction()
        self.refresh_history()</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.retry_response"><code class="name flex">
<span>def <span class="ident">retry_response</span></span>(<span>self, cont: bool = False, ai_start: str = '') ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def retry_response(self, cont: bool = False, ai_start: str = &#39;&#39;) -&gt; None:
    if not self.generating:
        if len(self.history_dict) == 0:
            return None
        print(ai_start)
        st.session_state.ai_start_text = ai_start
        history = self.get_history(self.backend.memory)
        user = history[-1][&#39;user&#39;]
        self.create_generation_config(&#39;continue&#39; if cont else &#39;retry&#39;, user, history[-1][&#39;assistant&#39;] if cont else ai_start)
        input_dict = self.input_template(user=user, ai_start=history[-1][&#39;assistant&#39;] if cont else ai_start)
        self.backend.memory.remove_last_interaction()
        self.refresh_history()
        self.history_dict.append(input_dict)
        self.toggle_generating()</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.run_tool"><code class="name flex">
<span>def <span class="ident">run_tool</span></span>(<span>self, tool: <a title="llmflex.Tools.base_tool.BaseTool" href="../Tools/base_tool.html#llmflex.Tools.base_tool.BaseTool">BaseTool</a>, user_input: str) ‑> Iterator[Union[str, Tuple[str, str], Iterator[str]]]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_tool(self, tool: BaseTool, user_input: str) -&gt; Iterator[Union[str, Tuple[str, str], Iterator[str]]]:
    recent_history = self.backend.memory.get_token_memory(llm=self.backend.llm, token_limit=self.backend.short_limit)
    return tool.run_with_chat(tool_input=user_input, llm=self.backend.llm, stream=True, history=recent_history)</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.save_interaction"><code class="name flex">
<span>def <span class="ident">save_interaction</span></span>(<span>self, user: str, assistant: str, **kwargs) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_interaction(self, user: str, assistant: str, **kwargs) -&gt; None:
    self.backend.memory.save_interaction(user_input=user, assistant_output=assistant, **kwargs)
    self.refresh_history()</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.set_exeperimental"><code class="name flex">
<span>def <span class="ident">set_exeperimental</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_exeperimental(self) -&gt; None:
    if not self.generating:
        st.session_state.allow_ai_start = st.session_state.use_ai_start</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.set_llm_config"><code class="name flex">
<span>def <span class="ident">set_llm_config</span></span>(<span>self, temperature: float, max_new_tokens: int, repetition_penalty: float, top_p: float, top_k: int) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_llm_config(self, temperature: float, max_new_tokens: int, repetition_penalty: float, top_p: float, top_k: int) -&gt; None:
    if not self.generating:
        self.backend.llm = self.backend.model(temperature=temperature, max_new_tokens=max_new_tokens, repetition_penalty=repetition_penalty, top_p=top_p, top_k=top_k)</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.set_memory_settings"><code class="name flex">
<span>def <span class="ident">set_memory_settings</span></span>(<span>self, long: int, short: int, score: float) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_memory_settings(self, long: int, short: int, score: float) -&gt; None:
    if not self.generating:
        self.backend.short_limit = short
        self.backend.long_limit = long
        self.backend.score_threshold = score</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.set_mobile"><code class="name flex">
<span>def <span class="ident">set_mobile</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_mobile(self) -&gt; None:
    if not self.generating:
        st.session_state.mobile = st.session_state.mobile_toggle</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.set_prompt_template"><code class="name flex">
<span>def <span class="ident">set_prompt_template</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_prompt_template(self) -&gt; None:
    if not self.generating:
        from ..Prompts.prompt_template import PromptTemplate
        preset = st.session_state.prompt_format
        self.backend.template = PromptTemplate.from_preset(preset)</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.set_system_message"><code class="name flex">
<span>def <span class="ident">set_system_message</span></span>(<span>self, system: str) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_system_message(self, system: str) -&gt; None:
    if not self.generating:
        self.backend.system = system.strip(&#39; \n\r\t&#39;)
        self.backend.memory.vectordb._info[&#39;system&#39;] = self.backend.system
        self.backend.memory.save()</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.set_time_info"><code class="name flex">
<span>def <span class="ident">set_time_info</span></span>(<span>self, time: Union[str, float] = '--') ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_time_info(self, time: Union[str, float] = &#39;--&#39;) -&gt; None:
    st.session_state.generation_time_info = time     </code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.settings"><code class="name flex">
<span>def <span class="ident">settings</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Settings of the webapp.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def settings(self) -&gt; None:
    &#34;&#34;&#34;Settings of the webapp.&#34;&#34;&#34;
    st.subheader(&#39;:gear: Settings&#39;)
    st.toggle(label=&#39;Experimental&#39;, 
                value=self.experimental,
                disabled=self.generating,
                on_change=self.set_exeperimental,
                key=&#39;use_ai_start&#39;, 
                help=&#39;More features such as retrying, adding response starting message etc.&#39;)
    st.toggle(label=&#39;Mobile&#39;, 
                value=self.mobile,
                disabled=self.generating,
                on_change=self.set_mobile,
                key=&#39;mobile_toggle&#39;, 
                help=&#39;Better layout for mobile device.&#39;)
    with st.expander(label=&#39;Prompt format settings&#39;):
        self.prompt_template_settings()
    with st.expander(label=&#39;System message settings&#39;):
        self.system_prompt_settings()
    with st.expander(label=&#39;Memory settings&#39;):
        self.memory_settings()
    with st.expander(label=&#39;Model settings&#39;):
        self.llm_settings()
    self.tool_settings()</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.sidebar"><code class="name flex">
<span>def <span class="ident">sidebar</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Sidebar of the webapp.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sidebar(self) -&gt; None:
    &#34;&#34;&#34;Sidebar of the webapp.
    &#34;&#34;&#34;
    app_summary = [&#39;Powered by:&#39;, 
                   f&#39;* LLM: {self.backend.model.model_id}&#39;, 
                   f&#39;* Embedding model: {self.backend.embeddings.name}&#39;, 
                   &#39;&#39;, 
                   &#39;Current conversation:&#39;, 
                   f&#39;* {self.backend.current_title}&#39;,
                   &#39;&#39;,
                   &#39;Current prompt format:&#39;,
                   f&#39;* {self.backend.template.template_name}&#39;]
    st.header(PACKAGE_DISPLAY_NAME.upper(), help=&#39;  \n&#39;.join(app_summary), divider=&#34;grey&#34;)
    st.subheader(&#39;:left_speech_bubble: Conversations&#39;)
    self.new_chat_form()
    with st.expander(label=&#39;Previous conversations&#39;):
        self.conversations()
    self.settings()
    if self.debug:
        st.subheader(&#39;Debug&#39;)
        self.test_buttons()</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.switch_chat"><code class="name flex">
<span>def <span class="ident">switch_chat</span></span>(<span>self, title: str) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def switch_chat(self, title: str) -&gt; None:
    if not self.generating:
        from ..Memory.assistant_long_term_memory import AssistantLongTermChatMemory
        from ..Prompts.prompt_template import DEFAULT_SYSTEM_MESSAGE
        st.session_state.generating = True
        self.set_time_info()
        if title == self.backend.current_title:
            pass
        else:
            print(f&#39;Switch to: {title}&#39;)
            from_exist = title != &#39;Untitled 0&#39;
            self.backend.memory = AssistantLongTermChatMemory(title=title, embeddings=self.backend.embeddings, text_splitter=self.backend.text_splitter, from_exist=from_exist)
            self.backend.system = self.backend.memory.info.get(&#39;system&#39;, DEFAULT_SYSTEM_MESSAGE)
            self.backend.memory.save()
        st.session_state.generating = False</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.system_prompt_settings"><code class="name flex">
<span>def <span class="ident">system_prompt_settings</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>System prompt settings.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def system_prompt_settings(self) -&gt; None:
    &#34;&#34;&#34;System prompt settings.&#34;&#34;&#34;
    self.system_text = st.text_area(label=&#39;System message&#39;, height=250, key=&#39;system_msg&#39;,label_visibility=&#39;collapsed&#39;, value=self.backend.system, disabled=self.generating)
    st.markdown(f&#39;System message token count: {self.backend.llm.get_num_tokens(self.backend.system)}&#39;)
    st.button(label=&#39;:floppy_disk:&#39;, key=&#39;system_save&#39;, disabled=self.generating, use_container_width=True, on_click=self.set_system_message, kwargs=dict(system=self.system_text))</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.test_buttons"><code class="name flex">
<span>def <span class="ident">test_buttons</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_buttons(self) -&gt; None:
    cols = st.columns([1, 1])
    with cols[0]:
        st.button(&#39;Test print&#39;, key=&#39;test&#39;, on_click=lambda: print(self.ai_start))

    with cols[1]:
        st.button(&#39;gen_toggle&#39;, key=&#39;toggle_gen&#39;, on_click=lambda: self.toggle_generating())</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.toggle_conversation_delete"><code class="name flex">
<span>def <span class="ident">toggle_conversation_delete</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def toggle_conversation_delete(self) -&gt; None:
    st.session_state.conversation_delete = not self.conversation_delete</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.toggle_exeperimental"><code class="name flex">
<span>def <span class="ident">toggle_exeperimental</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def toggle_exeperimental(self) -&gt; None:
    if not self.generating:
        st.session_state.allow_ai_start = not st.session_state.allow_ai_start</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.toggle_generating"><code class="name flex">
<span>def <span class="ident">toggle_generating</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def toggle_generating(self) -&gt; None:
    st.session_state.generating = not self.generating
    # print(f&#39;Generating = {self.generating}&#39;)</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.toggle_mobile"><code class="name flex">
<span>def <span class="ident">toggle_mobile</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def toggle_mobile(self) -&gt; None:
    st.session_state.mobile = not self.mobile</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.toggle_tool"><code class="name flex">
<span>def <span class="ident">toggle_tool</span></span>(<span>self, tool_name: str) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def toggle_tool(self, tool_name: str) -&gt; None:
    if not self.generating:
        self.tool_states[tool_name] = not self.tool_states[tool_name]</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.tool_settings"><code class="name flex">
<span>def <span class="ident">tool_settings</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tool_settings(self) -&gt; None:
    if self.backend.tool_selector is not None:
        with st.expander(&#39;Tools settings&#39;):
            for i in self.backend.tool_selector.tools:
                st.toggle(label=f&#39;{i.pretty_name}&#39;, value=self.tool_states[i.name], on_change=lambda: self.toggle_tool(i.name),
                          disabled=self.generating)
            self.tool_threshold = st.slider(label=&#39;Tool trigger threshold&#39;, min_value=0.0, max_value=1.0, step=0.005, 
                                            value=self.backend.tool_selector.score_threshold, disabled=self.generating)</code></pre>
</details>
</dd>
<dt id="llmflex.Frontend.streamlit_interface.StreamlitInterface.user_input_box"><code class="name flex">
<span>def <span class="ident">user_input_box</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def user_input_box(self) -&gt; None:
    self.user_input = st.chat_input(placeholder=&#39;Your message...&#39;, disabled=self.generating)
    if self.user_input:
        if self.user_input.strip(&#39; \r\n\t&#39;) != &#39;&#39;:
            input_dict = self.input_template(user=self.user_input, ai_start=self.ai_start)
            self.history_dict.append(input_dict)
            self.create_generation_config(&#39;new&#39;, self.user_input)
            self.toggle_generating()
            st.rerun()</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="llmflex.Frontend" href="index.html">llmflex.Frontend</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="llmflex.Frontend.streamlit_interface.create_streamlit_script" href="#llmflex.Frontend.streamlit_interface.create_streamlit_script">create_streamlit_script</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.embeddings_loader" href="#llmflex.Frontend.streamlit_interface.embeddings_loader">embeddings_loader</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.run_streamlit_interface" href="#llmflex.Frontend.streamlit_interface.run_streamlit_interface">run_streamlit_interface</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.tool_loader" href="#llmflex.Frontend.streamlit_interface.tool_loader">tool_loader</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="llmflex.Frontend.streamlit_interface.InterfaceState" href="#llmflex.Frontend.streamlit_interface.InterfaceState">InterfaceState</a></code></h4>
<ul class="">
<li><code><a title="llmflex.Frontend.streamlit_interface.InterfaceState.current_title" href="#llmflex.Frontend.streamlit_interface.InterfaceState.current_title">current_title</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.InterfaceState.history" href="#llmflex.Frontend.streamlit_interface.InterfaceState.history">history</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.InterfaceState.presets" href="#llmflex.Frontend.streamlit_interface.InterfaceState.presets">presets</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.InterfaceState.titles" href="#llmflex.Frontend.streamlit_interface.InterfaceState.titles">titles</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface">StreamlitInterface</a></code></h4>
<ul class="">
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.add_chat" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.add_chat">add_chat</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.ai_start_text" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.ai_start_text">ai_start_text</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.ai_start_textbox" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.ai_start_textbox">ai_start_textbox</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.assistant_response" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.assistant_response">assistant_response</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.backend" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.backend">backend</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.chatbot" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.chatbot">chatbot</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.conversation_delete" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.conversation_delete">conversation_delete</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.conversation_history" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.conversation_history">conversation_history</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.conversations" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.conversations">conversations</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.create_generation_config" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.create_generation_config">create_generation_config</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.current_prompt_index" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.current_prompt_index">current_prompt_index</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.delete_chat" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.delete_chat">delete_chat</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.experimental" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.experimental">experimental</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.experimental_buttons" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.experimental_buttons">experimental_buttons</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.generating" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.generating">generating</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.generation_config" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.generation_config">generation_config</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.generation_time_info" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.generation_time_info">generation_time_info</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.get_generation_iterator" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.get_generation_iterator">get_generation_iterator</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.get_history" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.get_history">get_history</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.get_tool" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.get_tool">get_tool</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.history_dict" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.history_dict">history_dict</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.input_template" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.input_template">input_template</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.islogin" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.islogin">islogin</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.launch" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.launch">launch</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.llm_settings" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.llm_settings">llm_settings</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.login" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.login">login</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.login_with_cred" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.login_with_cred">login_with_cred</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.login_wrong" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.login_wrong">login_wrong</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.memory_settings" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.memory_settings">memory_settings</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.mobile" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.mobile">mobile</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.new_chat_form" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.new_chat_form">new_chat_form</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.prompt_template_settings" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.prompt_template_settings">prompt_template_settings</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.refresh_history" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.refresh_history">refresh_history</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.remove_last" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.remove_last">remove_last</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.retry_response" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.retry_response">retry_response</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.run_tool" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.run_tool">run_tool</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.save_interaction" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.save_interaction">save_interaction</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.set_exeperimental" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.set_exeperimental">set_exeperimental</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.set_llm_config" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.set_llm_config">set_llm_config</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.set_memory_settings" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.set_memory_settings">set_memory_settings</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.set_mobile" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.set_mobile">set_mobile</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.set_prompt_template" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.set_prompt_template">set_prompt_template</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.set_system_message" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.set_system_message">set_system_message</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.set_time_info" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.set_time_info">set_time_info</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.settings" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.settings">settings</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.sidebar" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.sidebar">sidebar</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.switch_chat" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.switch_chat">switch_chat</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.system_prompt_settings" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.system_prompt_settings">system_prompt_settings</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.test_buttons" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.test_buttons">test_buttons</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.toggle_conversation_delete" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.toggle_conversation_delete">toggle_conversation_delete</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.toggle_exeperimental" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.toggle_exeperimental">toggle_exeperimental</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.toggle_generating" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.toggle_generating">toggle_generating</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.toggle_mobile" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.toggle_mobile">toggle_mobile</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.toggle_tool" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.toggle_tool">toggle_tool</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.tool_settings" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.tool_settings">tool_settings</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.tool_states" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.tool_states">tool_states</a></code></li>
<li><code><a title="llmflex.Frontend.streamlit_interface.StreamlitInterface.user_input_box" href="#llmflex.Frontend.streamlit_interface.StreamlitInterface.user_input_box">user_input_box</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>