<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>llmflex.VectorDBs.base_vectordb API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>llmflex.VectorDBs.base_vectordb</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from __future__ import annotations
from ..Embeddings.base_embeddings import BaseEmbeddingsToolkit
from ..TextSplitters.base_text_splitter import BaseTextSplitter
from ..Schemas.documents import Document
from abc import abstractmethod, ABC
from typing import List, Dict, Union, Any, Type, Optional, Sequence, Callable, Tuple
import os, numpy as np

def default_vectordb_dir() -&gt; str:
    &#34;&#34;&#34;Default home directory of vector databases.

    Returns:
        str: Default home directory of vector databases.
    &#34;&#34;&#34;
    from ..utils import get_config
    home = os.path.join(get_config()[&#39;package_home&#39;], &#39;vector_databases&#39;)
    if not os.path.exists(home):
        os.makedirs(home)
    return home

def list_vectordbs(vectordb_dir: Optional[str] = None) -&gt; List[str]:
    &#34;&#34;&#34;List all the vector databases in the given directory.

    Args:
        vectordb_dir (Optional[str], optional): Directory where the vector databases live. If None is given, the default_vectordb_dir will be used. Defaults to None.

    Returns:
        List[str]: List all the vector databases in the given directory.
    &#34;&#34;&#34;
    vectordb_dir = vectordb_dir if ((isinstance(vectordb_dir, str)) &amp; (os.path.exists(vectordb_dir))) else default_vectordb_dir()
    dbs = list(filter(lambda x: os.path.isdir(os.path.join(vectordb_dir, x)), os.listdir(vectordb_dir)))
    dbs = list(filter(lambda x: os.path.exists(os.path.join(vectordb_dir, x, &#39;info.json&#39;)), dbs))
    return dbs

def name_checker(name: str) -&gt; str:
    &#34;&#34;&#34;Raise error if the given string has space, newline characters, or tab characters.

    Args:
        name (str): String to check.

    Returns:
        str: Return the given text if it passes all the checkes.
    &#34;&#34;&#34;
    if &#39; &#39; in name:
        raise ValueError(f&#39;Spaces cannot be in the name&#39;)
    if &#39;\n&#39; in name:
        raise ValueError(f&#39;Newline characters cannot be in the name.&#39;)
    if &#39;\r&#39; in name:
        raise ValueError(f&#39;Newline characters cannot be in the name.&#39;)
    if &#39;\t&#39; in name:
        raise ValueError(f&#39;Tab characters cannot be in the name.&#39;)
    return name

class BaseVectorDatabase(ABC):
    &#34;&#34;&#34;Base class for vector databases.
    &#34;&#34;&#34;
    def __init__(self, embeddings: Type[BaseEmbeddingsToolkit], name: Optional[str] = None, vectordb_dir: Optional[str] = None) -&gt; None:
        &#34;&#34;&#34;Initialise a vector database.

        Args:
            embeddings (Type[BaseEmbeddingsToolkit]): Embeddings toolkit to use.
            name (Optional[str], optional): Name of the vector database. Will be used as the directory base name of the vector database in vectordb_dir. If None is given, the vector database will not be saved. Defaults to None.
            vectordb_dir (Optional[str], optional): Directory where the vector databases live. If None is given, the default_vectordb_dir will be used. Defaults to None.
        &#34;&#34;&#34;
        self._embeddings = embeddings
        self._name = name_checker(name) if name is not None else None
        vectordb_dir = default_vectordb_dir() if vectordb_dir is None else vectordb_dir
        self._db_dir = os.path.join(vectordb_dir, self.name) if self.name is not None else None
        if self.db_dir is not None:
            os.makedirs(self.db_dir, exist_ok=True)
        self._index = self._get_empty_index()
        self._data = dict()

    @property
    def embeddings(self) -&gt; BaseEmbeddingsToolkit:
        &#34;&#34;&#34;Embeddings toolkit used in the vector database.

        Returns:
            BaseEmbeddingsToolkit: Embeddings toolkit used in the vector database.
        &#34;&#34;&#34;
        return self._embeddings
    
    @property
    def index(self) -&gt; Any:
        &#34;&#34;&#34;Index of the vector database.

        Returns:
            Any: Index of the vector database.
        &#34;&#34;&#34;
        return self._index
    
    @property
    def name(self) -&gt; Optional[str]:
        &#34;&#34;&#34;Name of the vector database.

        Returns:
            Optional[str]: Name of the vector database.
        &#34;&#34;&#34;
        return self._name
    
    @property
    def info(self) -&gt; Dict[str, Any]:
        &#34;&#34;&#34;Information of the vector database.

        Returns:
            Dict[str, Any]: Information of the vector database.
        &#34;&#34;&#34;
        if not hasattr(self, &#39;_info&#39;):
            from ..utils import current_time, read_json
            if self.db_dir is not None:
                info_dir = os.path.join(self.db_dir, &#39;info.json&#39;)
                if os.path.exists(info_dir):
                    self._info = read_json(info_dir)
                else:
                    self._info = dict(embeddings=self.embeddings.name, last_update=current_time())
            else:
                self._info = dict(embeddings=self.embeddings.name, last_update=current_time())
        return self._info
    
    @property
    def db_dir(self) -&gt; Optional[str]:
        &#34;&#34;&#34;Directory of the vector database.

        Returns:
            Optional[str]: Directory of the vector database.
        &#34;&#34;&#34;
        return self._db_dir
    
    @property
    def data(self) -&gt; Dict[int, Document]:
        &#34;&#34;&#34;Dictionary of all the documents in the vector database.

        Returns:
            Dict[int, Document]: Dictionary of all the documents in the vector database.
        &#34;&#34;&#34;
        return self._data
    
    @property
    def size(self) -&gt; int:
        &#34;&#34;&#34;Number of documents in the vector database.

        Returns:
            int: Number of documents in the vector database.
        &#34;&#34;&#34;
        return len(self.data)
    
    @abstractmethod
    def _get_empty_index(self) -&gt; Any:
        &#34;&#34;&#34;Return an empty index.

        Returns:
            Any: An empty index.
        &#34;&#34;&#34;
        pass

    @property
    @abstractmethod
    def _index_filename(self) -&gt; str:
        &#34;&#34;&#34;Base name of the file for the index in the vector database directory.

        Returns:
            str: Base name of the file for the index in the vector database directory.
        &#34;&#34;&#34;
        pass
    
    @abstractmethod
    def _save_index(self) -&gt; None:
        &#34;&#34;&#34;Save the index of the vector database.
        &#34;&#34;&#34;
        pass

    @abstractmethod
    def _load_index(self, index_dir: str) -&gt; Any:
        &#34;&#34;&#34;Load the index from an existing saved file.
        &#34;&#34;&#34;
        pass

    @abstractmethod
    def _add(self, vectors: np.ndarray[np.float32], docs: List[Document]) -&gt; None:
        &#34;&#34;&#34;Core method to add documents into the vector database.

        Args:
            vectors (np.ndarray[np.float32]): Array of vectors created by the indexes of the documents.
            docs (List[Document]): List of documents to add.
        &#34;&#34;&#34;
        pass

    @abstractmethod
    def _delete(self, ids: List[int]) -&gt; None:
        &#34;&#34;&#34;Core method to remove records by ids.

        Args:
            ids (List[int]): Ids to remove.
        &#34;&#34;&#34;
        pass

    @abstractmethod
    def _batch_search_with_scores(self, vectors: np.ndarray[np.float32], k: int = 5, ids_scope: Optional[List[int]] = None) -&gt; Tuple[np.ndarray[np.float32], np.ndarray[np.int64]]:
        &#34;&#34;&#34;Batch similarity search with multiple vectors.

        Args:
            vectors (np.ndarray[np.float32]): Array of vectors for the search.
            k (int, optional): Maximum results for each vector. Defaults to 5.
            ids_scope (Optional[List[int]], optional): The list of allowed ids to return for the similarity search. Defaults to None.

        Returns:
            Tuple[np.ndarray[np.float32], np.ndarray[np.int64]]: Tuple of scores and ids. Both matrices must be in the same shape.
        &#34;&#34;&#34;
        pass

    @abstractmethod
    def _get_vectors_by_ids(self, ids: List[int]) -&gt; np.ndarray[np.float32]:
        &#34;&#34;&#34;Get the array of vectors by ids.

        Args:
            ids (List[int]): Document ids.

        Returns:
            np.ndarray[np.float32]: Arrray of vectors.
        &#34;&#34;&#34;
        pass

    @classmethod
    def from_exist(cls, embeddings: Type[BaseEmbeddingsToolkit], name: str, vectordb_dir: Optional[str] = None) -&gt; BaseVectorDatabase:
        &#34;&#34;&#34;Load the vector database from an existing vector database.

        Args:
            embeddings (Type[BaseEmbeddingsToolkit]): Embeddings toolkit to use.
            name (str): Name of the existing database.
            vectordbs_dir (Optional[str], optional): Directory where the vector databases live. If None is given, the default_vectordb_dir will be used. Defaults to None.

        Returns:
            BaseVectorDatabase: The initialised vector database.
        &#34;&#34;&#34;
        vectordbs_dir = default_vectordb_dir() if vectordb_dir is None else vectordb_dir
        name = name_checker(name)
        existing_dbs = list_vectordbs(vectordb_dir=vectordbs_dir)
        if name not in existing_dbs:
            raise ValueError(f&#39;The vector database &#34;{name}&#34; does not exist.&#39;)
        
        from ..utils import read_json
        import pickle
        db_info_dir = os.path.join(vectordbs_dir, name, &#39;info.json&#39;)
        db_info = read_json(db_info_dir)
        db_embeddings_name = db_info.get(&#39;embeddings&#39;, None)
        vdb = cls(embeddings, name, vectordbs_dir)
        data_dir =os.path.join(vdb.db_dir, &#39;data.pkl&#39;)
        if os.path.exists(data_dir):
            with open(data_dir, &#39;rb&#39;) as f:
                vdb._data = pickle.load(f)
        else: # recovering from old format
            print(&#39;Trying to recover from old format...&#39;)
            old_data_dir = os.path.join(vdb.db_dir, &#39;index.pkl&#39;)
            if os.path.exists(old_data_dir):
                with open(old_data_dir, &#39;rb&#39;) as f:
                    data = pickle.load(f)
                data = list(map(lambda x: Document(index=x.page_content, metadata=x.metadata), data[0]._dict.values()))
                vdb.add_documents(data, split_text=False)
            else:
                raise FileExistsError(f&#39;No raw data has been saved. Vector database cannot be recovered.&#39;)
        if db_embeddings_name == embeddings.name:
            vdb._index = vdb._load_index(os.path.join(vdb.db_dir, vdb._index_filename))
        else:
            print(f&#39;You are using a different embeddings model. Switching from embedding model {db_embeddings_name} to {embeddings.name}.&#39;)
            vdb.add_documents(list(vdb.data.values()), split_text=False)
            vdb.info[&#39;embeddings&#39;] = embeddings.name
        vdb.save()
        return vdb

    @classmethod
    def from_documents(cls, embeddings: Type[BaseEmbeddingsToolkit], docs: List[Document], 
                      name: Optional[str] = None, vectordb_dir: Optional[str] = None,
                      split_text: bool = True, text_splitter: Optional[Type[BaseTextSplitter]] = None) -&gt; BaseVectorDatabase:
        &#34;&#34;&#34;Load the vector database from existing documents.

        Args:
            embeddings (Type[BaseEmbeddingsToolkit]): Embeddings toolkit to use.
            docs (List[Document]): List of documents to use.
            name (Optional[str], optional): Name of the vector database. Will be used as the directory base name of the vector database in vectordb_dir. If None is given, the vector database will not be saved. Defaults to None.
            vectordb_dir (Optional[str], optional): Directory where the vector databases live. If None is given, the default_vectordb_dir will be used. Defaults to None.
            split_text (bool, optional): Whether to split the docuements with the embeddings toolkit text splitter. Defaults to True.
            text_splitter (Optional[Type[BaseTextSplitter]], optional): Text splitter to split the documents. If none given, the embeddings toolkit text splitter will be used. Defaults to None.

        Returns:
            BaseVectorDatabase: The initialised vector database.
        &#34;&#34;&#34;
        vdb = cls(embeddings, name, vectordb_dir)
        vdb.add_documents(docs=docs, split_text=split_text, text_splitter=text_splitter)
        vdb.save() # In case an empty list of docs is given.
        return vdb
    
    @classmethod
    def from_texts(cls, embeddings: Type[BaseEmbeddingsToolkit], texts: List[str], metadata: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,
                      name: Optional[str] = None, vectordb_dir: Optional[str] = None,
                      split_text: bool = True, text_splitter: Optional[Type[BaseTextSplitter]] = None) -&gt; BaseVectorDatabase:
        &#34;&#34;&#34;Load the vector database from existing texts.

        Args:
            embeddings (Type[BaseEmbeddingsToolkit]): Embeddings toolkit to use.
            texts (List[str]): List of texts to add.
            metadata (Optional[Union[Dict[str, Any], List[Dict[str, Any]]]], optional): Metadata to add along with the texts. Defaults to None.
            name (Optional[str], optional): Name of the vector database. Will be used as the directory base name of the vector database in vectordb_dir. If None is given, the vector database will not be saved. Defaults to None.
            vectordb_dir (Optional[str], optional): Directory where the vector databases live. If None is given, the default_vectordb_dir will be used. Defaults to None.
            split_text (bool, optional): Whether to split the docuements with the embeddings toolkit text splitter. Defaults to True.
            text_splitter (Optional[Type[BaseTextSplitter]], optional): Text splitter to split the documents. If none given, the embeddings toolkit text splitter will be used. Defaults to None.

        Returns:
            BaseVectorDatabase: The initialised vector database.
        &#34;&#34;&#34;
        vdb = cls(embeddings, name, vectordb_dir)
        vdb.add_texts(texts=texts, metadata=metadata, split_text=split_text, text_splitter=text_splitter)
        vdb.save() # In case an empty list of texts is given.
        return vdb

    def _save_data(self) -&gt; None:
        &#34;&#34;&#34;Save the documents in the vector database.
        &#34;&#34;&#34;
        if self.db_dir is not None:
            import pickle
            with open(os.path.join(self.db_dir, &#39;data.pkl&#39;), &#39;wb&#39;) as f:
                pickle.dump(self.data, f)

    def _save_info(self) -&gt; None:
        &#34;&#34;&#34;Save information about the vector database.
        &#34;&#34;&#34;
        from ..utils import save_json, current_time
        self.info[&#39;last_update&#39;] = current_time()
        if self.db_dir is not None:
            save_json(self.info, os.path.join(self.db_dir, &#39;info.json&#39;))

    def save(self) -&gt; None:
        &#34;&#34;&#34;Save the vector database.
        &#34;&#34;&#34;
        if self.db_dir is not None:
            self._save_data()
            self._save_index()
            self._save_info()

    def add_documents(self, docs: List[Document], split_text: bool = True, text_splitter: Optional[Type[BaseTextSplitter]] = None) -&gt; None:
        &#34;&#34;&#34;Add documents into the vector database.

        Args:
            docs (List[Document]): List of documents to split.
            split_text (bool, optional): Whether to split the docuements with the embeddings toolkit text splitter. Defaults to True.
            text_splitter (Optional[Type[BaseTextSplitter]], optional): Text splitter to split the documents. If none given, the embeddings toolkit text splitter will be used. Defaults to None.
        &#34;&#34;&#34;
        if len(docs) != 0:
            text_splitter = self.embeddings.text_splitter if text_splitter is None else text_splitter
            docs = text_splitter.split_documents(docs) if split_text else docs
            vectors = list(map(lambda x: x.index, docs))
            vectors = self.embeddings.batch_embed(vectors)
            vectors = np.array(vectors, dtype=np.float32)
            self._add(vectors=vectors, docs=docs)
            self.save()

    def add_docs_with_vectors(self, vectors: Sequence[Sequence[float]], docs: List[Document]) -&gt; None:
        &#34;&#34;&#34;Add documents with pre-embedded vectors into the vector database.

        Args:
            vectors (Sequence[Sequence[float]]): Pre-embedded vectors.
            docs (List[Document]): List of documents.
        &#34;&#34;&#34;
        len_vec = len(vectors)
        len_doc = len(docs)
        if len_vec != 0:
            if len_vec != len_doc:
                raise ValueError(f&#39;{len_vec} vectors are given but the number of documents given is {len_doc}. Make sure the number of documents match with the number of vectors given.&#39;)
            vectors = np.array(vectors, dtype=np.float32)
            self._add(vectors, docs)
            self.save()

    def add_texts(self, texts: List[str], metadata: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None, 
                  split_text: bool = True, text_splitter: Optional[Type[BaseTextSplitter]] = None) -&gt; None:
        &#34;&#34;&#34;Add texts into the vector database.

        Args:
            texts (List[str]): List of texts to add.
            metadata (Optional[Union[Dict[str, Any], List[Dict[str, Any]]]], optional): Metadata to add along with the texts. Defaults to None.
            split_text (bool, optional): Whether to split the docuements with the embeddings toolkit text splitter. Defaults to True.
            text_splitter (Optional[Type[BaseTextSplitter]], optional): Text splitter to split the documents. If none given, the embeddings toolkit text splitter will be used. Defaults to None.
        &#34;&#34;&#34;
        if len(texts) != 0:
            if metadata is None:
                metadata = [dict()] * len(texts)
            if isinstance(metadata, list):
                if len(metadata) != len(texts):
                    raise ValueError(&#39;Number of texts does not match with number of metadata.&#39;)
            else:
                metadata = [metadata] * len(texts)
            docs = list(map(lambda x: Document(index=x[0], metadata=x[1]), list(zip(texts, metadata))))
            self.add_documents(docs=docs, split_text=split_text, text_splitter=text_splitter)

    def batch_search(self, queries: List[str], top_k: int = 5, index_only: bool = True,
                      batch_size: int = 100, filter_fn: Optional[Callable[[Document], bool]] = None, **kwargs) -&gt; List[List[Union[str, Dict[str, Any]]]]:
        &#34;&#34;&#34;Batch simlarity search on multiple queries.

        Args:
            queries (List[str]): List of queries.
            top_k (int, optional): Maximum number of results for each query. Defaults to 5.
            index_only (bool, optional): Whether to return the list of indexes only. Defaults to True.
            batch_size (int, optional): Batch size to perform similarity search. Defaults to 100.
            filter_fn (Optional[Callable[[Document], bool]], optional): The filter function to limit the scope of similarity search. Defaults to None.

        Returns:
            List[List[Union[str, Dict[str, Any]]]]: List of list of search results.
        &#34;&#34;&#34;
        import gc
        # Filtering the scope of search
        scope = None
        if filter_fn:
            scope = filter(lambda x: filter_fn(x[1]), self.data.items())
        if kwargs:
            scope = self.data.items() if scope is None else scope
            for k, v in kwargs.items():
                scope = filter(lambda x: x[1].metadata.get(k) == v, scope)
        ids_scope = scope if scope is None else list(map(lambda x: x[0], scope))
        top_k = min(self.size, top_k) if ids_scope is None else min(self.size, top_k, len(ids_scope))
        if top_k == 0:
            return [[]] * len(queries)

        q_num = len(queries)
        batch_num = q_num // batch_size if ((q_num // batch_size) == (q_num / batch_size)) else (q_num // batch_size) + 1
        batches = list(map(lambda x: (x * batch_size, min(q_num, (x + 1) * batch_size)), range(batch_num)))
        scores = list()
        ids = list()
        for b in batches:
            qvecs = self.embeddings.batch_embed(queries[b[0]:b[1]])
            score, id = self._batch_search_with_scores(vectors=qvecs, k=top_k, ids_scope=ids_scope)
            scores.append(score)
            ids.append(id)
            del qvecs
            gc.collect()
        scores = np.concatenate(scores, axis=0)
        ids = np.concatenate(ids, axis=0)
        get_docs = np.vectorize(lambda x: self.data[x])
        get_indexes = np.vectorize(lambda x: x.index)
        get_metadatas = np.vectorize(lambda x: x.metadata)
        get_results = np.vectorize(lambda index, score, id, metadata: dict(index=index, score=score, id=id, metadata=metadata))
        docs = get_docs(ids)
        indexes = get_indexes(docs)
        metadatas = get_metadatas(docs)
        results = get_results(indexes, scores, ids, metadatas)
        if index_only:
            get_str = np.vectorize(lambda x: x[&#39;index&#39;])
            return get_str(results).tolist()
        else:
            return results.tolist()
        
    def search(self, query: str, top_k: int = 5, index_only: bool = True, filter_fn: Optional[Callable[[Document], bool]] = None, **kwargs) -&gt; List[Union[str, Dict[str, Any]]]:
        &#34;&#34;&#34;Simlarity search on the given query.

        Args:
            query (str): Query for similarity search.
            top_k (int, optional): Maximum number of results. Defaults to 5.
            index_only (bool, optional): Whether to return the list of indexes only. Defaults to True.
            filter_fn (Optional[Callable[[Document], bool]], optional): The filter function to limit the scope of similarity search. Defaults to None.

        Returns:
            List[Union[str, Dict[str, Any]]]: List of search results.
        &#34;&#34;&#34;
        return self.batch_search(queries=[query], top_k=top_k, index_only=index_only,filter_fn=filter_fn, **kwargs)[0]
            
    def search_by_metadata(self, ids_only: bool = False, filter_fn: Optional[Callable[[Document], bool]] = None, **kwargs) -&gt; Union[List[int], Dict[int, Document]]:
        &#34;&#34;&#34;Search documents or ids by metadata. Pass the filters on metadata as keyword arguments or pass a filter_fn.

        Args:
            ids_only (bool, optional): Whether to return a list of ids or a dictionary with the ids as keys and documents as values. Defaults to False.
            filter_fn (Optional[Callable[[Document], bool]], optional): The filter function. Defaults to None.

        Returns:
            Union[List[int], Dict[int, Document]]: List of ids or dictionary with the ids as keys and documents as values.
        &#34;&#34;&#34;
        results = self.data.items()
        if filter_fn:
            results = filter(lambda x: filter_fn(x[1]), results)
        if kwargs:
            for k, v in kwargs.items():
                results = filter(lambda x: x[1].metadata.get(k) == v, results)

        if ids_only:
            return list(map(lambda x: x[0], results))
        return dict(results)
    
    def delete_by_metadata(self, filter_fn: Optional[Callable[[Document], bool]] = None, **kwargs) -&gt; None:
        &#34;&#34;&#34;Remove records by metadata. Pass the filters on metadata as keyword arguments or pass a filter_fn.

        Args:
            filter_fn (Optional[Callable[[Document], bool]], optional): The filter function. Defaults to None.
        &#34;&#34;&#34;
        if ((not kwargs) and (not filter_fn)):
            raise ValueError(&#39;No keyword arguments or filter_fn are passed. Use the &#34;clear&#34; method to clear the entire database.&#39;)
        ids = self.search_by_metadata(ids_only=True, filter_fn=filter_fn, **kwargs)
        self._delete(ids)
        self.save()

    def clear(self) -&gt; None:
        &#34;&#34;&#34;Clear the entire vector database. Use it with caution.
        &#34;&#34;&#34;
        import gc
        del self._index
        del self._data
        gc.collect()
        self._index = self._get_empty_index()
        self._data
        self.save()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="llmflex.VectorDBs.base_vectordb.default_vectordb_dir"><code class="name flex">
<span>def <span class="ident">default_vectordb_dir</span></span>(<span>) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Default home directory of vector databases.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>Default home directory of vector databases.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def default_vectordb_dir() -&gt; str:
    &#34;&#34;&#34;Default home directory of vector databases.

    Returns:
        str: Default home directory of vector databases.
    &#34;&#34;&#34;
    from ..utils import get_config
    home = os.path.join(get_config()[&#39;package_home&#39;], &#39;vector_databases&#39;)
    if not os.path.exists(home):
        os.makedirs(home)
    return home</code></pre>
</details>
</dd>
<dt id="llmflex.VectorDBs.base_vectordb.list_vectordbs"><code class="name flex">
<span>def <span class="ident">list_vectordbs</span></span>(<span>vectordb_dir: Optional[str] = None) ‑> List[str]</span>
</code></dt>
<dd>
<div class="desc"><p>List all the vector databases in the given directory.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>vectordb_dir</code></strong> :&ensp;<code>Optional[str]</code>, optional</dt>
<dd>Directory where the vector databases live. If None is given, the default_vectordb_dir will be used. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>List[str]</code></dt>
<dd>List all the vector databases in the given directory.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def list_vectordbs(vectordb_dir: Optional[str] = None) -&gt; List[str]:
    &#34;&#34;&#34;List all the vector databases in the given directory.

    Args:
        vectordb_dir (Optional[str], optional): Directory where the vector databases live. If None is given, the default_vectordb_dir will be used. Defaults to None.

    Returns:
        List[str]: List all the vector databases in the given directory.
    &#34;&#34;&#34;
    vectordb_dir = vectordb_dir if ((isinstance(vectordb_dir, str)) &amp; (os.path.exists(vectordb_dir))) else default_vectordb_dir()
    dbs = list(filter(lambda x: os.path.isdir(os.path.join(vectordb_dir, x)), os.listdir(vectordb_dir)))
    dbs = list(filter(lambda x: os.path.exists(os.path.join(vectordb_dir, x, &#39;info.json&#39;)), dbs))
    return dbs</code></pre>
</details>
</dd>
<dt id="llmflex.VectorDBs.base_vectordb.name_checker"><code class="name flex">
<span>def <span class="ident">name_checker</span></span>(<span>name: str) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Raise error if the given string has space, newline characters, or tab characters.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code></dt>
<dd>String to check.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>Return the given text if it passes all the checkes.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def name_checker(name: str) -&gt; str:
    &#34;&#34;&#34;Raise error if the given string has space, newline characters, or tab characters.

    Args:
        name (str): String to check.

    Returns:
        str: Return the given text if it passes all the checkes.
    &#34;&#34;&#34;
    if &#39; &#39; in name:
        raise ValueError(f&#39;Spaces cannot be in the name&#39;)
    if &#39;\n&#39; in name:
        raise ValueError(f&#39;Newline characters cannot be in the name.&#39;)
    if &#39;\r&#39; in name:
        raise ValueError(f&#39;Newline characters cannot be in the name.&#39;)
    if &#39;\t&#39; in name:
        raise ValueError(f&#39;Tab characters cannot be in the name.&#39;)
    return name</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase"><code class="flex name class">
<span>class <span class="ident">BaseVectorDatabase</span></span>
<span>(</span><span>embeddings: Type[BaseEmbeddingsToolkit], name: Optional[str] = None, vectordb_dir: Optional[str] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for vector databases.</p>
<p>Initialise a vector database.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>embeddings</code></strong> :&ensp;<code>Type[BaseEmbeddingsToolkit]</code></dt>
<dd>Embeddings toolkit to use.</dd>
<dt><strong><code>name</code></strong> :&ensp;<code>Optional[str]</code>, optional</dt>
<dd>Name of the vector database. Will be used as the directory base name of the vector database in vectordb_dir. If None is given, the vector database will not be saved. Defaults to None.</dd>
<dt><strong><code>vectordb_dir</code></strong> :&ensp;<code>Optional[str]</code>, optional</dt>
<dd>Directory where the vector databases live. If None is given, the default_vectordb_dir will be used. Defaults to None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BaseVectorDatabase(ABC):
    &#34;&#34;&#34;Base class for vector databases.
    &#34;&#34;&#34;
    def __init__(self, embeddings: Type[BaseEmbeddingsToolkit], name: Optional[str] = None, vectordb_dir: Optional[str] = None) -&gt; None:
        &#34;&#34;&#34;Initialise a vector database.

        Args:
            embeddings (Type[BaseEmbeddingsToolkit]): Embeddings toolkit to use.
            name (Optional[str], optional): Name of the vector database. Will be used as the directory base name of the vector database in vectordb_dir. If None is given, the vector database will not be saved. Defaults to None.
            vectordb_dir (Optional[str], optional): Directory where the vector databases live. If None is given, the default_vectordb_dir will be used. Defaults to None.
        &#34;&#34;&#34;
        self._embeddings = embeddings
        self._name = name_checker(name) if name is not None else None
        vectordb_dir = default_vectordb_dir() if vectordb_dir is None else vectordb_dir
        self._db_dir = os.path.join(vectordb_dir, self.name) if self.name is not None else None
        if self.db_dir is not None:
            os.makedirs(self.db_dir, exist_ok=True)
        self._index = self._get_empty_index()
        self._data = dict()

    @property
    def embeddings(self) -&gt; BaseEmbeddingsToolkit:
        &#34;&#34;&#34;Embeddings toolkit used in the vector database.

        Returns:
            BaseEmbeddingsToolkit: Embeddings toolkit used in the vector database.
        &#34;&#34;&#34;
        return self._embeddings
    
    @property
    def index(self) -&gt; Any:
        &#34;&#34;&#34;Index of the vector database.

        Returns:
            Any: Index of the vector database.
        &#34;&#34;&#34;
        return self._index
    
    @property
    def name(self) -&gt; Optional[str]:
        &#34;&#34;&#34;Name of the vector database.

        Returns:
            Optional[str]: Name of the vector database.
        &#34;&#34;&#34;
        return self._name
    
    @property
    def info(self) -&gt; Dict[str, Any]:
        &#34;&#34;&#34;Information of the vector database.

        Returns:
            Dict[str, Any]: Information of the vector database.
        &#34;&#34;&#34;
        if not hasattr(self, &#39;_info&#39;):
            from ..utils import current_time, read_json
            if self.db_dir is not None:
                info_dir = os.path.join(self.db_dir, &#39;info.json&#39;)
                if os.path.exists(info_dir):
                    self._info = read_json(info_dir)
                else:
                    self._info = dict(embeddings=self.embeddings.name, last_update=current_time())
            else:
                self._info = dict(embeddings=self.embeddings.name, last_update=current_time())
        return self._info
    
    @property
    def db_dir(self) -&gt; Optional[str]:
        &#34;&#34;&#34;Directory of the vector database.

        Returns:
            Optional[str]: Directory of the vector database.
        &#34;&#34;&#34;
        return self._db_dir
    
    @property
    def data(self) -&gt; Dict[int, Document]:
        &#34;&#34;&#34;Dictionary of all the documents in the vector database.

        Returns:
            Dict[int, Document]: Dictionary of all the documents in the vector database.
        &#34;&#34;&#34;
        return self._data
    
    @property
    def size(self) -&gt; int:
        &#34;&#34;&#34;Number of documents in the vector database.

        Returns:
            int: Number of documents in the vector database.
        &#34;&#34;&#34;
        return len(self.data)
    
    @abstractmethod
    def _get_empty_index(self) -&gt; Any:
        &#34;&#34;&#34;Return an empty index.

        Returns:
            Any: An empty index.
        &#34;&#34;&#34;
        pass

    @property
    @abstractmethod
    def _index_filename(self) -&gt; str:
        &#34;&#34;&#34;Base name of the file for the index in the vector database directory.

        Returns:
            str: Base name of the file for the index in the vector database directory.
        &#34;&#34;&#34;
        pass
    
    @abstractmethod
    def _save_index(self) -&gt; None:
        &#34;&#34;&#34;Save the index of the vector database.
        &#34;&#34;&#34;
        pass

    @abstractmethod
    def _load_index(self, index_dir: str) -&gt; Any:
        &#34;&#34;&#34;Load the index from an existing saved file.
        &#34;&#34;&#34;
        pass

    @abstractmethod
    def _add(self, vectors: np.ndarray[np.float32], docs: List[Document]) -&gt; None:
        &#34;&#34;&#34;Core method to add documents into the vector database.

        Args:
            vectors (np.ndarray[np.float32]): Array of vectors created by the indexes of the documents.
            docs (List[Document]): List of documents to add.
        &#34;&#34;&#34;
        pass

    @abstractmethod
    def _delete(self, ids: List[int]) -&gt; None:
        &#34;&#34;&#34;Core method to remove records by ids.

        Args:
            ids (List[int]): Ids to remove.
        &#34;&#34;&#34;
        pass

    @abstractmethod
    def _batch_search_with_scores(self, vectors: np.ndarray[np.float32], k: int = 5, ids_scope: Optional[List[int]] = None) -&gt; Tuple[np.ndarray[np.float32], np.ndarray[np.int64]]:
        &#34;&#34;&#34;Batch similarity search with multiple vectors.

        Args:
            vectors (np.ndarray[np.float32]): Array of vectors for the search.
            k (int, optional): Maximum results for each vector. Defaults to 5.
            ids_scope (Optional[List[int]], optional): The list of allowed ids to return for the similarity search. Defaults to None.

        Returns:
            Tuple[np.ndarray[np.float32], np.ndarray[np.int64]]: Tuple of scores and ids. Both matrices must be in the same shape.
        &#34;&#34;&#34;
        pass

    @abstractmethod
    def _get_vectors_by_ids(self, ids: List[int]) -&gt; np.ndarray[np.float32]:
        &#34;&#34;&#34;Get the array of vectors by ids.

        Args:
            ids (List[int]): Document ids.

        Returns:
            np.ndarray[np.float32]: Arrray of vectors.
        &#34;&#34;&#34;
        pass

    @classmethod
    def from_exist(cls, embeddings: Type[BaseEmbeddingsToolkit], name: str, vectordb_dir: Optional[str] = None) -&gt; BaseVectorDatabase:
        &#34;&#34;&#34;Load the vector database from an existing vector database.

        Args:
            embeddings (Type[BaseEmbeddingsToolkit]): Embeddings toolkit to use.
            name (str): Name of the existing database.
            vectordbs_dir (Optional[str], optional): Directory where the vector databases live. If None is given, the default_vectordb_dir will be used. Defaults to None.

        Returns:
            BaseVectorDatabase: The initialised vector database.
        &#34;&#34;&#34;
        vectordbs_dir = default_vectordb_dir() if vectordb_dir is None else vectordb_dir
        name = name_checker(name)
        existing_dbs = list_vectordbs(vectordb_dir=vectordbs_dir)
        if name not in existing_dbs:
            raise ValueError(f&#39;The vector database &#34;{name}&#34; does not exist.&#39;)
        
        from ..utils import read_json
        import pickle
        db_info_dir = os.path.join(vectordbs_dir, name, &#39;info.json&#39;)
        db_info = read_json(db_info_dir)
        db_embeddings_name = db_info.get(&#39;embeddings&#39;, None)
        vdb = cls(embeddings, name, vectordbs_dir)
        data_dir =os.path.join(vdb.db_dir, &#39;data.pkl&#39;)
        if os.path.exists(data_dir):
            with open(data_dir, &#39;rb&#39;) as f:
                vdb._data = pickle.load(f)
        else: # recovering from old format
            print(&#39;Trying to recover from old format...&#39;)
            old_data_dir = os.path.join(vdb.db_dir, &#39;index.pkl&#39;)
            if os.path.exists(old_data_dir):
                with open(old_data_dir, &#39;rb&#39;) as f:
                    data = pickle.load(f)
                data = list(map(lambda x: Document(index=x.page_content, metadata=x.metadata), data[0]._dict.values()))
                vdb.add_documents(data, split_text=False)
            else:
                raise FileExistsError(f&#39;No raw data has been saved. Vector database cannot be recovered.&#39;)
        if db_embeddings_name == embeddings.name:
            vdb._index = vdb._load_index(os.path.join(vdb.db_dir, vdb._index_filename))
        else:
            print(f&#39;You are using a different embeddings model. Switching from embedding model {db_embeddings_name} to {embeddings.name}.&#39;)
            vdb.add_documents(list(vdb.data.values()), split_text=False)
            vdb.info[&#39;embeddings&#39;] = embeddings.name
        vdb.save()
        return vdb

    @classmethod
    def from_documents(cls, embeddings: Type[BaseEmbeddingsToolkit], docs: List[Document], 
                      name: Optional[str] = None, vectordb_dir: Optional[str] = None,
                      split_text: bool = True, text_splitter: Optional[Type[BaseTextSplitter]] = None) -&gt; BaseVectorDatabase:
        &#34;&#34;&#34;Load the vector database from existing documents.

        Args:
            embeddings (Type[BaseEmbeddingsToolkit]): Embeddings toolkit to use.
            docs (List[Document]): List of documents to use.
            name (Optional[str], optional): Name of the vector database. Will be used as the directory base name of the vector database in vectordb_dir. If None is given, the vector database will not be saved. Defaults to None.
            vectordb_dir (Optional[str], optional): Directory where the vector databases live. If None is given, the default_vectordb_dir will be used. Defaults to None.
            split_text (bool, optional): Whether to split the docuements with the embeddings toolkit text splitter. Defaults to True.
            text_splitter (Optional[Type[BaseTextSplitter]], optional): Text splitter to split the documents. If none given, the embeddings toolkit text splitter will be used. Defaults to None.

        Returns:
            BaseVectorDatabase: The initialised vector database.
        &#34;&#34;&#34;
        vdb = cls(embeddings, name, vectordb_dir)
        vdb.add_documents(docs=docs, split_text=split_text, text_splitter=text_splitter)
        vdb.save() # In case an empty list of docs is given.
        return vdb
    
    @classmethod
    def from_texts(cls, embeddings: Type[BaseEmbeddingsToolkit], texts: List[str], metadata: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,
                      name: Optional[str] = None, vectordb_dir: Optional[str] = None,
                      split_text: bool = True, text_splitter: Optional[Type[BaseTextSplitter]] = None) -&gt; BaseVectorDatabase:
        &#34;&#34;&#34;Load the vector database from existing texts.

        Args:
            embeddings (Type[BaseEmbeddingsToolkit]): Embeddings toolkit to use.
            texts (List[str]): List of texts to add.
            metadata (Optional[Union[Dict[str, Any], List[Dict[str, Any]]]], optional): Metadata to add along with the texts. Defaults to None.
            name (Optional[str], optional): Name of the vector database. Will be used as the directory base name of the vector database in vectordb_dir. If None is given, the vector database will not be saved. Defaults to None.
            vectordb_dir (Optional[str], optional): Directory where the vector databases live. If None is given, the default_vectordb_dir will be used. Defaults to None.
            split_text (bool, optional): Whether to split the docuements with the embeddings toolkit text splitter. Defaults to True.
            text_splitter (Optional[Type[BaseTextSplitter]], optional): Text splitter to split the documents. If none given, the embeddings toolkit text splitter will be used. Defaults to None.

        Returns:
            BaseVectorDatabase: The initialised vector database.
        &#34;&#34;&#34;
        vdb = cls(embeddings, name, vectordb_dir)
        vdb.add_texts(texts=texts, metadata=metadata, split_text=split_text, text_splitter=text_splitter)
        vdb.save() # In case an empty list of texts is given.
        return vdb

    def _save_data(self) -&gt; None:
        &#34;&#34;&#34;Save the documents in the vector database.
        &#34;&#34;&#34;
        if self.db_dir is not None:
            import pickle
            with open(os.path.join(self.db_dir, &#39;data.pkl&#39;), &#39;wb&#39;) as f:
                pickle.dump(self.data, f)

    def _save_info(self) -&gt; None:
        &#34;&#34;&#34;Save information about the vector database.
        &#34;&#34;&#34;
        from ..utils import save_json, current_time
        self.info[&#39;last_update&#39;] = current_time()
        if self.db_dir is not None:
            save_json(self.info, os.path.join(self.db_dir, &#39;info.json&#39;))

    def save(self) -&gt; None:
        &#34;&#34;&#34;Save the vector database.
        &#34;&#34;&#34;
        if self.db_dir is not None:
            self._save_data()
            self._save_index()
            self._save_info()

    def add_documents(self, docs: List[Document], split_text: bool = True, text_splitter: Optional[Type[BaseTextSplitter]] = None) -&gt; None:
        &#34;&#34;&#34;Add documents into the vector database.

        Args:
            docs (List[Document]): List of documents to split.
            split_text (bool, optional): Whether to split the docuements with the embeddings toolkit text splitter. Defaults to True.
            text_splitter (Optional[Type[BaseTextSplitter]], optional): Text splitter to split the documents. If none given, the embeddings toolkit text splitter will be used. Defaults to None.
        &#34;&#34;&#34;
        if len(docs) != 0:
            text_splitter = self.embeddings.text_splitter if text_splitter is None else text_splitter
            docs = text_splitter.split_documents(docs) if split_text else docs
            vectors = list(map(lambda x: x.index, docs))
            vectors = self.embeddings.batch_embed(vectors)
            vectors = np.array(vectors, dtype=np.float32)
            self._add(vectors=vectors, docs=docs)
            self.save()

    def add_docs_with_vectors(self, vectors: Sequence[Sequence[float]], docs: List[Document]) -&gt; None:
        &#34;&#34;&#34;Add documents with pre-embedded vectors into the vector database.

        Args:
            vectors (Sequence[Sequence[float]]): Pre-embedded vectors.
            docs (List[Document]): List of documents.
        &#34;&#34;&#34;
        len_vec = len(vectors)
        len_doc = len(docs)
        if len_vec != 0:
            if len_vec != len_doc:
                raise ValueError(f&#39;{len_vec} vectors are given but the number of documents given is {len_doc}. Make sure the number of documents match with the number of vectors given.&#39;)
            vectors = np.array(vectors, dtype=np.float32)
            self._add(vectors, docs)
            self.save()

    def add_texts(self, texts: List[str], metadata: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None, 
                  split_text: bool = True, text_splitter: Optional[Type[BaseTextSplitter]] = None) -&gt; None:
        &#34;&#34;&#34;Add texts into the vector database.

        Args:
            texts (List[str]): List of texts to add.
            metadata (Optional[Union[Dict[str, Any], List[Dict[str, Any]]]], optional): Metadata to add along with the texts. Defaults to None.
            split_text (bool, optional): Whether to split the docuements with the embeddings toolkit text splitter. Defaults to True.
            text_splitter (Optional[Type[BaseTextSplitter]], optional): Text splitter to split the documents. If none given, the embeddings toolkit text splitter will be used. Defaults to None.
        &#34;&#34;&#34;
        if len(texts) != 0:
            if metadata is None:
                metadata = [dict()] * len(texts)
            if isinstance(metadata, list):
                if len(metadata) != len(texts):
                    raise ValueError(&#39;Number of texts does not match with number of metadata.&#39;)
            else:
                metadata = [metadata] * len(texts)
            docs = list(map(lambda x: Document(index=x[0], metadata=x[1]), list(zip(texts, metadata))))
            self.add_documents(docs=docs, split_text=split_text, text_splitter=text_splitter)

    def batch_search(self, queries: List[str], top_k: int = 5, index_only: bool = True,
                      batch_size: int = 100, filter_fn: Optional[Callable[[Document], bool]] = None, **kwargs) -&gt; List[List[Union[str, Dict[str, Any]]]]:
        &#34;&#34;&#34;Batch simlarity search on multiple queries.

        Args:
            queries (List[str]): List of queries.
            top_k (int, optional): Maximum number of results for each query. Defaults to 5.
            index_only (bool, optional): Whether to return the list of indexes only. Defaults to True.
            batch_size (int, optional): Batch size to perform similarity search. Defaults to 100.
            filter_fn (Optional[Callable[[Document], bool]], optional): The filter function to limit the scope of similarity search. Defaults to None.

        Returns:
            List[List[Union[str, Dict[str, Any]]]]: List of list of search results.
        &#34;&#34;&#34;
        import gc
        # Filtering the scope of search
        scope = None
        if filter_fn:
            scope = filter(lambda x: filter_fn(x[1]), self.data.items())
        if kwargs:
            scope = self.data.items() if scope is None else scope
            for k, v in kwargs.items():
                scope = filter(lambda x: x[1].metadata.get(k) == v, scope)
        ids_scope = scope if scope is None else list(map(lambda x: x[0], scope))
        top_k = min(self.size, top_k) if ids_scope is None else min(self.size, top_k, len(ids_scope))
        if top_k == 0:
            return [[]] * len(queries)

        q_num = len(queries)
        batch_num = q_num // batch_size if ((q_num // batch_size) == (q_num / batch_size)) else (q_num // batch_size) + 1
        batches = list(map(lambda x: (x * batch_size, min(q_num, (x + 1) * batch_size)), range(batch_num)))
        scores = list()
        ids = list()
        for b in batches:
            qvecs = self.embeddings.batch_embed(queries[b[0]:b[1]])
            score, id = self._batch_search_with_scores(vectors=qvecs, k=top_k, ids_scope=ids_scope)
            scores.append(score)
            ids.append(id)
            del qvecs
            gc.collect()
        scores = np.concatenate(scores, axis=0)
        ids = np.concatenate(ids, axis=0)
        get_docs = np.vectorize(lambda x: self.data[x])
        get_indexes = np.vectorize(lambda x: x.index)
        get_metadatas = np.vectorize(lambda x: x.metadata)
        get_results = np.vectorize(lambda index, score, id, metadata: dict(index=index, score=score, id=id, metadata=metadata))
        docs = get_docs(ids)
        indexes = get_indexes(docs)
        metadatas = get_metadatas(docs)
        results = get_results(indexes, scores, ids, metadatas)
        if index_only:
            get_str = np.vectorize(lambda x: x[&#39;index&#39;])
            return get_str(results).tolist()
        else:
            return results.tolist()
        
    def search(self, query: str, top_k: int = 5, index_only: bool = True, filter_fn: Optional[Callable[[Document], bool]] = None, **kwargs) -&gt; List[Union[str, Dict[str, Any]]]:
        &#34;&#34;&#34;Simlarity search on the given query.

        Args:
            query (str): Query for similarity search.
            top_k (int, optional): Maximum number of results. Defaults to 5.
            index_only (bool, optional): Whether to return the list of indexes only. Defaults to True.
            filter_fn (Optional[Callable[[Document], bool]], optional): The filter function to limit the scope of similarity search. Defaults to None.

        Returns:
            List[Union[str, Dict[str, Any]]]: List of search results.
        &#34;&#34;&#34;
        return self.batch_search(queries=[query], top_k=top_k, index_only=index_only,filter_fn=filter_fn, **kwargs)[0]
            
    def search_by_metadata(self, ids_only: bool = False, filter_fn: Optional[Callable[[Document], bool]] = None, **kwargs) -&gt; Union[List[int], Dict[int, Document]]:
        &#34;&#34;&#34;Search documents or ids by metadata. Pass the filters on metadata as keyword arguments or pass a filter_fn.

        Args:
            ids_only (bool, optional): Whether to return a list of ids or a dictionary with the ids as keys and documents as values. Defaults to False.
            filter_fn (Optional[Callable[[Document], bool]], optional): The filter function. Defaults to None.

        Returns:
            Union[List[int], Dict[int, Document]]: List of ids or dictionary with the ids as keys and documents as values.
        &#34;&#34;&#34;
        results = self.data.items()
        if filter_fn:
            results = filter(lambda x: filter_fn(x[1]), results)
        if kwargs:
            for k, v in kwargs.items():
                results = filter(lambda x: x[1].metadata.get(k) == v, results)

        if ids_only:
            return list(map(lambda x: x[0], results))
        return dict(results)
    
    def delete_by_metadata(self, filter_fn: Optional[Callable[[Document], bool]] = None, **kwargs) -&gt; None:
        &#34;&#34;&#34;Remove records by metadata. Pass the filters on metadata as keyword arguments or pass a filter_fn.

        Args:
            filter_fn (Optional[Callable[[Document], bool]], optional): The filter function. Defaults to None.
        &#34;&#34;&#34;
        if ((not kwargs) and (not filter_fn)):
            raise ValueError(&#39;No keyword arguments or filter_fn are passed. Use the &#34;clear&#34; method to clear the entire database.&#39;)
        ids = self.search_by_metadata(ids_only=True, filter_fn=filter_fn, **kwargs)
        self._delete(ids)
        self.save()

    def clear(self) -&gt; None:
        &#34;&#34;&#34;Clear the entire vector database. Use it with caution.
        &#34;&#34;&#34;
        import gc
        del self._index
        del self._data
        gc.collect()
        self._index = self._get_empty_index()
        self._data
        self.save()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="llmflex.VectorDBs.faiss_vectordb.FaissVectorDatabase" href="faiss_vectordb.html#llmflex.VectorDBs.faiss_vectordb.FaissVectorDatabase">FaissVectorDatabase</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.from_documents"><code class="name flex">
<span>def <span class="ident">from_documents</span></span>(<span>embeddings: Type[BaseEmbeddingsToolkit], docs: List[Document], name: Optional[str] = None, vectordb_dir: Optional[str] = None, split_text: bool = True, text_splitter: Optional[Type[BaseTextSplitter]] = None) ‑> <a title="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase" href="#llmflex.VectorDBs.base_vectordb.BaseVectorDatabase">BaseVectorDatabase</a></span>
</code></dt>
<dd>
<div class="desc"><p>Load the vector database from existing documents.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>embeddings</code></strong> :&ensp;<code>Type[BaseEmbeddingsToolkit]</code></dt>
<dd>Embeddings toolkit to use.</dd>
<dt><strong><code>docs</code></strong> :&ensp;<code>List[Document]</code></dt>
<dd>List of documents to use.</dd>
<dt><strong><code>name</code></strong> :&ensp;<code>Optional[str]</code>, optional</dt>
<dd>Name of the vector database. Will be used as the directory base name of the vector database in vectordb_dir. If None is given, the vector database will not be saved. Defaults to None.</dd>
<dt><strong><code>vectordb_dir</code></strong> :&ensp;<code>Optional[str]</code>, optional</dt>
<dd>Directory where the vector databases live. If None is given, the default_vectordb_dir will be used. Defaults to None.</dd>
<dt><strong><code>split_text</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to split the docuements with the embeddings toolkit text splitter. Defaults to True.</dd>
<dt><strong><code>text_splitter</code></strong> :&ensp;<code>Optional[Type[BaseTextSplitter]]</code>, optional</dt>
<dd>Text splitter to split the documents. If none given, the embeddings toolkit text splitter will be used. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase" href="#llmflex.VectorDBs.base_vectordb.BaseVectorDatabase">BaseVectorDatabase</a></code></dt>
<dd>The initialised vector database.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def from_documents(cls, embeddings: Type[BaseEmbeddingsToolkit], docs: List[Document], 
                  name: Optional[str] = None, vectordb_dir: Optional[str] = None,
                  split_text: bool = True, text_splitter: Optional[Type[BaseTextSplitter]] = None) -&gt; BaseVectorDatabase:
    &#34;&#34;&#34;Load the vector database from existing documents.

    Args:
        embeddings (Type[BaseEmbeddingsToolkit]): Embeddings toolkit to use.
        docs (List[Document]): List of documents to use.
        name (Optional[str], optional): Name of the vector database. Will be used as the directory base name of the vector database in vectordb_dir. If None is given, the vector database will not be saved. Defaults to None.
        vectordb_dir (Optional[str], optional): Directory where the vector databases live. If None is given, the default_vectordb_dir will be used. Defaults to None.
        split_text (bool, optional): Whether to split the docuements with the embeddings toolkit text splitter. Defaults to True.
        text_splitter (Optional[Type[BaseTextSplitter]], optional): Text splitter to split the documents. If none given, the embeddings toolkit text splitter will be used. Defaults to None.

    Returns:
        BaseVectorDatabase: The initialised vector database.
    &#34;&#34;&#34;
    vdb = cls(embeddings, name, vectordb_dir)
    vdb.add_documents(docs=docs, split_text=split_text, text_splitter=text_splitter)
    vdb.save() # In case an empty list of docs is given.
    return vdb</code></pre>
</details>
</dd>
<dt id="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.from_exist"><code class="name flex">
<span>def <span class="ident">from_exist</span></span>(<span>embeddings: Type[BaseEmbeddingsToolkit], name: str, vectordb_dir: Optional[str] = None) ‑> <a title="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase" href="#llmflex.VectorDBs.base_vectordb.BaseVectorDatabase">BaseVectorDatabase</a></span>
</code></dt>
<dd>
<div class="desc"><p>Load the vector database from an existing vector database.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>embeddings</code></strong> :&ensp;<code>Type[BaseEmbeddingsToolkit]</code></dt>
<dd>Embeddings toolkit to use.</dd>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the existing database.</dd>
<dt><strong><code>vectordbs_dir</code></strong> :&ensp;<code>Optional[str]</code>, optional</dt>
<dd>Directory where the vector databases live. If None is given, the default_vectordb_dir will be used. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase" href="#llmflex.VectorDBs.base_vectordb.BaseVectorDatabase">BaseVectorDatabase</a></code></dt>
<dd>The initialised vector database.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def from_exist(cls, embeddings: Type[BaseEmbeddingsToolkit], name: str, vectordb_dir: Optional[str] = None) -&gt; BaseVectorDatabase:
    &#34;&#34;&#34;Load the vector database from an existing vector database.

    Args:
        embeddings (Type[BaseEmbeddingsToolkit]): Embeddings toolkit to use.
        name (str): Name of the existing database.
        vectordbs_dir (Optional[str], optional): Directory where the vector databases live. If None is given, the default_vectordb_dir will be used. Defaults to None.

    Returns:
        BaseVectorDatabase: The initialised vector database.
    &#34;&#34;&#34;
    vectordbs_dir = default_vectordb_dir() if vectordb_dir is None else vectordb_dir
    name = name_checker(name)
    existing_dbs = list_vectordbs(vectordb_dir=vectordbs_dir)
    if name not in existing_dbs:
        raise ValueError(f&#39;The vector database &#34;{name}&#34; does not exist.&#39;)
    
    from ..utils import read_json
    import pickle
    db_info_dir = os.path.join(vectordbs_dir, name, &#39;info.json&#39;)
    db_info = read_json(db_info_dir)
    db_embeddings_name = db_info.get(&#39;embeddings&#39;, None)
    vdb = cls(embeddings, name, vectordbs_dir)
    data_dir =os.path.join(vdb.db_dir, &#39;data.pkl&#39;)
    if os.path.exists(data_dir):
        with open(data_dir, &#39;rb&#39;) as f:
            vdb._data = pickle.load(f)
    else: # recovering from old format
        print(&#39;Trying to recover from old format...&#39;)
        old_data_dir = os.path.join(vdb.db_dir, &#39;index.pkl&#39;)
        if os.path.exists(old_data_dir):
            with open(old_data_dir, &#39;rb&#39;) as f:
                data = pickle.load(f)
            data = list(map(lambda x: Document(index=x.page_content, metadata=x.metadata), data[0]._dict.values()))
            vdb.add_documents(data, split_text=False)
        else:
            raise FileExistsError(f&#39;No raw data has been saved. Vector database cannot be recovered.&#39;)
    if db_embeddings_name == embeddings.name:
        vdb._index = vdb._load_index(os.path.join(vdb.db_dir, vdb._index_filename))
    else:
        print(f&#39;You are using a different embeddings model. Switching from embedding model {db_embeddings_name} to {embeddings.name}.&#39;)
        vdb.add_documents(list(vdb.data.values()), split_text=False)
        vdb.info[&#39;embeddings&#39;] = embeddings.name
    vdb.save()
    return vdb</code></pre>
</details>
</dd>
<dt id="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.from_texts"><code class="name flex">
<span>def <span class="ident">from_texts</span></span>(<span>embeddings: Type[BaseEmbeddingsToolkit], texts: List[str], metadata: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None, name: Optional[str] = None, vectordb_dir: Optional[str] = None, split_text: bool = True, text_splitter: Optional[Type[BaseTextSplitter]] = None) ‑> <a title="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase" href="#llmflex.VectorDBs.base_vectordb.BaseVectorDatabase">BaseVectorDatabase</a></span>
</code></dt>
<dd>
<div class="desc"><p>Load the vector database from existing texts.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>embeddings</code></strong> :&ensp;<code>Type[BaseEmbeddingsToolkit]</code></dt>
<dd>Embeddings toolkit to use.</dd>
<dt><strong><code>texts</code></strong> :&ensp;<code>List[str]</code></dt>
<dd>List of texts to add.</dd>
<dt><strong><code>metadata</code></strong> :&ensp;<code>Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]</code>, optional</dt>
<dd>Metadata to add along with the texts. Defaults to None.</dd>
<dt><strong><code>name</code></strong> :&ensp;<code>Optional[str]</code>, optional</dt>
<dd>Name of the vector database. Will be used as the directory base name of the vector database in vectordb_dir. If None is given, the vector database will not be saved. Defaults to None.</dd>
<dt><strong><code>vectordb_dir</code></strong> :&ensp;<code>Optional[str]</code>, optional</dt>
<dd>Directory where the vector databases live. If None is given, the default_vectordb_dir will be used. Defaults to None.</dd>
<dt><strong><code>split_text</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to split the docuements with the embeddings toolkit text splitter. Defaults to True.</dd>
<dt><strong><code>text_splitter</code></strong> :&ensp;<code>Optional[Type[BaseTextSplitter]]</code>, optional</dt>
<dd>Text splitter to split the documents. If none given, the embeddings toolkit text splitter will be used. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase" href="#llmflex.VectorDBs.base_vectordb.BaseVectorDatabase">BaseVectorDatabase</a></code></dt>
<dd>The initialised vector database.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def from_texts(cls, embeddings: Type[BaseEmbeddingsToolkit], texts: List[str], metadata: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,
                  name: Optional[str] = None, vectordb_dir: Optional[str] = None,
                  split_text: bool = True, text_splitter: Optional[Type[BaseTextSplitter]] = None) -&gt; BaseVectorDatabase:
    &#34;&#34;&#34;Load the vector database from existing texts.

    Args:
        embeddings (Type[BaseEmbeddingsToolkit]): Embeddings toolkit to use.
        texts (List[str]): List of texts to add.
        metadata (Optional[Union[Dict[str, Any], List[Dict[str, Any]]]], optional): Metadata to add along with the texts. Defaults to None.
        name (Optional[str], optional): Name of the vector database. Will be used as the directory base name of the vector database in vectordb_dir. If None is given, the vector database will not be saved. Defaults to None.
        vectordb_dir (Optional[str], optional): Directory where the vector databases live. If None is given, the default_vectordb_dir will be used. Defaults to None.
        split_text (bool, optional): Whether to split the docuements with the embeddings toolkit text splitter. Defaults to True.
        text_splitter (Optional[Type[BaseTextSplitter]], optional): Text splitter to split the documents. If none given, the embeddings toolkit text splitter will be used. Defaults to None.

    Returns:
        BaseVectorDatabase: The initialised vector database.
    &#34;&#34;&#34;
    vdb = cls(embeddings, name, vectordb_dir)
    vdb.add_texts(texts=texts, metadata=metadata, split_text=split_text, text_splitter=text_splitter)
    vdb.save() # In case an empty list of texts is given.
    return vdb</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.data"><code class="name">var <span class="ident">data</span> : Dict[int, <a title="llmflex.Schemas.documents.Document" href="../Schemas/documents.html#llmflex.Schemas.documents.Document">Document</a>]</code></dt>
<dd>
<div class="desc"><p>Dictionary of all the documents in the vector database.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Dict[int, Document]</code></dt>
<dd>Dictionary of all the documents in the vector database.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def data(self) -&gt; Dict[int, Document]:
    &#34;&#34;&#34;Dictionary of all the documents in the vector database.

    Returns:
        Dict[int, Document]: Dictionary of all the documents in the vector database.
    &#34;&#34;&#34;
    return self._data</code></pre>
</details>
</dd>
<dt id="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.db_dir"><code class="name">var <span class="ident">db_dir</span> : Optional[str]</code></dt>
<dd>
<div class="desc"><p>Directory of the vector database.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Optional[str]</code></dt>
<dd>Directory of the vector database.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def db_dir(self) -&gt; Optional[str]:
    &#34;&#34;&#34;Directory of the vector database.

    Returns:
        Optional[str]: Directory of the vector database.
    &#34;&#34;&#34;
    return self._db_dir</code></pre>
</details>
</dd>
<dt id="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.embeddings"><code class="name">var <span class="ident">embeddings</span> : <a title="llmflex.Embeddings.base_embeddings.BaseEmbeddingsToolkit" href="../Embeddings/base_embeddings.html#llmflex.Embeddings.base_embeddings.BaseEmbeddingsToolkit">BaseEmbeddingsToolkit</a></code></dt>
<dd>
<div class="desc"><p>Embeddings toolkit used in the vector database.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>BaseEmbeddingsToolkit</code></dt>
<dd>Embeddings toolkit used in the vector database.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def embeddings(self) -&gt; BaseEmbeddingsToolkit:
    &#34;&#34;&#34;Embeddings toolkit used in the vector database.

    Returns:
        BaseEmbeddingsToolkit: Embeddings toolkit used in the vector database.
    &#34;&#34;&#34;
    return self._embeddings</code></pre>
</details>
</dd>
<dt id="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.index"><code class="name">var <span class="ident">index</span> : Any</code></dt>
<dd>
<div class="desc"><p>Index of the vector database.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Any</code></dt>
<dd>Index of the vector database.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def index(self) -&gt; Any:
    &#34;&#34;&#34;Index of the vector database.

    Returns:
        Any: Index of the vector database.
    &#34;&#34;&#34;
    return self._index</code></pre>
</details>
</dd>
<dt id="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.info"><code class="name">var <span class="ident">info</span> : Dict[str, Any]</code></dt>
<dd>
<div class="desc"><p>Information of the vector database.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Dict[str, Any]</code></dt>
<dd>Information of the vector database.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def info(self) -&gt; Dict[str, Any]:
    &#34;&#34;&#34;Information of the vector database.

    Returns:
        Dict[str, Any]: Information of the vector database.
    &#34;&#34;&#34;
    if not hasattr(self, &#39;_info&#39;):
        from ..utils import current_time, read_json
        if self.db_dir is not None:
            info_dir = os.path.join(self.db_dir, &#39;info.json&#39;)
            if os.path.exists(info_dir):
                self._info = read_json(info_dir)
            else:
                self._info = dict(embeddings=self.embeddings.name, last_update=current_time())
        else:
            self._info = dict(embeddings=self.embeddings.name, last_update=current_time())
    return self._info</code></pre>
</details>
</dd>
<dt id="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.name"><code class="name">var <span class="ident">name</span> : Optional[str]</code></dt>
<dd>
<div class="desc"><p>Name of the vector database.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Optional[str]</code></dt>
<dd>Name of the vector database.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def name(self) -&gt; Optional[str]:
    &#34;&#34;&#34;Name of the vector database.

    Returns:
        Optional[str]: Name of the vector database.
    &#34;&#34;&#34;
    return self._name</code></pre>
</details>
</dd>
<dt id="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.size"><code class="name">var <span class="ident">size</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of documents in the vector database.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>Number of documents in the vector database.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def size(self) -&gt; int:
    &#34;&#34;&#34;Number of documents in the vector database.

    Returns:
        int: Number of documents in the vector database.
    &#34;&#34;&#34;
    return len(self.data)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.add_docs_with_vectors"><code class="name flex">
<span>def <span class="ident">add_docs_with_vectors</span></span>(<span>self, vectors: Sequence[Sequence[float]], docs: List[Document]) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Add documents with pre-embedded vectors into the vector database.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>vectors</code></strong> :&ensp;<code>Sequence[Sequence[float]]</code></dt>
<dd>Pre-embedded vectors.</dd>
<dt><strong><code>docs</code></strong> :&ensp;<code>List[Document]</code></dt>
<dd>List of documents.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_docs_with_vectors(self, vectors: Sequence[Sequence[float]], docs: List[Document]) -&gt; None:
    &#34;&#34;&#34;Add documents with pre-embedded vectors into the vector database.

    Args:
        vectors (Sequence[Sequence[float]]): Pre-embedded vectors.
        docs (List[Document]): List of documents.
    &#34;&#34;&#34;
    len_vec = len(vectors)
    len_doc = len(docs)
    if len_vec != 0:
        if len_vec != len_doc:
            raise ValueError(f&#39;{len_vec} vectors are given but the number of documents given is {len_doc}. Make sure the number of documents match with the number of vectors given.&#39;)
        vectors = np.array(vectors, dtype=np.float32)
        self._add(vectors, docs)
        self.save()</code></pre>
</details>
</dd>
<dt id="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.add_documents"><code class="name flex">
<span>def <span class="ident">add_documents</span></span>(<span>self, docs: List[Document], split_text: bool = True, text_splitter: Optional[Type[BaseTextSplitter]] = None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Add documents into the vector database.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>docs</code></strong> :&ensp;<code>List[Document]</code></dt>
<dd>List of documents to split.</dd>
<dt><strong><code>split_text</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to split the docuements with the embeddings toolkit text splitter. Defaults to True.</dd>
<dt><strong><code>text_splitter</code></strong> :&ensp;<code>Optional[Type[BaseTextSplitter]]</code>, optional</dt>
<dd>Text splitter to split the documents. If none given, the embeddings toolkit text splitter will be used. Defaults to None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_documents(self, docs: List[Document], split_text: bool = True, text_splitter: Optional[Type[BaseTextSplitter]] = None) -&gt; None:
    &#34;&#34;&#34;Add documents into the vector database.

    Args:
        docs (List[Document]): List of documents to split.
        split_text (bool, optional): Whether to split the docuements with the embeddings toolkit text splitter. Defaults to True.
        text_splitter (Optional[Type[BaseTextSplitter]], optional): Text splitter to split the documents. If none given, the embeddings toolkit text splitter will be used. Defaults to None.
    &#34;&#34;&#34;
    if len(docs) != 0:
        text_splitter = self.embeddings.text_splitter if text_splitter is None else text_splitter
        docs = text_splitter.split_documents(docs) if split_text else docs
        vectors = list(map(lambda x: x.index, docs))
        vectors = self.embeddings.batch_embed(vectors)
        vectors = np.array(vectors, dtype=np.float32)
        self._add(vectors=vectors, docs=docs)
        self.save()</code></pre>
</details>
</dd>
<dt id="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.add_texts"><code class="name flex">
<span>def <span class="ident">add_texts</span></span>(<span>self, texts: List[str], metadata: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None, split_text: bool = True, text_splitter: Optional[Type[BaseTextSplitter]] = None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Add texts into the vector database.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>texts</code></strong> :&ensp;<code>List[str]</code></dt>
<dd>List of texts to add.</dd>
<dt><strong><code>metadata</code></strong> :&ensp;<code>Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]</code>, optional</dt>
<dd>Metadata to add along with the texts. Defaults to None.</dd>
<dt><strong><code>split_text</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to split the docuements with the embeddings toolkit text splitter. Defaults to True.</dd>
<dt><strong><code>text_splitter</code></strong> :&ensp;<code>Optional[Type[BaseTextSplitter]]</code>, optional</dt>
<dd>Text splitter to split the documents. If none given, the embeddings toolkit text splitter will be used. Defaults to None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_texts(self, texts: List[str], metadata: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None, 
              split_text: bool = True, text_splitter: Optional[Type[BaseTextSplitter]] = None) -&gt; None:
    &#34;&#34;&#34;Add texts into the vector database.

    Args:
        texts (List[str]): List of texts to add.
        metadata (Optional[Union[Dict[str, Any], List[Dict[str, Any]]]], optional): Metadata to add along with the texts. Defaults to None.
        split_text (bool, optional): Whether to split the docuements with the embeddings toolkit text splitter. Defaults to True.
        text_splitter (Optional[Type[BaseTextSplitter]], optional): Text splitter to split the documents. If none given, the embeddings toolkit text splitter will be used. Defaults to None.
    &#34;&#34;&#34;
    if len(texts) != 0:
        if metadata is None:
            metadata = [dict()] * len(texts)
        if isinstance(metadata, list):
            if len(metadata) != len(texts):
                raise ValueError(&#39;Number of texts does not match with number of metadata.&#39;)
        else:
            metadata = [metadata] * len(texts)
        docs = list(map(lambda x: Document(index=x[0], metadata=x[1]), list(zip(texts, metadata))))
        self.add_documents(docs=docs, split_text=split_text, text_splitter=text_splitter)</code></pre>
</details>
</dd>
<dt id="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.batch_search"><code class="name flex">
<span>def <span class="ident">batch_search</span></span>(<span>self, queries: List[str], top_k: int = 5, index_only: bool = True, batch_size: int = 100, filter_fn: Optional[Callable[[Document], bool]] = None, **kwargs) ‑> List[List[Union[str, Dict[str, Any]]]]</span>
</code></dt>
<dd>
<div class="desc"><p>Batch simlarity search on multiple queries.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>queries</code></strong> :&ensp;<code>List[str]</code></dt>
<dd>List of queries.</dd>
<dt><strong><code>top_k</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Maximum number of results for each query. Defaults to 5.</dd>
<dt><strong><code>index_only</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to return the list of indexes only. Defaults to True.</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Batch size to perform similarity search. Defaults to 100.</dd>
<dt><strong><code>filter_fn</code></strong> :&ensp;<code>Optional[Callable[[Document], bool]]</code>, optional</dt>
<dd>The filter function to limit the scope of similarity search. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>List[List[Union[str, Dict[str, Any]]]]</code></dt>
<dd>List of list of search results.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def batch_search(self, queries: List[str], top_k: int = 5, index_only: bool = True,
                  batch_size: int = 100, filter_fn: Optional[Callable[[Document], bool]] = None, **kwargs) -&gt; List[List[Union[str, Dict[str, Any]]]]:
    &#34;&#34;&#34;Batch simlarity search on multiple queries.

    Args:
        queries (List[str]): List of queries.
        top_k (int, optional): Maximum number of results for each query. Defaults to 5.
        index_only (bool, optional): Whether to return the list of indexes only. Defaults to True.
        batch_size (int, optional): Batch size to perform similarity search. Defaults to 100.
        filter_fn (Optional[Callable[[Document], bool]], optional): The filter function to limit the scope of similarity search. Defaults to None.

    Returns:
        List[List[Union[str, Dict[str, Any]]]]: List of list of search results.
    &#34;&#34;&#34;
    import gc
    # Filtering the scope of search
    scope = None
    if filter_fn:
        scope = filter(lambda x: filter_fn(x[1]), self.data.items())
    if kwargs:
        scope = self.data.items() if scope is None else scope
        for k, v in kwargs.items():
            scope = filter(lambda x: x[1].metadata.get(k) == v, scope)
    ids_scope = scope if scope is None else list(map(lambda x: x[0], scope))
    top_k = min(self.size, top_k) if ids_scope is None else min(self.size, top_k, len(ids_scope))
    if top_k == 0:
        return [[]] * len(queries)

    q_num = len(queries)
    batch_num = q_num // batch_size if ((q_num // batch_size) == (q_num / batch_size)) else (q_num // batch_size) + 1
    batches = list(map(lambda x: (x * batch_size, min(q_num, (x + 1) * batch_size)), range(batch_num)))
    scores = list()
    ids = list()
    for b in batches:
        qvecs = self.embeddings.batch_embed(queries[b[0]:b[1]])
        score, id = self._batch_search_with_scores(vectors=qvecs, k=top_k, ids_scope=ids_scope)
        scores.append(score)
        ids.append(id)
        del qvecs
        gc.collect()
    scores = np.concatenate(scores, axis=0)
    ids = np.concatenate(ids, axis=0)
    get_docs = np.vectorize(lambda x: self.data[x])
    get_indexes = np.vectorize(lambda x: x.index)
    get_metadatas = np.vectorize(lambda x: x.metadata)
    get_results = np.vectorize(lambda index, score, id, metadata: dict(index=index, score=score, id=id, metadata=metadata))
    docs = get_docs(ids)
    indexes = get_indexes(docs)
    metadatas = get_metadatas(docs)
    results = get_results(indexes, scores, ids, metadatas)
    if index_only:
        get_str = np.vectorize(lambda x: x[&#39;index&#39;])
        return get_str(results).tolist()
    else:
        return results.tolist()</code></pre>
</details>
</dd>
<dt id="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.clear"><code class="name flex">
<span>def <span class="ident">clear</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Clear the entire vector database. Use it with caution.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clear(self) -&gt; None:
    &#34;&#34;&#34;Clear the entire vector database. Use it with caution.
    &#34;&#34;&#34;
    import gc
    del self._index
    del self._data
    gc.collect()
    self._index = self._get_empty_index()
    self._data
    self.save()</code></pre>
</details>
</dd>
<dt id="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.delete_by_metadata"><code class="name flex">
<span>def <span class="ident">delete_by_metadata</span></span>(<span>self, filter_fn: Optional[Callable[[Document], bool]] = None, **kwargs) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Remove records by metadata. Pass the filters on metadata as keyword arguments or pass a filter_fn.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filter_fn</code></strong> :&ensp;<code>Optional[Callable[[Document], bool]]</code>, optional</dt>
<dd>The filter function. Defaults to None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_by_metadata(self, filter_fn: Optional[Callable[[Document], bool]] = None, **kwargs) -&gt; None:
    &#34;&#34;&#34;Remove records by metadata. Pass the filters on metadata as keyword arguments or pass a filter_fn.

    Args:
        filter_fn (Optional[Callable[[Document], bool]], optional): The filter function. Defaults to None.
    &#34;&#34;&#34;
    if ((not kwargs) and (not filter_fn)):
        raise ValueError(&#39;No keyword arguments or filter_fn are passed. Use the &#34;clear&#34; method to clear the entire database.&#39;)
    ids = self.search_by_metadata(ids_only=True, filter_fn=filter_fn, **kwargs)
    self._delete(ids)
    self.save()</code></pre>
</details>
</dd>
<dt id="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Save the vector database.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save(self) -&gt; None:
    &#34;&#34;&#34;Save the vector database.
    &#34;&#34;&#34;
    if self.db_dir is not None:
        self._save_data()
        self._save_index()
        self._save_info()</code></pre>
</details>
</dd>
<dt id="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.search"><code class="name flex">
<span>def <span class="ident">search</span></span>(<span>self, query: str, top_k: int = 5, index_only: bool = True, filter_fn: Optional[Callable[[Document], bool]] = None, **kwargs) ‑> List[Union[str, Dict[str, Any]]]</span>
</code></dt>
<dd>
<div class="desc"><p>Simlarity search on the given query.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>query</code></strong> :&ensp;<code>str</code></dt>
<dd>Query for similarity search.</dd>
<dt><strong><code>top_k</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Maximum number of results. Defaults to 5.</dd>
<dt><strong><code>index_only</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to return the list of indexes only. Defaults to True.</dd>
<dt><strong><code>filter_fn</code></strong> :&ensp;<code>Optional[Callable[[Document], bool]]</code>, optional</dt>
<dd>The filter function to limit the scope of similarity search. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>List[Union[str, Dict[str, Any]]]</code></dt>
<dd>List of search results.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def search(self, query: str, top_k: int = 5, index_only: bool = True, filter_fn: Optional[Callable[[Document], bool]] = None, **kwargs) -&gt; List[Union[str, Dict[str, Any]]]:
    &#34;&#34;&#34;Simlarity search on the given query.

    Args:
        query (str): Query for similarity search.
        top_k (int, optional): Maximum number of results. Defaults to 5.
        index_only (bool, optional): Whether to return the list of indexes only. Defaults to True.
        filter_fn (Optional[Callable[[Document], bool]], optional): The filter function to limit the scope of similarity search. Defaults to None.

    Returns:
        List[Union[str, Dict[str, Any]]]: List of search results.
    &#34;&#34;&#34;
    return self.batch_search(queries=[query], top_k=top_k, index_only=index_only,filter_fn=filter_fn, **kwargs)[0]</code></pre>
</details>
</dd>
<dt id="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.search_by_metadata"><code class="name flex">
<span>def <span class="ident">search_by_metadata</span></span>(<span>self, ids_only: bool = False, filter_fn: Optional[Callable[[Document], bool]] = None, **kwargs) ‑> Union[List[int], Dict[int, <a title="llmflex.Schemas.documents.Document" href="../Schemas/documents.html#llmflex.Schemas.documents.Document">Document</a>]]</span>
</code></dt>
<dd>
<div class="desc"><p>Search documents or ids by metadata. Pass the filters on metadata as keyword arguments or pass a filter_fn.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>ids_only</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to return a list of ids or a dictionary with the ids as keys and documents as values. Defaults to False.</dd>
<dt><strong><code>filter_fn</code></strong> :&ensp;<code>Optional[Callable[[Document], bool]]</code>, optional</dt>
<dd>The filter function. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Union[List[int], Dict[int, Document]]</code></dt>
<dd>List of ids or dictionary with the ids as keys and documents as values.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def search_by_metadata(self, ids_only: bool = False, filter_fn: Optional[Callable[[Document], bool]] = None, **kwargs) -&gt; Union[List[int], Dict[int, Document]]:
    &#34;&#34;&#34;Search documents or ids by metadata. Pass the filters on metadata as keyword arguments or pass a filter_fn.

    Args:
        ids_only (bool, optional): Whether to return a list of ids or a dictionary with the ids as keys and documents as values. Defaults to False.
        filter_fn (Optional[Callable[[Document], bool]], optional): The filter function. Defaults to None.

    Returns:
        Union[List[int], Dict[int, Document]]: List of ids or dictionary with the ids as keys and documents as values.
    &#34;&#34;&#34;
    results = self.data.items()
    if filter_fn:
        results = filter(lambda x: filter_fn(x[1]), results)
    if kwargs:
        for k, v in kwargs.items():
            results = filter(lambda x: x[1].metadata.get(k) == v, results)

    if ids_only:
        return list(map(lambda x: x[0], results))
    return dict(results)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="llmflex.VectorDBs" href="index.html">llmflex.VectorDBs</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="llmflex.VectorDBs.base_vectordb.default_vectordb_dir" href="#llmflex.VectorDBs.base_vectordb.default_vectordb_dir">default_vectordb_dir</a></code></li>
<li><code><a title="llmflex.VectorDBs.base_vectordb.list_vectordbs" href="#llmflex.VectorDBs.base_vectordb.list_vectordbs">list_vectordbs</a></code></li>
<li><code><a title="llmflex.VectorDBs.base_vectordb.name_checker" href="#llmflex.VectorDBs.base_vectordb.name_checker">name_checker</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase" href="#llmflex.VectorDBs.base_vectordb.BaseVectorDatabase">BaseVectorDatabase</a></code></h4>
<ul class="">
<li><code><a title="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.add_docs_with_vectors" href="#llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.add_docs_with_vectors">add_docs_with_vectors</a></code></li>
<li><code><a title="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.add_documents" href="#llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.add_documents">add_documents</a></code></li>
<li><code><a title="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.add_texts" href="#llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.add_texts">add_texts</a></code></li>
<li><code><a title="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.batch_search" href="#llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.batch_search">batch_search</a></code></li>
<li><code><a title="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.clear" href="#llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.clear">clear</a></code></li>
<li><code><a title="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.data" href="#llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.data">data</a></code></li>
<li><code><a title="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.db_dir" href="#llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.db_dir">db_dir</a></code></li>
<li><code><a title="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.delete_by_metadata" href="#llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.delete_by_metadata">delete_by_metadata</a></code></li>
<li><code><a title="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.embeddings" href="#llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.embeddings">embeddings</a></code></li>
<li><code><a title="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.from_documents" href="#llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.from_documents">from_documents</a></code></li>
<li><code><a title="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.from_exist" href="#llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.from_exist">from_exist</a></code></li>
<li><code><a title="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.from_texts" href="#llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.from_texts">from_texts</a></code></li>
<li><code><a title="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.index" href="#llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.index">index</a></code></li>
<li><code><a title="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.info" href="#llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.info">info</a></code></li>
<li><code><a title="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.name" href="#llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.name">name</a></code></li>
<li><code><a title="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.save" href="#llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.save">save</a></code></li>
<li><code><a title="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.search" href="#llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.search">search</a></code></li>
<li><code><a title="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.search_by_metadata" href="#llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.search_by_metadata">search_by_metadata</a></code></li>
<li><code><a title="llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.size" href="#llmflex.VectorDBs.base_vectordb.BaseVectorDatabase.size">size</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>